{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28458af0-7dba-412f-b8cf-9dd8d76c8435",
   "metadata": {},
   "source": [
    "# Edge Classifier Snippet Method – Ensemble / 3-Judge Classifier\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "* Classify each `(:Case)-[:CITES_TO]->(:Case)` edge as **Positive / Neutral / Negative / Unknown** using:\n",
    "  * **Pre-extracted snippets** stored on the relation (`snippet_1 … snippet_N`)\n",
    "  * Short **case summaries** for citing and cited cases\n",
    "* Use **three independent LLM “judges”** (Mistral, Llama, Claude) and combine them with a **majority-vote ensemble**.\n",
    "* Write **per-model** and **global** results back to Neo4j and (optionally) export CSVs for analysis / QA.\n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook uses\n",
    "\n",
    "* **Snippets from the relation**  \n",
    "  * `snippet_1, snippet_2, …` on each `[:CITES_TO]` relation (created by the Snippet Retriever).\n",
    "* **Case metadata** (for both source = citing, target = cited):\n",
    "  * `name`, `decision_date`, `citation_pipe`, optional `opinion_summary`\n",
    "* **Short citation**  \n",
    "  * First element from target’s `citation_pipe` (used to anchor the LLMs).\n",
    "* **Models via AWS Bedrock**:\n",
    "  * **Mistral 7B Instruct** (`mistral.mistral-7b-instruct-v0:2`)\n",
    "  * **Llama 3 70B Instruct** (`meta.llama3-70b-instruct-v1:0`)\n",
    "  * **Claude 3.5 Sonnet** (`anthropic.claude-3-5-sonnet-20240620-v1:0`)\n",
    "\n",
    "---\n",
    "\n",
    "## How it works (high level)\n",
    "\n",
    "1. **Page through edges**  \n",
    "   * Query `Q_PAGE_REL` to get `(:Case)-[r:CITES_TO]->(:Case)` in batches by `id(r)`.\n",
    "\n",
    "2. **Gather snippets**  \n",
    "   * Read `properties(r)` and pull all keys matching `snippet_\\d+`.\n",
    "   * Sort by number and build a labeled block:\n",
    "\n",
    "     ```text\n",
    "     snippet 1:\n",
    "\n",
    "     [text]\n",
    "\n",
    "     snippet 2:\n",
    "\n",
    "     [text]\n",
    "     ```\n",
    "\n",
    "3. **Build the prompt (shared across models)**  \n",
    "   Each judge sees the same input:\n",
    "   * Citing **case name** + **summary**\n",
    "   * Cited **case name** + **summary**\n",
    "   * **Cited short citation** (first from `citation_pipe`)\n",
    "   * **Snippets block**\n",
    "\n",
    "4. **Call the three judges independently**\n",
    "\n",
    "   * `classify_with_bedrock_mistral(...)`\n",
    "   * `classify_with_bedrock_llama(...)`\n",
    "   * `classify_with_bedrock_claude(...)`\n",
    "\n",
    "   Each judge returns (or fails to return):\n",
    "\n",
    "   ```json\n",
    "   {\n",
    "     \"classification\": \"Positive|Neutral|Negative\",\n",
    "     \"rationale\": \"Four sentences with one direct quote.\"\n",
    "   }\n",
    "\n",
    "\n",
    "If a judge fails (too long / JSON error / empty output, etc.), that model’s label is set to **`\"Unknown\"`** with a failure message in its per-model rationale.\n",
    "\n",
    "5. **Ensemble / majority vote**\n",
    "\n",
    "   * Take the three model labels and compute a **final global label**.\n",
    "   * Combine the selected judges’ rationales into a single **global rationale**.\n",
    "\n",
    "6. **Write back to Neo4j**\n",
    "\n",
    "   For each edge, set:\n",
    "\n",
    "   * Per-model:\n",
    "\n",
    "     * `r.mistral_treatment_label`\n",
    "     * `r.mistral_treatment_rationale`\n",
    "     * `r.llama_treatment_label`\n",
    "     * `r.llama_treatment_rationale`\n",
    "     * `r.claude_treatment_label`\n",
    "     * `r.claude_treatment_rationale`\n",
    "   * Global (ensemble):\n",
    "\n",
    "     * `r.treatment_label`          (final label)\n",
    "     * `r.treatment_rationale`      (explanation + selected judges’ rationales)\n",
    "     * `r.treatment_snippet`        (joined snippets)\n",
    "     * `r.model_used`               (list of model display names)\n",
    "     * `r.updated_at_utc`           (Neo4j `datetime()`)\n",
    "\n",
    "7. **Already-labeled edges**\n",
    "\n",
    "   * If `force=False` and `r.treatment_label` exists and is not `\"Unknown\"`, the edge is **skipped**.\n",
    "\n",
    "8. **No snippets**\n",
    "\n",
    "   * If the relation has **no `snippet_1..N`**, all judges are treated as `Unknown`.\n",
    "   * The global label is set to **`\"Unknown\"`** with a rationale explaining that no snippets were available.\n",
    "\n",
    "---\n",
    "\n",
    "## Ensemble decision logic\n",
    "\n",
    "Let **M**, **L**, **C** be the three model labels.\n",
    "\n",
    "1. **Strict majority** (any label appears 2 or 3 times)\n",
    "\n",
    "   * The global label is that majority label.\n",
    "\n",
    "2. **All three different** (e.g., `Positive`, `Neutral`, `Negative`):\n",
    "\n",
    "   * If **any** judge is `\"Unknown\"` → global label = **`\"Unknown\"`**.\n",
    "   * If **none** are `\"Unknown\"` (e.g., one Positive, one Neutral, one Negative) → global label = **`\"Neutral\"`** by rule.\n",
    "\n",
    "3. **Unknown-dominated**\n",
    "\n",
    "   * If at least **two** judges return `\"Unknown\"` → global label = **`\"Unknown\"`**.\n",
    "   * If exactly one judge is `\"Unknown\"` and the other two **disagree** → global label = **`\"Unknown\"`**.\n",
    "\n",
    "4. **Global rationale content**\n",
    "\n",
    "   * Starts with a short explanation of **why** the final label was chosen (majority vs tie-break rule).\n",
    "   * Then includes **one or more judges’ rationales**:\n",
    "\n",
    "     * If global label is **not** `\"Unknown\"` and not the all-different case:\n",
    "       → include only the judges that voted for the final label.\n",
    "     * If global label is `\"Unknown\"` or all three disagree:\n",
    "       → include **all three** judges’ rationales (so you can inspect their views).\n",
    "\n",
    "---\n",
    "\n",
    "## Token budgeting & overflow handling\n",
    "\n",
    "Each model has its own token budgeting step:\n",
    "\n",
    "* Uses a shared tokenizer to estimate **system + user** tokens.\n",
    "* If the prompt exceeds the context limit:\n",
    "\n",
    "  * Compute the overhead of system + empty user prompt.\n",
    "  * **Trim only the snippet block** down to the remaining allowance.\n",
    "  * Rebuild the user prompt and recheck.\n",
    "* If the prompt is **still too long** after trimming:\n",
    "\n",
    "  * That model returns status **`\"too_long\"`** and is treated as `\"Unknown\"` for that edge.\n",
    "  * The edge can still be labeled globally if the other two judges succeed.\n",
    "\n",
    "Summaries and metadata are preserved; only snippets are shortened.\n",
    "\n",
    "---\n",
    "\n",
    "## Robust JSON parsing & retries (per model)\n",
    "\n",
    "For each judge:\n",
    "\n",
    "* Up to **3 attempts** per edge with **increasingly strict** system instructions:\n",
    "\n",
    "  1. Normal instructions.\n",
    "  2. “Output JSON only.”\n",
    "  3. “Output only a JSON object with keys `classification` and `rationale`.”\n",
    "\n",
    "* Parsing pipeline:\n",
    "\n",
    "  * Clean output:\n",
    "\n",
    "    * Strip backticks / ```json fences.\n",
    "    * Normalize whitespace and smart quotes.\n",
    "    * Remove trailing commas.\n",
    "    * Optionally fix unescaped quotes inside the `rationale` string.\n",
    "  * Try direct JSON parsing of the **whole string**.\n",
    "  * If that fails, search for the **largest `{...}` block** and parse that.\n",
    "  * As a last resort, use a regex-style extraction (for Llama) to salvage `classification` and `rationale`.\n",
    "\n",
    "* Status codes used:\n",
    "\n",
    "  * `\"ok\"`\n",
    "  * `\"json_parse_failed\"`\n",
    "  * `\"bad_keys_or_values\"`\n",
    "  * `\"too_long\"`\n",
    "  * `\"api_error\"`, `\"api_error_throttled\"`, `\"api_response_error\"`\n",
    "  * `\"empty_response\"`\n",
    "\n",
    "These statuses drive both the per-model `Unknown` handling and the `failed_citations.csv` log.\n",
    "\n",
    "---\n",
    "\n",
    "## Neo4j I/O\n",
    "\n",
    "**Reads**\n",
    "\n",
    "* `Q_PAGE_REL`:\n",
    "\n",
    "  * Source and target IDs, names, decision dates, `citation_pipe`\n",
    "  * `src_summary`, `tgt_summary`\n",
    "  * Source URL (CourtListener or case URL)\n",
    "  * `properties(r)` (for snippets and any existing fields)\n",
    "  * Existing `r.treatment_label` (used to skip unless `force=True`)\n",
    "\n",
    "* Snippet extraction:\n",
    "\n",
    "  * Look for keys matching `snippet_\\d+`, sort them numerically, and build the ordered list.\n",
    "\n",
    "**Writes**\n",
    "\n",
    "* `Q_WRITE_REL_ANNOT`:\n",
    "\n",
    "  * Per-model labels / rationales (Mistral, Llama, Claude)\n",
    "  * Global `treatment_label` + `treatment_rationale`\n",
    "  * `treatment_snippet` (joined snippets)\n",
    "  * `model_used` (list of model display names)\n",
    "  * `updated_at_utc` timestamp\n",
    "\n",
    "---\n",
    "\n",
    "## CSV outputs (optional)\n",
    "\n",
    "### Results CSV\n",
    "\n",
    "With `results_csv=True`, the pipeline writes (by default):\n",
    "\n",
    "* `edge_classifications_ensemble.csv`\n",
    "\n",
    "Each row includes:\n",
    "\n",
    "* Source / Target:\n",
    "\n",
    "  * IDs, names, decision dates, `citation_pipe`\n",
    "  * Summaries (source/target)\n",
    "* Snippets and labels:\n",
    "\n",
    "  * Joined `Opinion Snippet`\n",
    "  * `Mistral Citation Evaluation`\n",
    "  * `LLama Citation Evaluation`\n",
    "  * `Claude Citation Evaluation`\n",
    "  * `Global Citation Evaluation`\n",
    "* Rationales:\n",
    "\n",
    "  * `Mistral Rationale`\n",
    "  * `LLama Rationale`\n",
    "  * `Claude Rationale`\n",
    "  * `Global Rationale`\n",
    "* URL:\n",
    "\n",
    "  * Source case URL (CourtListener if available)\n",
    "\n",
    "You can choose to export only rows from the current run or replace `df_results` with **all labeled relations** via `show_all_labels_in_output_csv=True`.\n",
    "\n",
    "### Failed classifications CSV\n",
    "\n",
    "With `failed_csv=True`, the pipeline writes:\n",
    "\n",
    "* `failed_citations.csv`\n",
    "\n",
    "Each row captures:\n",
    "\n",
    "* The first 11 columns (source/target metadata and `Opinion Snippet`).\n",
    "* `Model Name`\n",
    "* `Attempt` (1–3)\n",
    "* `Input to LLM` (full prompt used for that attempt)\n",
    "* `LLM Output` (raw text)\n",
    "* `Type of Error` (human-readable status from `_describe_error`)\n",
    "\n",
    "This file is useful to debug JSON issues, timeouts, and token overflows.\n",
    "\n",
    "### Optional labeled dataset merge\n",
    "\n",
    "If you pass a labeled CSV (human ground truth) via `append_to_labeled_dataset_csv`:\n",
    "\n",
    "* The script:\n",
    "\n",
    "  * Reads your labeled dataset (with expected columns like `Source ID`, `Target ID`, `Chunk`, `Label`, `Rationale`).\n",
    "  * Joins it with the model outputs on `Source Case ID` and `Target Case ID`.\n",
    "  * Writes a **model comparison** CSV (default: `\"<stem> - model comparison.csv\"`), which includes:\n",
    "\n",
    "    * Human label vs **global model label** and per-model labels.\n",
    "    * Human rationale vs per-model and global rationales.\n",
    "    * Both the labeled chunk and the pipeline’s retrieved chunk.\n",
    "\n",
    "---\n",
    "\n",
    "## Key parameters\n",
    "\n",
    "```python\n",
    "label_all_citations(\n",
    "    results_csv: bool = False,\n",
    "    results_csv_filename: str = \"edge_classifications_ensemble.csv\",\n",
    "    batch_size: int = 200,\n",
    "    echo: bool = True,\n",
    "    force: bool = False,\n",
    "    append_to_labeled_dataset_csv: Optional[str] = None,\n",
    "    labeled_output_csv: Optional[str] = None,   # default: \"<labeled_csv_stem> - model comparison.csv\"\n",
    "    show_all_labels_in_output_csv: bool = False,\n",
    "    failed_csv: bool = False,\n",
    ")\n",
    "```\n",
    "\n",
    "* `results_csv` / `results_csv_filename`\n",
    "  Control whether to write the main ensemble results CSV.\n",
    "* `batch_size`\n",
    "  Number of edges per Neo4j page.\n",
    "* `echo`\n",
    "  If `True`, prints per-edge summary lines and diagnostics.\n",
    "* `force`\n",
    "  If `True`, re-labels edges even if `treatment_label` already exists and is not `\"Unknown\"`.\n",
    "* `append_to_labeled_dataset_csv` / `labeled_output_csv`\n",
    "  Merge with a human-labeled dataset and export a comparison file.\n",
    "* `show_all_labels_in_output_csv`\n",
    "  If `True`, replace `df_results` with **all labeled edges** currently in Neo4j before writing `results_csv`.\n",
    "* `failed_csv`\n",
    "  If `True`, write `failed_citations.csv` with all non-OK LLM attempts.\n",
    "\n",
    "---\n",
    "\n",
    "## Environment\n",
    "\n",
    "* Loads `../.env`. Required:\n",
    "\n",
    "  * `NEO4J_URI`, `NEO4J_USERNAME`, `NEO4J_PASSWORD`\n",
    "    (and optionally `NEO4J_DATABASE`, default `\"neo4j\"`)\n",
    "  * `BEDROCK_REGION` (default `\"us-east-1\"`)\n",
    "\n",
    "The model IDs for Mistral, Llama, and Claude are set at the top of the script.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick start\n",
    "\n",
    "```python\n",
    "label_all_citations(\n",
    "    force=False,\n",
    "    echo=True,\n",
    "    results_csv=True,\n",
    "    results_csv_filename=\"edge_classifications_ensemble.csv\",\n",
    "    failed_csv=True,\n",
    ")\n",
    "```\n",
    "\n",
    "This will:\n",
    "\n",
    "* Page through all `CITES_TO` edges that are not already labeled (unless `force=True`).\n",
    "* Run all **three** judges for each edge.\n",
    "* Write per-model and global labels back to Neo4j.\n",
    "* Export both:\n",
    "\n",
    "  * `edge_classifications_ensemble.csv`\n",
    "  * `failed_citations.csv` (for debugging)\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "* **Many “Unknown” labels**\n",
    "\n",
    "  * Check `failed_citations.csv` to see if failures come from:\n",
    "\n",
    "    * Missing snippets (`missing_snippets`)\n",
    "    * Token limits (`too_long`)\n",
    "    * JSON parsing issues (`json_parse_failed`, `bad_keys_or_values`)\n",
    "  * Verify that the Snippet Retriever has populated `snippet_1..N`.\n",
    "\n",
    "* **Prompt too long**\n",
    "\n",
    "  * Snippets are already auto-trimmed, but if you still hit `\"too_long\"`:\n",
    "\n",
    "    * Reduce number/length of stored snippets per edge, or\n",
    "    * Lower `max_new_tokens` for one or more models.\n",
    "\n",
    "* **Frequent JSON parse errors**\n",
    "\n",
    "  * Set `echo=True` and review the raw outputs.\n",
    "  * Consider simplifying case summaries or snippet size if the models are over-responding.\n",
    "\n",
    "* **Disagreement across models**\n",
    "\n",
    "  * Use the global rationale plus per-model labels/rationales in the CSV to:\n",
    "\n",
    "    * Inspect where models disagree.\n",
    "    * Decide if you want to trust the majority rule or override with human review for those edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbb61c4-d969-4934-9527-efdf3ab17ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in /opt/conda/lib/python3.12/site-packages (5.28.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from neo4j) (2024.2)\n"
     ]
    }
   ],
   "source": [
    "# Install (if applicable)\n",
    "! pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e06ba7-d97f-4154-bf06-c11d92755d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.12.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d94089-4492-45b6-8e6f-4711a7302298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, json, pathlib, logging, datetime as dt, random\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from botocore.exceptions import ClientError, BotoCoreError\n",
    "import tiktoken\n",
    "\n",
    "# Quiet noisy logs (incl. Neo4j notifications/deprecations)\n",
    "for _n in (\"neo4j\", \"neo4j.notifications\", \"neo4j.work.simple\"):\n",
    "    logging.getLogger(_n).setLevel(logging.ERROR)\n",
    "os.environ.setdefault(\"NEO4J_DRIVER_LOG_LEVEL\", \"ERROR\")\n",
    "\n",
    "# =========================\n",
    "# Config / ENV\n",
    "# =========================\n",
    "BEDROCK_REGION        = os.getenv(\"BEDROCK_REGION\", \"us-east-1\")\n",
    "MISTRAL_MODEL_ID      = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "LLAMA_MODEL_ID        = \"meta.llama3-70b-instruct-v1:0\"\n",
    "CLAUDE_MODEL_ID       = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "MISTRAL_DISPLAY_NAME  = \"Mistral 7B Instruct v0.2 (mistral.mistral-7b-instruct-v0:2)\"\n",
    "LLAMA_DISPLAY_NAME    = \"Llama 3 70B Instruct (meta.llama3-70b-instruct-v1:0)\"\n",
    "CLAUDE_DISPLAY_NAME   = \"Claude 3.5 Sonnet (anthropic.claude-3-5-sonnet-20240620-v1:0)\"\n",
    "\n",
    "# .env is always one level up from this notebook\n",
    "load_dotenv(\"../.env\", override=True)\n",
    "NEO4J_URI       = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME  = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD  = os.getenv(\"NEO4J_PASSWORD\")\n",
    "NEO4J_DATABASE  = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "\n",
    "if not (NEO4J_URI and NEO4J_USERNAME and NEO4J_PASSWORD):\n",
    "    raise RuntimeError(\"Missing Neo4j connection settings. Check ../.env for NEO4J_URI/NEO4J_USERNAME/NEO4J_PASSWORD.\")\n",
    "\n",
    "# =========================\n",
    "# Tokenizer for prompt sizing (Claude-style; used for all models)\n",
    "# =========================\n",
    "_tok = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def _count_tokens(text: str) -> int:\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(_tok.encode(text))\n",
    "\n",
    "def _trim_to_tokens(text: str, max_tokens: int) -> str:\n",
    "    if max_tokens <= 0:\n",
    "        return \"\"\n",
    "    token_ids = _tok.encode(text)\n",
    "    if len(token_ids) <= max_tokens:\n",
    "        return text\n",
    "    return _tok.decode(token_ids[:max_tokens])\n",
    "\n",
    "def _max_ctx_tokens_for_bedrock(max_new_tokens: int) -> int:\n",
    "    \"\"\"\n",
    "    Claude 3.5 Sonnet supports ~200k tokens of context.\n",
    "    Use that as a safe upper bound for all three models and keep a buffer.\n",
    "    \"\"\"\n",
    "    max_context = 200_000\n",
    "    buffer_for_overhead = 2_000\n",
    "    reserve_for_output  = max_new_tokens + 128\n",
    "    return max_context - buffer_for_overhead - reserve_for_output\n",
    "\n",
    "# =========================\n",
    "# Bedrock client (lazy)\n",
    "# =========================\n",
    "_bedrock_client = None\n",
    "\n",
    "def _bedrock():\n",
    "    global _bedrock_client\n",
    "    if _bedrock_client is None:\n",
    "        _bedrock_client = boto3.client(\"bedrock-runtime\", region_name=BEDROCK_REGION)\n",
    "    return _bedrock_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584297bb-8e9e-478f-9fd6-b694f7e2fd45",
   "metadata": {},
   "source": [
    "## Edge Classification Prompt (Snippet Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f3012ee-fc67-4dbf-9919-40de462896ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Prompts\n",
    "# =========================\n",
    "SYSTEM_PROMPT = \"\"\"Role: You are an experienced lawyer specializing in legal citation analysis. \n",
    "Goal: Your goal is to classify how a citing case treats a cited case (e.g., Positive, Neutral, Negative) when given:\n",
    "1.        Citing Case Name: the name of the citing case.\n",
    "2.        Citing Case Summary: the summary of the citing case.\n",
    "3.        Cited Case Name: the name of the cited case\n",
    "4.        Cited Case Summary: the summary of the cited case.\n",
    "5.        Snippets (from citing opinion where the cited case appears): List of text snippets where the cited case is cited in the citing opinion text \n",
    "\n",
    "CRITICAL RULES (must follow):\n",
    "1. **If the citing case uses the cited case to establish or support ANY legal rule, doctrine, standard, test, or conclusion, classify as POSITIVE.**\n",
    "   - This includes situations where the case:\n",
    "     • recites a standard from the case,\n",
    "     • cites the case as part of a string cite supporting a legal principle,\n",
    "     • uses the case as an example consistent with its reasoning,\n",
    "     • applies reasoning from the cited case.\n",
    "\n",
    "   - IMPORTANT: The opinion **does NOT need to use words like “follow,” “adopt,” “agree,” or “apply.”**  \n",
    "     Any supportive or explanatory use counts as Positive.\n",
    "\n",
    "2. Classify as NEUTRAL **only when the case is mentioned without being used as authority**.\n",
    "   - Examples:\n",
    "     • descriptive background\n",
    "     • illustrating a factual distinction\n",
    "     • quoting language without using it to support a rule\n",
    "     • noting procedural history\n",
    "\n",
    "3. Classify as NEGATIVE when the citing court:\n",
    "   - criticizes, limits, distinguishes, rejects, or declines to follow the cited case.\n",
    "\n",
    "4. When writing the rationale:\n",
    "   - Include ONE short direct quote from the citing opinion.\n",
    "   - The quote must illustrate why the treatment is Positive, Neutral, or Negative.\n",
    "   - Provide EXACTLY four sentences.\n",
    "\n",
    "OUTPUT FORMAT (strict):\n",
    "{\n",
    "  \"classification\": \"Positive|Neutral|Negative\",\n",
    "  \"rationale\": \"Four sentences with one direct quote.\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT_TMPL = \"\"\"Now, classify how the citing case treats the cited case. Return your answer as a json format: {{classification: \"\", rationale: \"\"}} where label is one of Positive, Neutral, or Negative and rationale is a four-sentence explanation that justifies your classification\n",
    "using evidence from the cited paragraph and case summaries.\n",
    "\n",
    "Input:\n",
    "Citing Case Name: {citing_case_name}\n",
    "Citing Case Summary: {citing_case_summary}\n",
    "Cited Case Name: {cited_case_name}\n",
    "Cited Case Citation: {cited_case_citation}\n",
    "Cited Case Summary: {cited_case_summary}\n",
    "\n",
    "Snippets (from citing opinion where the cited case appears):\n",
    "{snippet_block}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86060f1-a79a-43e9-b9da-e2af511b7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inst(sys: str, usr: str) -> str:\n",
    "    \"\"\"Mistral-style chat wrapper.\"\"\"\n",
    "    return f\"<s>[INST]{sys}\\n{usr}[/INST]\"\n",
    "\n",
    "def _llama_prompt(sys: str, usr: str) -> str:\n",
    "    \"\"\"Llama 3 chat wrapper, Bedrock-compatible.\"\"\"\n",
    "    return (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        f\"{sys}\\n\"\n",
    "        \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"{usr}\\n\"\n",
    "        \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    )\n",
    "\n",
    "def _format_snippet_block_labeled(snippets: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Render as:\n",
    "    snippet 1:\n",
    "\n",
    "    [text]\n",
    "\n",
    "    snippet 2:\n",
    "\n",
    "    [text]\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    for i, s in enumerate(snippets, 1):\n",
    "        ss = (s or \"\").strip()\n",
    "        if not ss:\n",
    "            continue\n",
    "        blocks.append(f\"snippet {i}:\\n\\n{ss}\")\n",
    "    return \"\\n\\n\".join(blocks) if blocks else \"snippet 1:\\n\\n[N/A]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ce592-2f9d-4aa0-b01a-53262af8023f",
   "metadata": {},
   "source": [
    "## Cypher Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bac7d4-77a5-454f-bdf4-d5c0281021ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cypher queries\n",
    "# =========================\n",
    "COL_URL_HEADER = \"Source Case CourtListener URL (may be incorrect for some cases)\"\n",
    "\n",
    "Q_PAGE_REL = \"\"\"\n",
    "MATCH (s:Case)-[r:CITES_TO]->(t:Case)\n",
    "WHERE id(r) > $after_id\n",
    "RETURN id(r) AS rel_id,\n",
    "       s.id AS src_id, coalesce(s.name,'') AS src_name, s.decision_date AS src_date, s.citation_pipe AS src_cite,\n",
    "       coalesce(s.court_listener_url, s.url, '') AS src_url,\n",
    "       coalesce(s.opinion_summary,'') AS src_summary,\n",
    "       t.id AS tgt_id, coalesce(t.name,'') AS tgt_name, t.decision_date AS tgt_date, t.citation_pipe AS tgt_cite,\n",
    "       coalesce(t.opinion_summary,'') AS tgt_summary,\n",
    "       properties(r) AS rel_props,\n",
    "       r.treatment_label AS existing_label\n",
    "ORDER BY rel_id\n",
    "LIMIT $limit\n",
    "\"\"\"\n",
    "\n",
    "Q_WRITE_REL_ANNOT = \"\"\"\n",
    "MATCH (s:Case {id:$src_id})-[r:CITES_TO]->(t:Case {id:$tgt_id})\n",
    "SET r.mistral_treatment_label     = $mistral_label,\n",
    "    r.mistral_treatment_rationale = $mistral_rationale,\n",
    "    r.llama_treatment_label       = $llama_label,\n",
    "    r.llama_treatment_rationale   = $llama_rationale,\n",
    "    r.claude_treatment_label      = $claude_label,\n",
    "    r.claude_treatment_rationale  = $claude_rationale,\n",
    "    r.treatment_label             = $label,\n",
    "    r.treatment_rationale         = $rationale,\n",
    "    r.treatment_snippet           = $snippet_joined,\n",
    "    r.model_used                  = $model_used,\n",
    "    r.updated_at_utc              = datetime()\n",
    "RETURN count(r) AS updated\n",
    "\"\"\"\n",
    "\n",
    "Q_COUNT_LABELS = \"\"\"\n",
    "MATCH ()-[r:CITES_TO]->()\n",
    "RETURN\n",
    "  sum(CASE WHEN r.treatment_label = 'Positive' THEN 1 ELSE 0 END) AS pos_total,\n",
    "  sum(CASE WHEN r.treatment_label = 'Neutral'  THEN 1 ELSE 0 END) AS neu_total,\n",
    "  sum(CASE WHEN r.treatment_label = 'Negative' THEN 1 ELSE 0 END) AS neg_total,\n",
    "  sum(CASE WHEN r.treatment_label = 'Unknown'  THEN 1 ELSE 0 END) AS unk_total\n",
    "\"\"\"\n",
    "\n",
    "Q_ALL_LABELED_REL = \"\"\"\n",
    "MATCH (s:Case)-[r:CITES_TO]->(t:Case)\n",
    "WHERE r.treatment_label IS NOT NULL\n",
    "RETURN\n",
    "  s.id AS src_id,\n",
    "  coalesce(s.name,'') AS src_name,\n",
    "  s.decision_date AS src_date,\n",
    "  s.citation_pipe AS src_cite,\n",
    "  coalesce(s.court_listener_url, s.url, '') AS src_url,\n",
    "  coalesce(s.opinion_summary,'') AS src_summary,\n",
    "  t.id AS tgt_id,\n",
    "  coalesce(t.name,'') AS tgt_name,\n",
    "  t.decision_date AS tgt_date,\n",
    "  t.citation_pipe AS tgt_cite,\n",
    "  coalesce(t.opinion_summary,'') AS tgt_summary,\n",
    "  coalesce(r.treatment_snippet,'') AS snippet_joined,\n",
    "  coalesce(r.mistral_treatment_label,'') AS mistral_label,\n",
    "  coalesce(r.llama_treatment_label,'')  AS llama_label,\n",
    "  coalesce(r.claude_treatment_label,'') AS claude_label,\n",
    "  r.treatment_label AS treatment_label,\n",
    "  coalesce(r.mistral_treatment_rationale,'') AS mistral_rationale,\n",
    "  coalesce(r.llama_treatment_rationale,'')  AS llama_rationale,\n",
    "  coalesce(r.claude_treatment_rationale,'') AS claude_rationale,\n",
    "  coalesce(r.treatment_rationale,'')        AS treatment_rationale\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0bd5cb-bf54-4db8-a4e2-f768a283d7ec",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0667006-5525-4592-9b77-26cd4f22ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _first_citation(citation_pipe: Optional[str]) -> str:\n",
    "    if not citation_pipe:\n",
    "        return \"\"\n",
    "    parts = [p.strip() for p in re.split(r'[|;]+', citation_pipe) if p.strip()]\n",
    "    return parts[0] if parts else \"\"\n",
    "\n",
    "def _extract_numbered_snippets(rel_props: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Pull snippet_1, snippet_2, ... from properties(r) and return as a list\n",
    "    sorted by numeric suffix. Skip empty/whitespace-only.\n",
    "    \"\"\"\n",
    "    out: List[Tuple[int, str]] = []\n",
    "    for k, v in (rel_props or {}).items():\n",
    "        m = re.fullmatch(r\"snippet_(\\d+)\", k)\n",
    "        if not m:\n",
    "            continue\n",
    "        try:\n",
    "            idx = int(m.group(1))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        text = str(v or \"\").strip()\n",
    "        if text:\n",
    "            out.append((idx, text))\n",
    "    out.sort(key=lambda p: p[0])\n",
    "    return [t for _, t in out]\n",
    "\n",
    "def _read_csv_fallback(path: str) -> pd.DataFrame:\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"cp1252\", errors=\"replace\") as fh:\n",
    "            return pd.read_csv(fh)\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise last_err if last_err else RuntimeError(\"Failed to read CSV with fallback encodings\")\n",
    "\n",
    "def _ordinal_word(n: int) -> str:\n",
    "    return {1: \"first\", 2: \"second\", 3: \"third\"}.get(n, f\"{n}th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4095b89-8d0e-4a2c-acea-93b2e8ac249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# JSON cleaning / normalization (improved)\n",
    "# =========================\n",
    "_BACKTICKS_RE = re.compile(r\"^```(?:json)?|```$\", re.MULTILINE)\n",
    "\n",
    "def _clean_json(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean common formatting issues:\n",
    "      - remove ```json fences\n",
    "      - normalize smart quotes\n",
    "      - strip a single outer pair of quotes if the whole thing is one big JSON string\n",
    "      - remove trailing commas before } or ]\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "\n",
    "    s = _BACKTICKS_RE.sub(\"\", s).strip()\n",
    "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "\n",
    "    # If the whole thing is wrapped in one pair of quotes, strip them\n",
    "    if len(s) >= 2 and s[0] == s[-1] == '\"':\n",
    "        inner = s[1:-1].strip()\n",
    "        if (\"{\" in inner and \"}\" in inner) or (\"[\" in inner and \"]\" in inner):\n",
    "            s = inner\n",
    "\n",
    "    # Remove trailing commas before object/array close\n",
    "    s = re.sub(r\",\\s*([}\\]])\", r\"\\1\", s)\n",
    "    return s\n",
    "\n",
    "def _normalize_from_js(js: Any) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Given a parsed JSON value (dict or string), try to extract:\n",
    "      classification ∈ {\"Positive\",\"Neutral\",\"Negative\"}\n",
    "      rationale: non-empty string\n",
    "    Handles nested or stringified JSON inside the 'rationale' field.\n",
    "    \"\"\"\n",
    "    valid_labels = {\"Positive\", \"Neutral\", \"Negative\"}\n",
    "\n",
    "    # Case 1: dict at top level\n",
    "    if isinstance(js, dict):\n",
    "        c = js.get(\"classification\") or js.get(\"label\")\n",
    "        r = js.get(\"rationale\")\n",
    "\n",
    "        # If rationale itself is another dict, try inner fields\n",
    "        if isinstance(r, dict):\n",
    "            inner_c = r.get(\"classification\") or r.get(\"label\")\n",
    "            inner_r = r.get(\"rationale\")\n",
    "            if inner_c in valid_labels and isinstance(inner_r, str) and inner_r.strip():\n",
    "                return inner_c, inner_r.strip()\n",
    "\n",
    "        # If rationale is a string that looks like JSON, try to parse it\n",
    "        if isinstance(r, str):\n",
    "            r_str = r.strip()\n",
    "            if len(r_str) >= 2 and r_str[0] == r_str[-1] == '\"':\n",
    "                r_str = r_str[1:-1].strip()\n",
    "            if r_str.startswith(\"{\") and \"classification\" in r_str and \"rationale\" in r_str:\n",
    "                try:\n",
    "                    inner_js = json.loads(_clean_json(r_str))\n",
    "                    inner_c, inner_r = _normalize_from_js(inner_js)\n",
    "                    if inner_c and inner_r:\n",
    "                        return inner_c, inner_r\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Normal case: classification + plain rationale\n",
    "        if c in valid_labels and isinstance(r, str) and r.strip():\n",
    "            return c, r.strip()\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    # Case 2: top-level string containing JSON\n",
    "    if isinstance(js, str):\n",
    "        txt = js.strip()\n",
    "        if len(txt) >= 2 and txt[0] == txt[-1] == '\"':\n",
    "            txt = txt[1:-1].strip()\n",
    "        if txt.startswith(\"{\") and \"classification\" in txt and \"rationale\" in txt:\n",
    "            try:\n",
    "                inner_js = json.loads(_clean_json(txt))\n",
    "                return _normalize_from_js(inner_js)\n",
    "            except Exception:\n",
    "                return None, None\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def _extract_rationale_between_markers(s: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Heuristic rationale extractor that ignores JSON validity.\n",
    "\n",
    "    Strategy:\n",
    "      - Find `\"rationale\"` key\n",
    "      - From the colon, find the first `\"` that starts the string\n",
    "      - Then take everything up to the LAST `\"` before the closing `}`\n",
    "\n",
    "    This allows inner unescaped \" characters inside the rationale, since\n",
    "    we are not trying to parse the JSON string literally.\n",
    "    \"\"\"\n",
    "    s_lower = s.lower()\n",
    "    key_idx = s_lower.find('\"rationale\"')\n",
    "    if key_idx == -1:\n",
    "        return None\n",
    "\n",
    "    colon_idx = s.find(\":\", key_idx)\n",
    "    if colon_idx == -1:\n",
    "        return None\n",
    "\n",
    "    # First quote after the colon = start of rationale string\n",
    "    first_quote = s.find('\"', colon_idx)\n",
    "    if first_quote == -1:\n",
    "        return None\n",
    "\n",
    "    # Try to limit search to this JSON object: last \" before the last }\n",
    "    last_brace = s.rfind(\"}\")\n",
    "    if last_brace == -1:\n",
    "        search_end = len(s)\n",
    "    else:\n",
    "        search_end = last_brace\n",
    "\n",
    "    last_quote = s.rfind('\"', first_quote + 1, search_end)\n",
    "    if last_quote == -1:\n",
    "        # Fallback: last quote in the whole string\n",
    "        last_quote = s.rfind('\"', first_quote + 1)\n",
    "        if last_quote == -1:\n",
    "            return None\n",
    "\n",
    "    rationale = s[first_quote + 1:last_quote]\n",
    "\n",
    "    # Simple cleanup of common escape sequences\n",
    "    rationale = rationale.replace('\\\\\"', '\"').replace(\"\\\\n\", \" \").replace(\"\\\\r\", \" \")\n",
    "    rationale = rationale.strip()\n",
    "    return rationale or None\n",
    "\n",
    "def _extract_classification_and_rationale(raw_text: str) -> Tuple[Optional[str], Optional[str], str]:\n",
    "    \"\"\"\n",
    "    Extract classification and rationale from Claude output.\n",
    "\n",
    "    New behavior:\n",
    "      - Classification is read via regex from \"classification\" / \"label\".\n",
    "      - Rationale is extracted heuristically as everything between\n",
    "        `\"rationale\": \"` and the closing `}` of the JSON object, so it\n",
    "        is not cut off by inner unescaped double quotes.\n",
    "\n",
    "    Fallback:\n",
    "      - If this fails, we still try a simple json.loads() on the cleaned\n",
    "        text and read classification / rationale from there.\n",
    "    \"\"\"\n",
    "    if not raw_text or not str(raw_text).strip():\n",
    "        return None, None, \"empty_response\"\n",
    "\n",
    "    # Normalize markdown fences, newlines, smart quotes, etc.\n",
    "    s = _clean_json(str(raw_text).strip())\n",
    "    valid_labels = {\"Positive\", \"Neutral\", \"Negative\"}\n",
    "\n",
    "    # ---- 1) Classification via regex (safe; label string has no inner quotes) ----\n",
    "    label_match = re.search(\n",
    "        r'\"(?:classification|label)\"\\s*:\\s*\"(?P<label>Positive|Neutral|Negative)\"',\n",
    "        s,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "    label: Optional[str] = None\n",
    "    if label_match:\n",
    "        label = label_match.group(\"label\").capitalize()\n",
    "        if label not in valid_labels:\n",
    "            label = None\n",
    "\n",
    "    # ---- 2) Rationale via heuristic between \"rationale\" and closing brace ----\n",
    "    rationale = _extract_rationale_between_markers(s)\n",
    "\n",
    "    if label and rationale:\n",
    "        return label, rationale, \"ok\"\n",
    "\n",
    "    # ---- 3) Simple JSON fallback (for weird but valid JSON cases) ----\n",
    "    try:\n",
    "        js = json.loads(s)\n",
    "        c = js.get(\"classification\") or js.get(\"label\")\n",
    "        r = js.get(\"rationale\")\n",
    "        if c in valid_labels and isinstance(r, str) and r.strip():\n",
    "            return c, r.strip(), \"ok\"\n",
    "        return None, None, \"bad_keys_or_values\"\n",
    "    except Exception:\n",
    "        return None, None, \"json_parse_failed\"\n",
    "\n",
    "\n",
    "def _describe_error(status: str) -> str:\n",
    "    mapping = {\n",
    "        \"too_long\": \"too_long: prompt exceeded maximum context tokens\",\n",
    "        \"json_parse_failed\": \"json_parse_failed: could not parse LLM output as JSON\",\n",
    "        \"bad_keys_or_values\": \"bad_keys_or_values: missing or invalid classification/rationale in JSON\",\n",
    "        \"api_error_throttled\": \"api_error_throttled: Bedrock throttled the request\",\n",
    "        \"api_error\": \"api_error: generic AWS/Bedrock client or service error\",\n",
    "        \"api_response_error\": \"api_response_error: malformed or unexpected API response\",\n",
    "        \"empty_response\": \"empty_response: model returned empty text\",\n",
    "        \"missing_snippets\": \"missing_snippets: no snippets available on the relation (snippet_1..N).\",\n",
    "        \"ok\": \"ok: successful classification\",\n",
    "    }\n",
    "    return mapping.get(status, status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37ff09-40ff-4e03-9024-2fb72d93e057",
   "metadata": {},
   "source": [
    "## LLM Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da923b8-fce7-446b-8a35-1e6320b7a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Per-model classify functions\n",
    "# =========================\n",
    "def classify_with_bedrock_claude(\n",
    "    *,\n",
    "    citing_case_name: str,\n",
    "    citing_case_summary: str,\n",
    "    cited_case_name: str,\n",
    "    cited_case_citation: str,\n",
    "    cited_case_summary: str,\n",
    "    snippet_block_labeled: str,\n",
    "    max_new_tokens: int = 512,\n",
    "    retries: int = 3,\n",
    "    echo: bool = False,\n",
    ") -> Tuple[\n",
    "    Optional[str], Optional[str], str, str, int,\n",
    "    List[Dict[str, Any]], str\n",
    "]:\n",
    "    \"\"\"\n",
    "    Claude judge.\n",
    "    Returns classification, rationale, status, raw_output, attempt_used, attempt_logs, user_input.\n",
    "    status: \"ok\", \"json_parse_failed\", \"bad_keys_or_values\", \"too_long\",\n",
    "            \"api_error\", \"api_error_throttled\", \"api_response_error\", \"empty_response\"\n",
    "    \"\"\"\n",
    "    client = _bedrock()\n",
    "\n",
    "    user = USER_PROMPT_TMPL.format(\n",
    "        citing_case_name=(citing_case_name or \"\").strip(),\n",
    "        citing_case_summary=(citing_case_summary or \"\").strip(),\n",
    "        cited_case_name=(cited_case_name or \"\").strip(),\n",
    "        cited_case_citation=(cited_case_citation or \"\").strip(),\n",
    "        cited_case_summary=(cited_case_summary or \"\").strip(),\n",
    "        snippet_block=snippet_block_labeled,\n",
    "    )\n",
    "\n",
    "    attempt_logs: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Token budget\n",
    "    max_in = _max_ctx_tokens_for_bedrock(max_new_tokens)\n",
    "    used = _count_tokens(SYSTEM_PROMPT + \"\\n\\n\" + user)\n",
    "\n",
    "    if used > max_in:\n",
    "        overage = used - max_in\n",
    "        snippet_tokens = _count_tokens(snippet_block_labeled)\n",
    "        target_snippet_tokens = max(512, snippet_tokens - int(overage * 1.2))\n",
    "        trimmed = _trim_to_tokens(snippet_block_labeled, target_snippet_tokens)\n",
    "\n",
    "        user = USER_PROMPT_TMPL.format(\n",
    "            citing_case_name=(citing_case_name or \"\").strip(),\n",
    "            citing_case_summary=(citing_case_summary or \"\").strip(),\n",
    "            cited_case_name=(cited_case_name or \"\").strip(),\n",
    "            cited_case_citation=(cited_case_citation or \"\").strip(),\n",
    "            cited_case_summary=(cited_case_summary or \"\").strip(),\n",
    "            snippet_block=trimmed,\n",
    "        )\n",
    "        used = _count_tokens(SYSTEM_PROMPT + \"\\n\\n\" + user)\n",
    "        if used > max_in:\n",
    "            if echo:\n",
    "                print(f\"      · [Claude] prompt still too long after trim ({used} > {max_in}); giving up\")\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": 1,\n",
    "                \"status\": \"too_long\",\n",
    "                \"raw\": \"\",\n",
    "                \"input\": user,\n",
    "            })\n",
    "            return None, None, \"too_long\", \"\", 1, attempt_logs, user\n",
    "\n",
    "    last_err = \"json_parse_failed\"\n",
    "    last_raw = \"\"\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        if attempt == 1:\n",
    "            sys = SYSTEM_PROMPT\n",
    "        elif attempt == 2:\n",
    "            sys = SYSTEM_PROMPT + \"\\n\\nSTRICT: Output JSON only. No prose, no backticks.\"\n",
    "        else:\n",
    "            sys = SYSTEM_PROMPT + '\\n\\nSTRICT: Output ONLY a JSON object with keys \"classification\" and \"rationale\". Use double quotes. No trailing commas. No markdown.'\n",
    "\n",
    "        if echo:\n",
    "            print(f\"      · [Claude] attempt {attempt}/{retries}\")\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        try:\n",
    "            resp = client.invoke_model(\n",
    "                modelId=CLAUDE_MODEL_ID,\n",
    "                body=json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": max_new_tokens,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"system\": sys,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": user}\n",
    "                    ],\n",
    "                }),\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "            )\n",
    "\n",
    "            raw_body = resp[\"body\"].read()\n",
    "            try:\n",
    "                body = json.loads(raw_body)\n",
    "            except Exception as e:\n",
    "                if echo:\n",
    "                    print(f\"      · [Claude] failed to decode response JSON: {e}\")\n",
    "                last_err = \"api_response_error\"\n",
    "                last_raw = raw_body.decode(\"utf-8\", errors=\"replace\") if isinstance(raw_body, (bytes, bytearray)) else str(raw_body)\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": last_err,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": user,\n",
    "                })\n",
    "                time.sleep(1.0 * attempt)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                text = body[\"content\"][0][\"text\"]\n",
    "            except (KeyError, IndexError, TypeError) as e:\n",
    "                if echo:\n",
    "                    print(f\"      · [Claude] unexpected response structure: {e}\")\n",
    "                last_err = \"api_response_error\"\n",
    "                last_raw = json.dumps(body)\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": last_err,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": user,\n",
    "                })\n",
    "                time.sleep(1.0 * attempt)\n",
    "                continue\n",
    "\n",
    "            last_raw = (text or \"\").strip()\n",
    "            if not last_raw:\n",
    "                if echo:\n",
    "                    print(\"      · [Claude] empty response from model\")\n",
    "                status = \"empty_response\"\n",
    "                last_err = status\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": status,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": user,\n",
    "                })\n",
    "                time.sleep(1.0 * attempt)\n",
    "                continue\n",
    "\n",
    "            c, r, status = _extract_classification_and_rationale(last_raw)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": status,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "            if status == \"ok\" and c and r:\n",
    "                return c, r, \"ok\", last_raw, attempt, attempt_logs, user\n",
    "\n",
    "            last_err = status\n",
    "            time.sleep(1.0 * attempt)\n",
    "            continue\n",
    "\n",
    "        except ClientError as e:\n",
    "            code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
    "            msg = e.response.get(\"Error\", {}).get(\"Message\", \"\")\n",
    "            if echo:\n",
    "                print(f\"      · [Claude] API ClientError on attempt {attempt}/{retries}: {code} - {msg}\")\n",
    "\n",
    "            msg_lower = msg.lower() if isinstance(msg, str) else \"\"\n",
    "            if \"token\" in msg_lower and (\"length\" in msg_lower or \"limit\" in msg_lower):\n",
    "                last_err = \"too_long\"\n",
    "            elif code in (\"ThrottlingException\", \"TooManyRequestsException\") or \"rate exceeded\" in msg_lower:\n",
    "                last_err = \"api_error_throttled\"\n",
    "            else:\n",
    "                last_err = \"api_error\"\n",
    "\n",
    "            last_raw = msg\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "\n",
    "            if last_err == \"api_error_throttled\":\n",
    "                backoff = min(60.0, float(2 ** attempt))\n",
    "                jitter = random.uniform(0.0, 0.5)\n",
    "                if echo:\n",
    "                    print(f\"      · [Claude] throttled, backing off for {backoff + jitter:.2f} seconds\")\n",
    "                time.sleep(backoff + jitter)\n",
    "            else:\n",
    "                time.sleep(1.5 * attempt)\n",
    "            continue\n",
    "\n",
    "        except BotoCoreError as e:\n",
    "            if echo:\n",
    "                print(f\"      · [Claude] BotoCoreError on attempt {attempt}/{retries}: {e}\")\n",
    "            last_err = \"api_error\"\n",
    "            last_raw = str(e)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "            time.sleep(1.0 * attempt)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            if echo:\n",
    "                print(f\"      · [Claude] API error on attempt {attempt}/{retries}: {e}\")\n",
    "            last_err = \"api_error\"\n",
    "            last_raw = str(e)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "            time.sleep(1.0 * attempt)\n",
    "            continue\n",
    "\n",
    "    return None, None, last_err, last_raw, attempt, attempt_logs, user\n",
    "\n",
    "\n",
    "def classify_with_bedrock_mistral(\n",
    "    *,\n",
    "    citing_case_name: str,\n",
    "    citing_case_summary: str,\n",
    "    cited_case_name: str,\n",
    "    cited_case_citation: str,\n",
    "    cited_case_summary: str,\n",
    "    snippet_block_labeled: str,\n",
    "    max_new_tokens: int = 320,\n",
    "    retries: int = 3,\n",
    "    echo: bool = False,\n",
    ") -> Tuple[\n",
    "    Optional[str], Optional[str], str, str, int,\n",
    "    List[Dict[str, Any]], str\n",
    "]:\n",
    "    \"\"\"\n",
    "    Mistral judge (text-generation format).\n",
    "    \"\"\"\n",
    "    client = _bedrock()\n",
    "\n",
    "    user = USER_PROMPT_TMPL.format(\n",
    "        citing_case_name=(citing_case_name or \"\").strip(),\n",
    "        citing_case_summary=(citing_case_summary or \"\").strip(),\n",
    "        cited_case_name=(cited_case_name or \"\").strip(),\n",
    "        cited_case_citation=(cited_case_citation or \"\").strip(),\n",
    "        cited_case_summary=(cited_case_summary or \"\").strip(),\n",
    "        snippet_block=snippet_block_labeled,\n",
    "    )\n",
    "\n",
    "    attempt_logs: List[Dict[str, Any]] = []\n",
    "\n",
    "    prompt = _inst(SYSTEM_PROMPT, user)\n",
    "    max_in = _max_ctx_tokens_for_bedrock(max_new_tokens)\n",
    "    used = _count_tokens(prompt)\n",
    "\n",
    "    if used > max_in:\n",
    "        empty_prompt = USER_PROMPT_TMPL.format(\n",
    "            citing_case_name=\"\",\n",
    "            citing_case_summary=\"\",\n",
    "            cited_case_name=\"\",\n",
    "            cited_case_citation=\"\",\n",
    "            cited_case_summary=\"\",\n",
    "            snippet_block=\"\",\n",
    "        )\n",
    "        allowance_for_user = max_in - _count_tokens(_inst(SYSTEM_PROMPT, empty_prompt))\n",
    "        allowance_for_user = max(512, allowance_for_user)\n",
    "        trimmed = _trim_to_tokens(snippet_block_labeled, allowance_for_user)\n",
    "        user = USER_PROMPT_TMPL.format(\n",
    "            citing_case_name=(citing_case_name or \"\").strip(),\n",
    "            citing_case_summary=(citing_case_summary or \"\").strip(),\n",
    "            cited_case_name=(cited_case_name or \"\").strip(),\n",
    "            cited_case_citation=(cited_case_citation or \"\").strip(),\n",
    "            cited_case_summary=(cited_case_summary or \"\").strip(),\n",
    "            snippet_block=trimmed,\n",
    "        )\n",
    "        prompt = _inst(SYSTEM_PROMPT, user)\n",
    "        used = _count_tokens(prompt)\n",
    "        if used > max_in:\n",
    "            if echo:\n",
    "                print(f\"      · [Mistral] prompt still too long after trim ({used} > {max_in}); giving up\")\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": 1,\n",
    "                \"status\": \"too_long\",\n",
    "                \"raw\": \"\",\n",
    "                \"input\": prompt,\n",
    "            })\n",
    "            return None, None, \"too_long\", \"\", 1, attempt_logs, user\n",
    "\n",
    "    base_payload = {\n",
    "        \"max_tokens\": max_new_tokens,\n",
    "        \"temperature\": 0,\n",
    "    }\n",
    "\n",
    "    last_err = \"json_parse_failed\"\n",
    "    last_raw = \"\"\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        if attempt == 1:\n",
    "            sys = SYSTEM_PROMPT\n",
    "        elif attempt == 2:\n",
    "            sys = SYSTEM_PROMPT + \"\\n\\nSTRICT: Output JSON only. No prose, no backticks.\"\n",
    "        else:\n",
    "            sys = SYSTEM_PROMPT + \"\\n\\nSTRICT: JSON OBJECT ONLY with keys exactly {'classification','rationale'}. Double quotes. No trailing commas.\"\n",
    "\n",
    "        eff_prompt = _inst(sys, user)\n",
    "\n",
    "        if echo:\n",
    "            print(f\"      · [Mistral] attempt {attempt}/{retries}\")\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        try:\n",
    "            resp = client.invoke_model(\n",
    "                modelId=MISTRAL_MODEL_ID,\n",
    "                body=json.dumps({\"prompt\": eff_prompt, **base_payload}),\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "            )\n",
    "\n",
    "            raw_body = resp[\"body\"].read()\n",
    "            try:\n",
    "                body = json.loads(raw_body)\n",
    "            except Exception as e:\n",
    "                if echo:\n",
    "                    print(f\"      · [Mistral] failed to decode response JSON: {e}\")\n",
    "                last_err = \"api_response_error\"\n",
    "                last_raw = raw_body.decode(\"utf-8\", errors=\"replace\") if isinstance(raw_body, (bytes, bytearray)) else str(raw_body)\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": last_err,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": eff_prompt,\n",
    "                })\n",
    "                time.sleep(1.0 * attempt)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                text = (body.get(\"outputs\") or [{}])[0].get(\"text\", \"\")\n",
    "            except Exception as e:\n",
    "                if echo:\n",
    "                    print(f\"      · [Mistral] unexpected response structure: {e}\")\n",
    "                last_err = \"api_response_error\"\n",
    "                last_raw = json.dumps(body)\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": last_err,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": eff_prompt,\n",
    "                })\n",
    "                time.sleep(1.0 * attempt)\n",
    "                continue\n",
    "\n",
    "            last_raw = (text or \"\").strip()\n",
    "            if not last_raw:\n",
    "                if echo:\n",
    "                    print(\"      · [Mistral] empty response from model\")\n",
    "                status = \"empty_response\"\n",
    "                last_err = status\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": status,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": eff_prompt,\n",
    "                })\n",
    "                time.sleep(1.0 * attempt)\n",
    "                continue\n",
    "\n",
    "            c, r, status = _extract_classification_and_rationale(last_raw)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": status,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": eff_prompt,\n",
    "            })\n",
    "            if status == \"ok\" and c and r:\n",
    "                return c, r, \"ok\", last_raw, attempt, attempt_logs, user\n",
    "\n",
    "            last_err = status\n",
    "            time.sleep(1.0 * attempt)\n",
    "            continue\n",
    "\n",
    "        except ClientError as e:\n",
    "            code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
    "            msg = e.response.get(\"Error\", {}).get(\"Message\", \"\")\n",
    "            if echo:\n",
    "                print(f\"      · [Mistral] API ClientError on attempt {attempt}/{retries}: {code} - {msg}\")\n",
    "\n",
    "            msg_lower = msg.lower() if isinstance(msg, str) else \"\"\n",
    "            if \"token\" in msg_lower and (\"length\" in msg_lower or \"limit\" in msg_lower):\n",
    "                last_err = \"too_long\"\n",
    "            elif code in (\"ThrottlingException\", \"TooManyRequestsException\") or \"rate exceeded\" in msg_lower:\n",
    "                last_err = \"api_error_throttled\"\n",
    "            else:\n",
    "                last_err = \"api_error\"\n",
    "\n",
    "            last_raw = msg\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": eff_prompt,\n",
    "            })\n",
    "\n",
    "            if last_err == \"api_error_throttled\":\n",
    "                backoff = min(60.0, float(2 ** attempt))\n",
    "                jitter = random.uniform(0.0, 0.5)\n",
    "                if echo:\n",
    "                    print(f\"      · [Mistral] throttled, backing off for {backoff + jitter:.2f} seconds\")\n",
    "                time.sleep(backoff + jitter)\n",
    "            else:\n",
    "                time.sleep(1.5 * attempt)\n",
    "            continue\n",
    "\n",
    "        except BotoCoreError as e:\n",
    "            if echo:\n",
    "                print(f\"      · [Mistral] BotoCoreError on attempt {attempt}/{retries}: {e}\")\n",
    "            last_err = \"api_error\"\n",
    "            last_raw = str(e)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": eff_prompt,\n",
    "            })\n",
    "            time.sleep(1.0 * attempt)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            if echo:\n",
    "                print(f\"      · [Mistral] API error on attempt {attempt}/{retries}: {e}\")\n",
    "            last_err = \"api_error\"\n",
    "            last_raw = str(e)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": eff_prompt,\n",
    "            })\n",
    "            time.sleep(1.0 * attempt)\n",
    "            continue\n",
    "\n",
    "    return None, None, last_err, last_raw, attempt, attempt_logs, user\n",
    "\n",
    "\n",
    "def classify_with_bedrock_llama(\n",
    "    *,\n",
    "    citing_case_name: str,\n",
    "    citing_case_summary: str,\n",
    "    cited_case_name: str,\n",
    "    cited_case_citation: str,\n",
    "    cited_case_summary: str,\n",
    "    snippet_block_labeled: str,\n",
    "    max_new_tokens: int = 512,\n",
    "    retries: int = 3,\n",
    "    echo: bool = False,\n",
    ") -> Tuple[\n",
    "    Optional[str], Optional[str], str, str, int,\n",
    "    List[Dict[str, Any]], str\n",
    "]:\n",
    "    \"\"\"\n",
    "    Llama 3 70B judge (Bedrock).\n",
    "    \"\"\"\n",
    "    client = _bedrock()\n",
    "\n",
    "    user = USER_PROMPT_TMPL.format(\n",
    "        citing_case_name=(citing_case_name or \"\").strip(),\n",
    "        citing_case_summary=(citing_case_summary or \"\").strip(),\n",
    "        cited_case_name=(cited_case_name or \"\").strip(),\n",
    "        cited_case_citation=(cited_case_citation or \"\").strip(),\n",
    "        cited_case_summary=(cited_case_summary or \"\").strip(),\n",
    "        snippet_block=snippet_block_labeled,\n",
    "    )\n",
    "\n",
    "    attempt_logs: List[Dict[str, Any]] = []\n",
    "\n",
    "    base_prompt = _llama_prompt(SYSTEM_PROMPT, user)\n",
    "    max_in = _max_ctx_tokens_for_bedrock(max_new_tokens)\n",
    "    used = _count_tokens(base_prompt)\n",
    "\n",
    "    if used > max_in:\n",
    "        empty_user = USER_PROMPT_TMPL.format(\n",
    "            citing_case_name=\"\", citing_case_summary=\"\",\n",
    "            cited_case_name=\"\", cited_case_citation=\"\", cited_case_summary=\"\",\n",
    "            snippet_block=\"\",\n",
    "        )\n",
    "        overhead_tokens = _count_tokens(_llama_prompt(SYSTEM_PROMPT, empty_user))\n",
    "        allowance_for_user = max(512, max_in - overhead_tokens)\n",
    "\n",
    "        snippet_tokens = _count_tokens(snippet_block_labeled)\n",
    "        target_snippet_tokens = max(128, min(snippet_tokens, allowance_for_user))\n",
    "        trimmed = _trim_to_tokens(snippet_block_labeled, target_snippet_tokens)\n",
    "\n",
    "        user = USER_PROMPT_TMPL.format(\n",
    "            citing_case_name=(citing_case_name or \"\").strip(),\n",
    "            citing_case_summary=(citing_case_summary or \"\").strip(),\n",
    "            cited_case_name=(cited_case_name or \"\").strip(),\n",
    "            cited_case_citation=(cited_case_citation or \"\").strip(),\n",
    "            cited_case_summary=(cited_case_summary or \"\").strip(),\n",
    "            snippet_block=trimmed,\n",
    "        )\n",
    "        base_prompt = _llama_prompt(SYSTEM_PROMPT, user)\n",
    "        used = _count_tokens(base_prompt)\n",
    "        if used > max_in:\n",
    "            if echo:\n",
    "                print(f\"      · [Llama] prompt still too long after trim ({used} > {max_in}); giving up\")\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": 1,\n",
    "                \"status\": \"too_long\",\n",
    "                \"raw\": \"\",\n",
    "                \"input\": user,\n",
    "            })\n",
    "            return None, None, \"too_long\", \"\", 1, attempt_logs, user\n",
    "\n",
    "    last_err = \"json_parse_failed\"\n",
    "    last_raw = \"\"\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        if attempt == 1:\n",
    "            sys = SYSTEM_PROMPT\n",
    "        elif attempt == 2:\n",
    "            sys = SYSTEM_PROMPT + \"\\n\\nSTRICT: Output JSON only. No prose, no backticks.\"\n",
    "        else:\n",
    "            sys = SYSTEM_PROMPT + '\\n\\nSTRICT: Output ONLY a JSON object with keys \"classification\" and \"rationale\". Use double quotes. No trailing commas. No markdown.'\n",
    "\n",
    "        prompt = _llama_prompt(sys, user)\n",
    "\n",
    "        if echo:\n",
    "            print(f\"      · [Llama] attempt {attempt}/{retries}\")\n",
    "\n",
    "        time.sleep(0.75)\n",
    "\n",
    "        try:\n",
    "            resp = client.invoke_model(\n",
    "                modelId=LLAMA_MODEL_ID,\n",
    "                body=json.dumps({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"max_gen_len\": max_new_tokens,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"top_p\": 0.9,\n",
    "                }),\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "            )\n",
    "\n",
    "            raw_body = resp[\"body\"].read()\n",
    "            try:\n",
    "                body = json.loads(raw_body)\n",
    "            except Exception as e:\n",
    "                if echo:\n",
    "                    print(f\"      · [Llama] failed to decode response JSON: {e}\")\n",
    "                last_err = \"api_response_error\"\n",
    "                last_raw = raw_body.decode(\"utf-8\", errors=\"replace\") if isinstance(raw_body, (bytes, bytearray)) else str(raw_body)\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": last_err,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": user,\n",
    "                })\n",
    "                time.sleep(1.5 * attempt)\n",
    "                continue\n",
    "\n",
    "            text = \"\"\n",
    "            if isinstance(body, dict):\n",
    "                if isinstance(body.get(\"generation\"), str):\n",
    "                    text = body[\"generation\"]\n",
    "                elif isinstance(body.get(\"output_text\"), str):\n",
    "                    text = body[\"output_text\"]\n",
    "                elif isinstance(body.get(\"outputs\"), list) and body[\"outputs\"]:\n",
    "                    first = body[\"outputs\"][0]\n",
    "                    if isinstance(first, dict):\n",
    "                        text = first.get(\"text\") or first.get(\"output_text\", \"\") or \"\"\n",
    "                    elif isinstance(first, str):\n",
    "                        text = first\n",
    "                elif isinstance(body.get(\"output\"), str):\n",
    "                    text = body[\"output\"]\n",
    "\n",
    "            last_raw = (text or \"\").strip()\n",
    "            if not last_raw:\n",
    "                if echo:\n",
    "                    print(\"      · [Llama] empty response from model\")\n",
    "                status = \"empty_response\"\n",
    "                last_err = status\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": status,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": user,\n",
    "                })\n",
    "                time.sleep(1.5 * attempt)\n",
    "                continue\n",
    "\n",
    "            c, r, status = _extract_classification_and_rationale(last_raw)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": status,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "            if status == \"ok\" and c and r:\n",
    "                return c, r, \"ok\", last_raw, attempt, attempt_logs, user\n",
    "\n",
    "            last_err = status\n",
    "            time.sleep(1.5 * attempt)\n",
    "            continue\n",
    "\n",
    "        except ClientError as e:\n",
    "            code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
    "            msg = e.response.get(\"Error\", {}).get(\"Message\", \"\")\n",
    "            if echo:\n",
    "                print(f\"      · [Llama] API ClientError on attempt {attempt}/{retries}: {code} - {msg}\")\n",
    "\n",
    "            msg_lower = msg.lower() if isinstance(msg, str) else \"\"\n",
    "            if \"token\" in msg_lower and (\"length\" in msg_lower or \"limit\" in msg_lower):\n",
    "                last_err = \"too_long\"\n",
    "            elif code in (\"ThrottlingException\", \"TooManyRequestsException\") or \"rate exceeded\" in msg_lower:\n",
    "                last_err = \"api_error_throttled\"\n",
    "            else:\n",
    "                last_err = \"api_error\"\n",
    "\n",
    "            last_raw = msg\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "\n",
    "            if last_err == \"api_error_throttled\":\n",
    "                backoff = min(60.0, float(2 ** attempt))\n",
    "                jitter = random.uniform(0.0, 0.5)\n",
    "                if echo:\n",
    "                    print(f\"      · [Llama] throttled, backing off for {backoff + jitter:.2f} seconds\")\n",
    "                time.sleep(backoff + jitter)\n",
    "            else:\n",
    "                time.sleep(2.0 * attempt)\n",
    "            continue\n",
    "\n",
    "        except BotoCoreError as e:\n",
    "            if echo:\n",
    "                print(f\"      · [Llama] BotoCoreError on attempt {attempt}/{retries}: {e}\")\n",
    "            last_err = \"api_error\"\n",
    "            last_raw = str(e)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "            time.sleep(1.5 * attempt)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            if echo:\n",
    "                print(f\"      · [Llama] API error on attempt {attempt}/{retries}: {e}\")\n",
    "            last_err = \"api_error\"\n",
    "            last_raw = str(e)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "            time.sleep(1.5 * attempt)\n",
    "            continue\n",
    "\n",
    "    return None, None, last_err, last_raw, attempt, attempt_logs, user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e41c69-dd79-45a2-a068-637318e03d4e",
   "metadata": {},
   "source": [
    "## Full-dataset fetch for CSV Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c59f042-8ec3-4347-9783-a3ce9abfa89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Full-dataset fetch for CSV outputs\n",
    "# =========================\n",
    "def _fetch_all_labeled_results(driver) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all currently labeled CITES_TO edges from Neo4j\n",
    "    and shape them like df_results, with per-model columns.\n",
    "    \"\"\"\n",
    "    with driver.session(database=NEO4J_DATABASE) as s_all:\n",
    "        data = s_all.run(Q_ALL_LABELED_REL, {}).data()\n",
    "\n",
    "    rows_all: List[Dict[str, Any]] = []\n",
    "    for row in data:\n",
    "        rows_all.append({\n",
    "            \"Source Case ID\": row[\"src_id\"],\n",
    "            \"Source Case Name\": row[\"src_name\"] or \"\",\n",
    "            \"Source Case Decision Date\": row.get(\"src_date\") or \"\",\n",
    "            \"Source Case citation_pipe\": row.get(\"src_cite\") or \"\",\n",
    "            \"Source Case Summary\": row.get(\"src_summary\") or \"\",\n",
    "            \"Target Case ID\": row[\"tgt_id\"],\n",
    "            \"Target Case Name\": row[\"tgt_name\"] or \"\",\n",
    "            \"Target Case Decision Date\": row.get(\"tgt_date\") or \"\",\n",
    "            \"Target Case citation_pipe\": row.get(\"tgt_cite\") or \"\",\n",
    "            \"Target Case Summary\": row.get(\"tgt_summary\") or \"\",\n",
    "            \"Opinion Snippet\": row.get(\"snippet_joined\") or \"\",\n",
    "            \"Mistral Citation Evaluation\": row.get(\"mistral_label\") or \"\",\n",
    "            \"LLama Citation Evaluation\": row.get(\"llama_label\") or \"\",\n",
    "            \"Claude Citation Evaluation\": row.get(\"claude_label\") or \"\",\n",
    "            \"Global Citation Evaluation\": row.get(\"treatment_label\") or \"\",\n",
    "            \"Mistral Rationale\": row.get(\"mistral_rationale\") or \"\",\n",
    "            \"LLama Rationale\": row.get(\"llama_rationale\") or \"\",\n",
    "            \"Claude Rationale\": row.get(\"claude_rationale\") or \"\",\n",
    "            \"Global Rationale\": row.get(\"treatment_rationale\") or \"\",\n",
    "            COL_URL_HEADER: row.get(\"src_url\") or \"\",\n",
    "        })\n",
    "    return pd.DataFrame(rows_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d155f9-5d20-47f6-9bc7-c1b22e0cc051",
   "metadata": {},
   "source": [
    "## Majority vote helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9078808-ecc9-42b4-b0f1-151efb569827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Majority vote helper\n",
    "# =========================\n",
    "def _compute_final_label(m_label: str, l_label: str, c_label: str) -> str:\n",
    "    labels = [m_label, l_label, c_label]\n",
    "    cnt = Counter(labels)\n",
    "    majority_label, majority_count = cnt.most_common(1)[0]\n",
    "\n",
    "    # If there is a strict majority (2 or 3 of the same), return that value\n",
    "    if majority_count >= 2:\n",
    "        return majority_label\n",
    "\n",
    "    # No majority: all three different\n",
    "    if \"Unknown\" in cnt:\n",
    "        # One model Unknown and other two disagree -> Unknown\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # All three different and none Unknown -> Neutral by rule\n",
    "    return \"Neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46acbe78-2ece-42f6-9989-19c6651ce905",
   "metadata": {},
   "source": [
    "## Batch Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89cb1f62-f95f-4bdb-9358-ec0ba9d3a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Batch driver\n",
    "# =========================\n",
    "def label_all_citations(\n",
    "    *,\n",
    "    results_csv: bool = False,\n",
    "    results_csv_filename: str = \"edge_classifications_ensemble.csv\",\n",
    "    batch_size: int = 200,\n",
    "    echo: bool = True,\n",
    "    force: bool = False,\n",
    "    append_to_labeled_dataset_csv: Optional[str] = None,\n",
    "    labeled_output_csv: Optional[str] = None,   # default = \"<labeled_csv_stem> - model comparison.csv\"\n",
    "    show_all_labels_in_output_csv: bool = False,\n",
    "    failed_csv: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ensemble Edge Classifier (3 judges: Mistral, Llama, Claude).\n",
    "\n",
    "    For each CITES_TO relation:\n",
    "      - Run all three models.\n",
    "      - Store per-model label/rationale.\n",
    "      - Compute final treatment_label by majority vote with the rules given.\n",
    "      - Build treatment_rationale explaining the decision and including\n",
    "        majority judges' rationales (or all three when needed).\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "    # Session with DB name; disable notifications where supported\n",
    "    try:\n",
    "        session = driver.session(database=NEO4J_DATABASE, notifications_min_severity=\"OFF\")\n",
    "    except TypeError:\n",
    "        session = driver.session(database=NEO4J_DATABASE)\n",
    "\n",
    "    rows_out: List[Dict[str, Any]] = []\n",
    "    failed_rows: List[Dict[str, Any]] = []\n",
    "    after = -1\n",
    "    processed = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    # diagnostics (global)\n",
    "    ok_rel = 0\n",
    "    missing_snippets = 0\n",
    "\n",
    "    # classification tallies (global final label)\n",
    "    pos_cnt = neu_cnt = neg_cnt = unk_cnt = 0\n",
    "\n",
    "    # per-model error counters (optional diagnostics)\n",
    "    model_error_counts = {\n",
    "        \"Mistral\": {\"too_long\": 0, \"json_parse_failed\": 0, \"bad_keys_or_values\": 0, \"api_error\": 0, \"other\": 0},\n",
    "        \"Llama\":   {\"too_long\": 0, \"json_parse_failed\": 0, \"bad_keys_or_values\": 0, \"api_error\": 0, \"other\": 0},\n",
    "        \"Claude\":  {\"too_long\": 0, \"json_parse_failed\": 0, \"bad_keys_or_values\": 0, \"api_error\": 0, \"other\": 0},\n",
    "    }\n",
    "\n",
    "    # First 11 columns for failed_citations.csv\n",
    "    first_11_cols = [\n",
    "        \"Source Case ID\",\n",
    "        \"Source Case Name\",\n",
    "        \"Source Case Decision Date\",\n",
    "        \"Source Case citation_pipe\",\n",
    "        \"Source Case Summary\",\n",
    "        \"Target Case ID\",\n",
    "        \"Target Case Name\",\n",
    "        \"Target Case Decision Date\",\n",
    "        \"Target Case citation_pipe\",\n",
    "        \"Target Case Summary\",\n",
    "        \"Opinion Snippet\",\n",
    "    ]\n",
    "\n",
    "    with session as s:\n",
    "        while True:\n",
    "            rels = s.run(Q_PAGE_REL, {\"after_id\": after, \"limit\": batch_size}).data()\n",
    "            if not rels:\n",
    "                if echo:\n",
    "                    print(\"All done. No more relations.\")\n",
    "                break\n",
    "\n",
    "            if echo:\n",
    "                print(f\"\\nBatch after rel_id {after}: {len(rels)} relation(s)\")\n",
    "\n",
    "            for row in rels:\n",
    "                after = row[\"rel_id\"]\n",
    "\n",
    "                existing_label = (row.get(\"existing_label\") or \"\").strip()\n",
    "                if (not force) and existing_label and existing_label.lower() != \"unknown\":\n",
    "                    # already globally labeled → skip\n",
    "                    continue\n",
    "\n",
    "                src_name = row[\"src_name\"] or \"\"\n",
    "                tgt_name = row[\"tgt_name\"] or \"\"\n",
    "                src_sum  = row.get(\"src_summary\") or \"\"\n",
    "                tgt_sum  = row.get(\"tgt_summary\") or \"\"\n",
    "                tgt_cit  = _first_citation(row.get(\"tgt_cite\") or \"\")\n",
    "\n",
    "                # Gather snippets\n",
    "                snippets = _extract_numbered_snippets(row.get(\"rel_props\") or {})\n",
    "                common_row = {\n",
    "                    \"Source Case ID\": row[\"src_id\"],\n",
    "                    \"Source Case Name\": src_name,\n",
    "                    \"Source Case Decision Date\": row.get(\"src_date\") or \"\",\n",
    "                    \"Source Case citation_pipe\": row.get(\"src_cite\") or \"\",\n",
    "                    \"Source Case Summary\": src_sum,\n",
    "                    \"Target Case ID\": row[\"tgt_id\"],\n",
    "                    \"Target Case Name\": tgt_name,\n",
    "                    \"Target Case Decision Date\": row.get(\"tgt_date\") or \"\",\n",
    "                    \"Target Case citation_pipe\": row.get(\"tgt_cite\") or \"\",\n",
    "                    \"Target Case Summary\": tgt_sum,\n",
    "                }\n",
    "\n",
    "                if not snippets:\n",
    "                    # No snippets: everything Unknown and explanation\n",
    "                    missing_snippets += 1\n",
    "                    global_label = \"Unknown\"\n",
    "                    explanation = \"The final treatment is 'Unknown' because no snippets were available on the relation (snippet_1..N).\"\n",
    "\n",
    "                    mistral_label = llama_label = claude_label = \"Unknown\"\n",
    "                    mistral_rat = llama_rat = claude_rat = \"No snippets available on the relation (snippet_1..N).\"\n",
    "\n",
    "                    joined_for_store = \"\"\n",
    "                    global_rationale = (\n",
    "                        explanation\n",
    "                        + f\"\\n\\nThe rationale for the first judge ('{MISTRAL_DISPLAY_NAME}') is:\\n\\n\"\n",
    "                        + mistral_rat\n",
    "                        + f\"\\n\\nThe rationale for the second judge ('{LLAMA_DISPLAY_NAME}') is:\\n\\n\"\n",
    "                        + llama_rat\n",
    "                        + f\"\\n\\nThe rationale for the third judge ('{CLAUDE_DISPLAY_NAME}') is:\\n\\n\"\n",
    "                        + claude_rat\n",
    "                    )\n",
    "\n",
    "                    # Write to Neo4j\n",
    "                    s.run(Q_WRITE_REL_ANNOT, {\n",
    "                        \"src_id\": row[\"src_id\"],\n",
    "                        \"tgt_id\": row[\"tgt_id\"],\n",
    "                        \"mistral_label\": mistral_label,\n",
    "                        \"mistral_rationale\": mistral_rat,\n",
    "                        \"llama_label\": llama_label,\n",
    "                        \"llama_rationale\": llama_rat,\n",
    "                        \"claude_label\": claude_label,\n",
    "                        \"claude_rationale\": claude_rat,\n",
    "                        \"label\": global_label,\n",
    "                        \"rationale\": global_rationale,\n",
    "                        \"snippet_joined\": joined_for_store,\n",
    "                        \"model_used\": [MISTRAL_DISPLAY_NAME, LLAMA_DISPLAY_NAME, CLAUDE_DISPLAY_NAME],\n",
    "                    })\n",
    "\n",
    "                    unk_cnt += 1\n",
    "                    if echo:\n",
    "                        print(f\"{src_name} → {tgt_name}: Global=Unknown (missing snippets)\")\n",
    "\n",
    "                    rows_out.append({\n",
    "                        **common_row,\n",
    "                        \"Opinion Snippet\": joined_for_store,\n",
    "                        \"Mistral Citation Evaluation\": mistral_label,\n",
    "                        \"LLama Citation Evaluation\": llama_label,\n",
    "                        \"Claude Citation Evaluation\": claude_label,\n",
    "                        \"Global Citation Evaluation\": global_label,\n",
    "                        \"Mistral Rationale\": mistral_rat,\n",
    "                        \"LLama Rationale\": llama_rat,\n",
    "                        \"Claude Rationale\": claude_rat,\n",
    "                        \"Global Rationale\": global_rationale,\n",
    "                        COL_URL_HEADER: row.get(\"src_url\") or \"\",\n",
    "                    })\n",
    "\n",
    "                    # Failed row (no LLM attempts)\n",
    "                    failed_rows.append({\n",
    "                        **{k: common_row.get(k, \"\") for k in first_11_cols},\n",
    "                        \"Model Name\": \"\",\n",
    "                        \"Attempt\": 0,\n",
    "                        \"Input to LLM\": \"\",\n",
    "                        \"LLM Output\": \"\",\n",
    "                        \"Type of Error\": _describe_error(\"missing_snippets\"),\n",
    "                    })\n",
    "\n",
    "                    processed += 1\n",
    "                    continue\n",
    "\n",
    "                # Build snippet block for models\n",
    "                snippet_block = _format_snippet_block_labeled(snippets)\n",
    "                joined_for_store = \"\\n\\n\".join(snippets)\n",
    "\n",
    "                # ----- Run three judges -----\n",
    "                # Mistral\n",
    "                m_lab, m_rat, m_status, m_raw, m_attempt, m_logs, m_user_input = classify_with_bedrock_mistral(\n",
    "                    citing_case_name=src_name,\n",
    "                    citing_case_summary=src_sum,\n",
    "                    cited_case_name=tgt_name,\n",
    "                    cited_case_citation=tgt_cit,\n",
    "                    cited_case_summary=tgt_sum,\n",
    "                    snippet_block_labeled=snippet_block,\n",
    "                    max_new_tokens=340,\n",
    "                    retries=3,\n",
    "                    echo=echo,\n",
    "                )\n",
    "                if not m_lab:\n",
    "                    label_m = \"Unknown\"\n",
    "                    rat_m = f\"Classification failed: {_describe_error(m_status)}\"\n",
    "                    # bump error counters\n",
    "                    if m_status in (\"too_long\", \"json_parse_failed\", \"bad_keys_or_values\", \"api_error\", \"api_error_throttled\", \"api_response_error\", \"empty_response\"):\n",
    "                        if m_status == \"too_long\":\n",
    "                            model_error_counts[\"Mistral\"][\"too_long\"] += 1\n",
    "                        elif m_status == \"json_parse_failed\":\n",
    "                            model_error_counts[\"Mistral\"][\"json_parse_failed\"] += 1\n",
    "                        elif m_status == \"bad_keys_or_values\":\n",
    "                            model_error_counts[\"Mistral\"][\"bad_keys_or_values\"] += 1\n",
    "                        else:\n",
    "                            model_error_counts[\"Mistral\"][\"api_error\"] += 1\n",
    "                    else:\n",
    "                        model_error_counts[\"Mistral\"][\"other\"] += 1\n",
    "                else:\n",
    "                    label_m = m_lab\n",
    "                    rat_m = m_rat\n",
    "\n",
    "                # Llama\n",
    "                l_lab, l_rat, l_status, l_raw, l_attempt, l_logs, l_user_input = classify_with_bedrock_llama(\n",
    "                    citing_case_name=src_name,\n",
    "                    citing_case_summary=src_sum,\n",
    "                    cited_case_name=tgt_name,\n",
    "                    cited_case_citation=tgt_cit,\n",
    "                    cited_case_summary=tgt_sum,\n",
    "                    snippet_block_labeled=snippet_block,\n",
    "                    max_new_tokens=340,\n",
    "                    retries=3,\n",
    "                    echo=echo,\n",
    "                )\n",
    "                if not l_lab:\n",
    "                    label_l = \"Unknown\"\n",
    "                    rat_l = f\"Classification failed: {_describe_error(l_status)}\"\n",
    "                    if l_status in (\"too_long\", \"json_parse_failed\", \"bad_keys_or_values\", \"api_error\", \"api_error_throttled\", \"api_response_error\", \"empty_response\"):\n",
    "                        if l_status == \"too_long\":\n",
    "                            model_error_counts[\"Llama\"][\"too_long\"] += 1\n",
    "                        elif l_status == \"json_parse_failed\":\n",
    "                            model_error_counts[\"Llama\"][\"json_parse_failed\"] += 1\n",
    "                        elif l_status == \"bad_keys_or_values\":\n",
    "                            model_error_counts[\"Llama\"][\"bad_keys_or_values\"] += 1\n",
    "                        else:\n",
    "                            model_error_counts[\"Llama\"][\"api_error\"] += 1\n",
    "                    else:\n",
    "                        model_error_counts[\"Llama\"][\"other\"] += 1\n",
    "                else:\n",
    "                    label_l = l_lab\n",
    "                    rat_l = l_rat\n",
    "\n",
    "                # Claude\n",
    "                c_lab, c_rat, c_status, c_raw, c_attempt, c_logs, c_user_input = classify_with_bedrock_claude(\n",
    "                    citing_case_name=src_name,\n",
    "                    citing_case_summary=src_sum,\n",
    "                    cited_case_name=tgt_name,\n",
    "                    cited_case_citation=tgt_cit,\n",
    "                    cited_case_summary=tgt_sum,\n",
    "                    snippet_block_labeled=snippet_block,\n",
    "                    max_new_tokens=512,\n",
    "                    retries=3,\n",
    "                    echo=echo,\n",
    "                )\n",
    "                if not c_lab:\n",
    "                    label_c = \"Unknown\"\n",
    "                    rat_c = f\"Classification failed: {_describe_error(c_status)}\"\n",
    "                    if c_status in (\"too_long\", \"json_parse_failed\", \"bad_keys_or_values\", \"api_error\", \"api_error_throttled\", \"api_response_error\", \"empty_response\"):\n",
    "                        if c_status == \"too_long\":\n",
    "                            model_error_counts[\"Claude\"][\"too_long\"] += 1\n",
    "                        elif c_status == \"json_parse_failed\":\n",
    "                            model_error_counts[\"Claude\"][\"json_parse_failed\"] += 1\n",
    "                        elif c_status == \"bad_keys_or_values\":\n",
    "                            model_error_counts[\"Claude\"][\"bad_keys_or_values\"] += 1\n",
    "                        else:\n",
    "                            model_error_counts[\"Claude\"][\"api_error\"] += 1\n",
    "                    else:\n",
    "                        model_error_counts[\"Claude\"][\"other\"] += 1\n",
    "                else:\n",
    "                    label_c = c_lab\n",
    "                    rat_c = c_rat\n",
    "\n",
    "                # Collect attempt logs into failed_rows (for failed_csv)\n",
    "                for model_name, logs in [\n",
    "                    (MISTRAL_DISPLAY_NAME, m_logs),\n",
    "                    (LLAMA_DISPLAY_NAME,   l_logs),\n",
    "                    (CLAUDE_DISPLAY_NAME,  c_logs),\n",
    "                ]:\n",
    "                    for log in logs:\n",
    "                        status = log.get(\"status\", \"\")\n",
    "                        if status == \"ok\":\n",
    "                            continue\n",
    "                        failed_rows.append({\n",
    "                            **{k: common_row.get(k, \"\") for k in first_11_cols},\n",
    "                            \"Model Name\": model_name,\n",
    "                            \"Attempt\": log.get(\"attempt\", 0),\n",
    "                            \"Input to LLM\": log.get(\"input\", \"\"),\n",
    "                            \"LLM Output\": log.get(\"raw\", \"\"),\n",
    "                            \"Type of Error\": _describe_error(status),\n",
    "                        })\n",
    "\n",
    "                # ----- Majority vote and global rationale -----\n",
    "                final_label = _compute_final_label(label_m, label_l, label_c)\n",
    "                labels_set = {label_m, label_l, label_c}\n",
    "\n",
    "                cnt = Counter([label_m, label_l, label_c])\n",
    "                explanation = \"\"\n",
    "                if final_label == \"Unknown\":\n",
    "                    if cnt[\"Unknown\"] >= 2:\n",
    "                        explanation = \"The final treatment is 'Unknown' because at least two of the three judges returned 'Unknown' labels.\"\n",
    "                    elif \"Unknown\" in cnt and len(cnt) == 3:\n",
    "                        explanation = \"The final treatment is 'Unknown' because one judge returned 'Unknown' and the other two judges disagreed on the label.\"\n",
    "                    else:\n",
    "                        explanation = \"The final treatment is 'Unknown' based on the ensemble voting rules.\"\n",
    "                else:\n",
    "                    if len(cnt) == 3 and \"Unknown\" not in cnt and final_label == \"Neutral\":\n",
    "                        explanation = \"The final treatment is 'Neutral' because all three judges disagreed (one Positive, one Neutral, one Negative), so we fall back to 'Neutral' by rule.\"\n",
    "                    else:\n",
    "                        explanation = f\"The final treatment is '{final_label}' because it is the majority vote among the three judges.\"\n",
    "\n",
    "                # Decide which judges' rationales to include\n",
    "                judges_info = [\n",
    "                    (\"Mistral\", MISTRAL_DISPLAY_NAME, label_m, rat_m),\n",
    "                    (\"Llama\",   LLAMA_DISPLAY_NAME,   label_l, rat_l),\n",
    "                    (\"Claude\",  CLAUDE_DISPLAY_NAME,  label_c, rat_c),\n",
    "                ]\n",
    "\n",
    "                # Default: judges whose label == final_label\n",
    "                if final_label == \"Unknown\":\n",
    "                    judges_to_include = judges_info  # show all in Unknown case\n",
    "                elif len(cnt) == 3 and \"Unknown\" not in cnt and final_label == \"Neutral\":\n",
    "                    judges_to_include = judges_info  # all three disagreed → include all\n",
    "                else:\n",
    "                    judges_to_include = [j for j in judges_info if j[2] == final_label]\n",
    "\n",
    "                parts = [explanation]\n",
    "                for idx, (_, display_name, _, rat_text) in enumerate(judges_to_include, 1):\n",
    "                    parts.append(\n",
    "                        f\"\\nThe rationale for the { _ordinal_word(idx) } judge ('{display_name}') is:\\n\\n{rat_text}\"\n",
    "                    )\n",
    "                global_rationale = \"\\n\".join(parts).strip()\n",
    "\n",
    "                # Update global label counts\n",
    "                if final_label == \"Positive\":\n",
    "                    pos_cnt += 1\n",
    "                elif final_label == \"Neutral\":\n",
    "                    neu_cnt += 1\n",
    "                elif final_label == \"Negative\":\n",
    "                    neg_cnt += 1\n",
    "                else:\n",
    "                    unk_cnt += 1\n",
    "\n",
    "                # Write to Neo4j\n",
    "                s.run(Q_WRITE_REL_ANNOT, {\n",
    "                    \"src_id\": row[\"src_id\"],\n",
    "                    \"tgt_id\": row[\"tgt_id\"],\n",
    "                    \"mistral_label\": label_m,\n",
    "                    \"mistral_rationale\": rat_m,\n",
    "                    \"llama_label\": label_l,\n",
    "                    \"llama_rationale\": rat_l,\n",
    "                    \"claude_label\": label_c,\n",
    "                    \"claude_rationale\": rat_c,\n",
    "                    \"label\": final_label,\n",
    "                    \"rationale\": global_rationale,\n",
    "                    \"snippet_joined\": joined_for_store,\n",
    "                    \"model_used\": [MISTRAL_DISPLAY_NAME, LLAMA_DISPLAY_NAME, CLAUDE_DISPLAY_NAME],\n",
    "                })\n",
    "\n",
    "                ok_rel += 1 if final_label != \"Unknown\" else 0\n",
    "\n",
    "                if echo:\n",
    "                    print(\n",
    "                        f\"{src_name} → {tgt_name}: \"\n",
    "                        f\"Mistral={label_m}, Llama={label_l}, Claude={label_c} → Global={final_label}\"\n",
    "                    )\n",
    "\n",
    "                rows_out.append({\n",
    "                    **common_row,\n",
    "                    \"Opinion Snippet\": joined_for_store,\n",
    "                    \"Mistral Citation Evaluation\": label_m,\n",
    "                    \"LLama Citation Evaluation\": label_l,\n",
    "                    \"Claude Citation Evaluation\": label_c,\n",
    "                    \"Global Citation Evaluation\": final_label,\n",
    "                    \"Mistral Rationale\": rat_m,\n",
    "                    \"LLama Rationale\": rat_l,\n",
    "                    \"Claude Rationale\": rat_c,\n",
    "                    \"Global Rationale\": global_rationale,\n",
    "                    COL_URL_HEADER: row.get(\"src_url\") or \"\",\n",
    "                })\n",
    "\n",
    "                processed += 1\n",
    "\n",
    "    # Build df_results from this run\n",
    "    df_results = pd.DataFrame(rows_out)\n",
    "\n",
    "    if show_all_labels_in_output_csv:\n",
    "        df_results = _fetch_all_labeled_results(driver)\n",
    "        if echo:\n",
    "            print(f\"\\nshow_all_labels_in_output_csv=True → df_results replaced with full labeled dataset (rows: {len(df_results)})\")\n",
    "    else:\n",
    "        if echo:\n",
    "            print(f\"\\nshow_all_labels_in_output_csv=False → df_results only has rows from this run (rows: {len(df_results)})\")\n",
    "\n",
    "    # -------- Optional CSV of results --------\n",
    "    if results_csv:\n",
    "        df_results.to_csv(results_csv_filename, index=False)\n",
    "        print(f\"\\nWrote {len(df_results)} rows → {results_csv_filename}\")\n",
    "    else:\n",
    "        print(f\"\\nSkipping results CSV write (results_csv=False). Rows buffered in df_results: {len(df_results)}\")\n",
    "\n",
    "    # -------- Optional CSV of failed classifications --------\n",
    "    if failed_csv:\n",
    "        df_failed = pd.DataFrame(failed_rows)\n",
    "        if not df_failed.empty:\n",
    "            ordered_cols = first_11_cols + [\n",
    "                c for c in [\"Model Name\", \"Attempt\", \"Input to LLM\", \"LLM Output\", \"Type of Error\"]\n",
    "                if c in df_failed.columns\n",
    "            ]\n",
    "            for c in ordered_cols:\n",
    "                if c not in df_failed.columns:\n",
    "                    df_failed[c] = \"\"\n",
    "            df_failed = df_failed[ordered_cols]\n",
    "            df_failed.to_csv(\"failed_citations.csv\", index=False)\n",
    "            print(f\"Wrote {len(df_failed)} failed rows → failed_citations.csv\")\n",
    "        else:\n",
    "            print(\"No failed classifications in this run; failed_citations.csv not written.\")\n",
    "    else:\n",
    "        if echo:\n",
    "            print(f\"Skipping failed CSV write (failed_csv=False). Failed rows in memory: {len(failed_rows)}\")\n",
    "\n",
    "    # -------- Optional labeled dataset join --------\n",
    "    if append_to_labeled_dataset_csv:\n",
    "        if labeled_output_csv is None:\n",
    "            stem = pathlib.Path(append_to_labeled_dataset_csv).stem\n",
    "            labeled_output_csv = f\"{stem} - model comparison.csv\"\n",
    "\n",
    "        try:\n",
    "            df_label = _read_csv_fallback(append_to_labeled_dataset_csv)\n",
    "            print(f\"Loaded labeled dataset with fallback reader: {append_to_labeled_dataset_csv}\")\n",
    "        except Exception as e:\n",
    "            df_label = None\n",
    "            print(f\"Could not read labeled dataset CSV: {e}\")\n",
    "\n",
    "        if df_label is not None:\n",
    "            expected = [\"Source ID\", \"Source Name\", \"Target ID\", \"Target Name\", \"Chunk\", \"Label\", \"Rationale\"]\n",
    "            missing = [c for c in expected if c not in df_label.columns]\n",
    "            if missing:\n",
    "                print(f\"Labeled CSV missing required columns: {missing}. Skipping append.\")\n",
    "            else:\n",
    "                # Standardize labeled columns\n",
    "                df_label_std = df_label.rename(columns={\n",
    "                    \"Source ID\":   \"Source Case ID\",\n",
    "                    \"Target ID\":   \"Target Case ID\",\n",
    "                    \"Source Name\": \"Source Case Name (Labeled)\",\n",
    "                    \"Target Name\": \"Target Case Name (Labeled)\",\n",
    "                    # keep \"Rationale\" as-is\n",
    "                })\n",
    "\n",
    "                # Prepare join frame from current df_results with MODEL-suffixed columns\n",
    "                df_join = df_results.rename(columns={\n",
    "                    \"Source Case Name\":        \"Source Case Name (Model)\",\n",
    "                    \"Target Case Name\":        \"Target Case Name (Model)\",\n",
    "                    \"Source Case Summary\":     \"Source Case Summary (Model)\",\n",
    "                    \"Target Case Summary\":     \"Target Case Summary (Model)\",\n",
    "                    \"Opinion Snippet\":         \"Chunk (Pulled by Pipeline)\",\n",
    "                    \"Global Citation Evaluation\": \"Label (Model)\",\n",
    "                    \"Global Rationale\":           \"Rationale (Model)\",\n",
    "                })\n",
    "\n",
    "                needed_from_results = [\n",
    "                    \"Source Case ID\", \"Target Case ID\",\n",
    "                    \"Source Case Name (Model)\", \"Target Case Name (Model)\",\n",
    "                    \"Source Case Decision Date\", \"Source Case citation_pipe\",\n",
    "                    \"Source Case Summary (Model)\",\n",
    "                    \"Target Case Decision Date\", \"Target Case citation_pipe\",\n",
    "                    \"Target Case Summary (Model)\",\n",
    "                    \"Chunk (Pulled by Pipeline)\",\n",
    "                    \"Label (Model)\",\n",
    "                    \"Mistral Citation Evaluation\",\n",
    "                    \"LLama Citation Evaluation\",\n",
    "                    \"Claude Citation Evaluation\",\n",
    "                    \"Rationale (Model)\",\n",
    "                    \"Mistral Rationale\",\n",
    "                    \"LLama Rationale\",\n",
    "                    \"Claude Rationale\",\n",
    "                    COL_URL_HEADER,\n",
    "                ]\n",
    "                for c in needed_from_results:\n",
    "                    if c not in df_join.columns:\n",
    "                        df_join[c] = \"\"\n",
    "\n",
    "                merged = df_label_std.merge(\n",
    "                    df_join[needed_from_results],\n",
    "                    on=[\"Source Case ID\", \"Target Case ID\"],\n",
    "                    how=\"inner\",\n",
    "                )\n",
    "\n",
    "                # Build unified case-name columns (prefer labeled, else model)\n",
    "                def _prefer_labeled(lbl_col: str, mdl_col: str):\n",
    "                    lbl_series = merged.get(lbl_col)\n",
    "                    mdl_series = merged.get(mdl_col)\n",
    "                    if lbl_series is None and mdl_series is None:\n",
    "                        return pd.Series([], dtype=\"object\")\n",
    "                    if lbl_series is None:\n",
    "                        return mdl_series\n",
    "                    if mdl_series is None:\n",
    "                        return lbl_series\n",
    "                    lbl_clean = lbl_series.fillna(\"\").astype(str)\n",
    "                    use_lbl = ~lbl_clean.str.fullmatch(r\"\\s*\")\n",
    "                    out = mdl_series.copy()\n",
    "                    out[use_lbl] = lbl_series[use_lbl]\n",
    "                    return out\n",
    "\n",
    "                merged[\"Source Case Name\"] = _prefer_labeled(\"Source Case Name (Labeled)\", \"Source Case Name (Model)\")\n",
    "                merged[\"Target Case Name\"] = _prefer_labeled(\"Target Case Name (Labeled)\", \"Target Case Name (Model)\")\n",
    "\n",
    "                # Map per-model label/rationale columns\n",
    "                merged[\"Mistral Label\"] = merged.get(\"Mistral Citation Evaluation\", \"\")\n",
    "                merged[\"LLama Label\"]   = merged.get(\"LLama Citation Evaluation\", \"\")\n",
    "                merged[\"Claude Label\"]  = merged.get(\"Claude Citation Evaluation\", \"\")\n",
    "\n",
    "                # Final ordered columns (with renamed Rational → Rationale)\n",
    "                merged = merged.rename(columns={\n",
    "                    \"Rationale\": \"Rationale\",\n",
    "                    \"Rationale (Model)\": \"Rationale (Model)\",\n",
    "                })\n",
    "\n",
    "                final_cols = [\n",
    "                    \"Source Case ID\",\n",
    "                    \"Source Case Name\",\n",
    "                    \"Source Case Decision Date\",\n",
    "                    \"Source Case citation_pipe\",\n",
    "                    \"Source Case Summary (Model)\",\n",
    "                    \"Target Case ID\",\n",
    "                    \"Target Case Name\",\n",
    "                    \"Target Case Decision Date\",\n",
    "                    \"Target Case citation_pipe\",\n",
    "                    \"Target Case Summary (Model)\",\n",
    "                    \"Chunk\",                        # from labeled dataset\n",
    "                    \"Chunk (Pulled by Pipeline)\",   # from model/pipeline\n",
    "                    \"Label\",                        # from labeled dataset\n",
    "                    \"Label (Model)\",                # final model label\n",
    "                    \"Mistral Label\",\n",
    "                    \"LLama Label\",\n",
    "                    \"Claude Label\",\n",
    "                    \"Rationale\",                    # from labeled dataset\n",
    "                    \"Mistral Rationale\",\n",
    "                    \"LLama Rationale\",\n",
    "                    \"Claude Rationale\",\n",
    "                    \"Rationale (Model)\",            # final model rationale\n",
    "                    COL_URL_HEADER,\n",
    "                ]\n",
    "                for c in final_cols:\n",
    "                    if c not in merged.columns:\n",
    "                        merged[c] = \"\"\n",
    "\n",
    "                merged = merged[[c for c in final_cols if c in merged.columns]]\n",
    "\n",
    "                merged.to_csv(labeled_output_csv, index=False)\n",
    "                print(f\"Wrote labeled+model comparison CSV → {labeled_output_csv} (rows: {len(merged)})\")\n",
    "\n",
    "    # -------- Global label totals (after this run) --------\n",
    "    with driver.session(database=NEO4J_DATABASE) as s_counts:\n",
    "        row_counts = s_counts.run(Q_COUNT_LABELS, {}).single()\n",
    "        total_pos = row_counts[\"pos_total\"] or 0\n",
    "        total_neu = row_counts[\"neu_total\"] or 0\n",
    "        total_neg = row_counts[\"neg_total\"] or 0\n",
    "        total_unk = row_counts[\"unk_total\"] or 0\n",
    "\n",
    "    dt_min = (time.time() - t0) / 60\n",
    "    print(f\"\\nProcessed relations: {processed} | Elapsed: {dt_min:.1f} min\")\n",
    "    print(\"=== Diagnostics (global final label) ===\")\n",
    "    print(f\"  Successful (classified, non-Unknown): {ok_rel}\")\n",
    "    print(f\"  Missing snippets:                    {missing_snippets}\")\n",
    "    print(\"=== Label counts (this run, global) ===\")\n",
    "    print(f\"  Positive: {pos_cnt}\")\n",
    "    print(f\"  Neutral : {neu_cnt}\")\n",
    "    print(f\"  Negative: {neg_cnt}\")\n",
    "    print(f\"  Unknown : {unk_cnt}\")\n",
    "    print(\"=== Dataset label totals (after run, global) ===\")\n",
    "    print(f\"  Positive: {total_pos}\")\n",
    "    print(f\"  Neutral : {total_neu}\")\n",
    "    print(f\"  Negative: {total_neg}\")\n",
    "    print(f\"  Unknown : {total_unk}\")\n",
    "    print(\"=== Per-model error counts (this run) ===\")\n",
    "    for model, counts in model_error_counts.items():\n",
    "        print(f\"  [{model}] too_long={counts['too_long']}, json_parse_failed={counts['json_parse_failed']}, \"\n",
    "              f\"bad_keys_or_values={counts['bad_keys_or_values']}, api_error={counts['api_error']}, other={counts['other']}\")\n",
    "\n",
    "# =========================\n",
    "# Example call (adjust paths as needed)\n",
    "# =========================\n",
    "# label_all_citations(\n",
    "#     force=False,\n",
    "#     echo=True,\n",
    "#     results_csv=True,\n",
    "#     results_csv_filename=\"edge_classifications_ensemble.csv\",\n",
    "#     append_to_labeled_dataset_csv=\"Phase One Final Labeled Dataset from WK.csv\",\n",
    "#     labeled_output_csv=None,\n",
    "#     show_all_labels_in_output_csv=True,\n",
    "#     failed_csv=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311426d6-601b-4bf7-9b53-1ae1128558f1",
   "metadata": {},
   "source": [
    "## Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556408ce-c406-4d74-965b-d5dff856bdb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch after rel_id -1: 200 relation(s)\n",
      "Niece v. Fitzner → Robert T. McGregor v. Louisiana State University Board of Supervisors: Global=Unknown (missing snippets)\n",
      "United States v. Agrawal → Coors Brewing Co. v. MENDEZ-TORRES: Global=Unknown (missing snippets)\n",
      "\n",
      "Batch after rel_id 1152924803141730530: 200 relation(s)\n",
      "\n",
      "Batch after rel_id 1152924803141730820: 200 relation(s)\n",
      "Palao v. Fel-Pro., Inc. → Paula S. Skorup v. Modern Door Corporation: Global=Unknown (missing snippets)\n",
      "Barber Lines A/s v. M/v Donau Maru → Dees v. Austin Travis County Mental Health & Mental Retardation: Global=Unknown (missing snippets)\n",
      "Conboy v. State → Jensen v. State, Department of Labor & Industry: Global=Unknown (missing snippets)\n",
      "\n",
      "Batch after rel_id 1152924803141731151: 200 relation(s)\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Llama] API ClientError on attempt 1/3: ValidationException - This model's maximum context length is 8192 tokens. Please reduce the length of the prompt\n",
      "      · [Llama] attempt 2/3\n",
      "      · [Llama] API ClientError on attempt 2/3: ValidationException - This model's maximum context length is 8192 tokens. Please reduce the length of the prompt\n",
      "      · [Claude] attempt 1/3\n",
      "LUPESCU v. Napolitano → Gordon R. Johnson v. Exxonmobil Corporation: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Parker v. Accellent → Billings v. Town of Grafton: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Lenker, Stephen P. v. Methodist Hospital → Shirley Weigel v. Target Stores, a Division of Dayton Hudson Corporation: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "McKIVITZ v. Township of Stowe → Margaret C. Wagner, by Her Next Friend George M. Wagner v. Fair Acres Geriatric Center. Margaret Wagner, by Her Next Friend, George M. Wagner: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Assisted Living Associates of Moorestown, L.L.C. v. Moorestown Township → Alexander v. Choate: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Bryant Woods Inn Inc v. Howard County MD → Alexander v. Choate: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Bryant Woods Inn, Incorporated v. Howard County, Maryland Howard County Planning Board, the American Association of Retired Persons the Bazelon Center Assisted Living Facilities Association of America Assisted Living Legal Defense and Education Fund, Incorporated National Fair Housing Alliance Oxford House, Amici Curiae. Bryant Woods Inn, Incorporated v. Howard County, Maryland Howard County Planning Board, the American Association of Retired Persons the Bazelon Center Assisted Living Facilities Association of America Assisted Living Legal Defense and Education Fund, Incorporated National Fair Housing Alliance Oxford House, Amici Curiae → Alexander v. Choate: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "US Equal Employment Opportunity Commission v. Placer ARC → Donna Braunling v. Countrywide Home Loans Inc., a New York Corp. Cathy Kister, an Individual: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Llama] API ClientError on attempt 1/3: ValidationException - This model's maximum context length is 8192 tokens. Please reduce the length of the prompt\n",
      "      · [Llama] attempt 2/3\n",
      "      · [Llama] API ClientError on attempt 2/3: ValidationException - This model's maximum context length is 8192 tokens. Please reduce the length of the prompt\n",
      "      · [Llama] attempt 3/3\n",
      "      · [Llama] API ClientError on attempt 3/3: ValidationException - This model's maximum context length is 8192 tokens. Please reduce the length of the prompt\n",
      "      · [Claude] attempt 1/3\n",
      "Noah S. Bunker, Paul Carrell, Everett Brew Houston, Jr., W. Andrew Buchholz, Scott J. Leighty, Jad L. Davis, and Holly Clause v. Tracy D. Strandhagen → Farrar v. Hobby: Mistral=Neutral, Llama=Unknown, Claude=Positive → Global=Unknown\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "McInteer v. Ashley Distribution Services, Ltd. → William Schechner v. Kpix-Tv: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Thomas Ballard v. Robert E. Rubin → Kathleen Mole v. Buckhorn Rubber Products, Inc.: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Walsted v. Woodbury County, IA → Cook v. Rhode Island, Department of Mental Health, Retardation, & Hospitals: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Norris v. Sysco Corp. → Cleveland v. Policy Management Systems Corp.: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Brenda Norris v. Sysco Corporation, a Texas Corporation,and Allied-Sysco Food Services, Inc., a California Corporation, Brenda Norris v. Sysco Corporation, a Texas Corporation, and Allied-Sysco Food Services, Inc., a California Corporation, Brenda Norris v. Sysco Corporation, a Texas Corporation, and Allied-Sysco Food Services, Inc., a California Corporation → Cleveland v. Policy Management Systems Corp.: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Caroline Guzman v. Brown County → David Burnett v. Lfw Inc., Doing Business as the Habitat Company: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Fleishman v. Continental Casualty Co. → Brunker v. Schwan's Home Service, Inc.: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Keith Powers v. Usf Holland, Incorp → Miller v. Illinois Department of Transportation: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "John Finnegan v. Commissioner of Internal Revenue → Lussier v. Dugger: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Bowers v. National Collegiate Athletic Ass'n → Consolidated Rail Corporation v. Darrone: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Kimel v. State of FL Bd. of Regents → Lussier v. Dugger: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Donaldson v. Texas Department of Aging & Disability Services → Davis v. City of Grapevine: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Picard v. St. Tammany Parish Hospital → US Airways, Inc. v. Barnett: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Latoya Basey v. Davita Inc., D/B/A Total Renal Care, Nelda Boatwright and Fresenius Medical Care Holding Inc., D/B/A Fresenius Medical Care North America D/B/A Northwest Houston Dialysis, and Biomedical Applications of Texas, Inc. → John R. Turco v. Hoechst Celanese Corporation, Hoechst Celanese Chemical Group, Inc.: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Smith v. Shinseki → Pegram v. Honeywell, Inc.: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Rehrs v. The Iams Company → Michael Aucutt v. Six Flags Over Mid-America, Inc., a Missouri Corporation in Good Standing, Equal Employment Advisory Council, Amicus Curiae: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Davis v. City of Grapevine → Dutcher v. Ingalls Shipbuilding: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Diaz v. Federal Express Corp. → Mary Bradley v. Harcourt, Brace and Company: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Downs v. Massachusetts Bay Transportation Authority → Cook v. Rhode Island, Department of Mental Health, Retardation, & Hospitals: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "D'Amico v. City of New York → Kathleen Borkowski v. Valley Central School District: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "D'Amico v. City of New York → John Teahan, Plaintiff-Appellant-Cross-Appellee v. Metro-North Commuter Railroad Company, Defendant-Appellee-Cross-Appellant: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Rodriguez v. Alcoa Inc. → US Airways, Inc. v. Barnett: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "St. John v. NCI Building Systems, Inc. → Daigle v. Liberty Life Insurance: Mistral=Neutral, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n",
      "      · [Claude] attempt 1/3\n",
      "Conine v. Universal Oil Products → Blanks v. Southwestern Bell Communications, Inc.: Mistral=Positive, Llama=Positive, Claude=Positive → Global=Positive\n",
      "      · [Mistral] attempt 1/3\n",
      "      · [Llama] attempt 1/3\n"
     ]
    }
   ],
   "source": [
    "# Assumes relations already have snippet_1, snippet_2, ... properties.\n",
    "\n",
    "label_all_citations(\n",
    "    force=False,\n",
    "    echo=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cab10b-2eb0-4623-923e-f4f8e844d298",
   "metadata": {},
   "source": [
    "## Compare with labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924abb0-c8c4-4838-87d6-3db14bbd11dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Example call\n",
    "# =========================\n",
    "# label_all_citations(\n",
    "#     force=True,\n",
    "#     echo=True,\n",
    "#     append_to_labeled_dataset_csv=\"Phase One Final Labeled Dataset from WK.csv\",\n",
    "#     labeled_output_csv=\"WK Labeled vs Snippet Method Model Comparison - Ensemble.csv\",\n",
    "#     show_all_labels_in_output_csv=True,\n",
    "#     failed_csv=True,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
