{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d9f1fd-cba7-41eb-bcfc-f42ccefd5b14",
   "metadata": {},
   "source": [
    "# Case Labeler – Continuous Time Weights\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "* Compute a **case-level label** (`\"Good\"`, `\"Bad\"`, `\"Moderate\"`, or `\"Unknown\"`) for each `(:Case)` node based on:\n",
    "\n",
    "  * Incoming `(:Case)-[:CITES_TO]->(:Case)` edges\n",
    "  * **Court level** of the citing case\n",
    "  * **Continuous, recency-based time weights** on citations\n",
    "  * Optionally, **jurisdiction-specific weights** on citations\n",
    "  * **Configurable thresholds** and **label priority**\n",
    "* Write the label, decision level, and a **human-readable rationale** back to the `Case` node.\n",
    "* Optionally, produce a **CSV** with detailed per-court-level metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## What this script uses\n",
    "\n",
    "**From Neo4j**\n",
    "\n",
    "* `(:Case)` nodes:\n",
    "\n",
    "  * `c.id` (internal case ID used in the graph)\n",
    "  * `c.name` (case name, may be empty string)\n",
    "* `(:Case)-[:CITES_TO]->(:Case)` edges:\n",
    "\n",
    "  * `r.treatment_label` (edge-level label: Positive / Neutral / Negative / Unknown)\n",
    "  * `src.decision_date` (decision date of the citing case)\n",
    "* `(:Case)-[:HEARD_IN]->(:Court)` edges:\n",
    "\n",
    "  * `ct.court_level` (integer 1–5, where lower usually means higher court)\n",
    "* `(:Case)-[:UNDER_JURISDICTION]->(:Jurisdiction)` edges:\n",
    "\n",
    "  * `j.jurisdiction_name` (used for optional jurisdiction weights)\n",
    "\n",
    "**Environment**\n",
    "\n",
    "* Loaded from `../.env`:\n",
    "\n",
    "  * `NEO4J_URI`\n",
    "  * `NEO4J_USERNAME`\n",
    "  * `NEO4J_PASSWORD`\n",
    "  * `NEO4J_DATABASE` (optional, default `\"neo4j\"`)\n",
    "\n",
    "---\n",
    "\n",
    "## How it works (high level)\n",
    "\n",
    "1. **Collect global time statistics**\n",
    "\n",
    "   * Query all citing cases with:\n",
    "\n",
    "     ```cypher\n",
    "     MATCH (s:Case)-[:CITES_TO]->(:Case)\n",
    "     WHERE s.decision_date IS NOT NULL\n",
    "     RETURN DISTINCT s.decision_date AS decision_date\n",
    "     ```\n",
    "\n",
    "   * Convert `decision_date` to Python ordinals (integers).\n",
    "\n",
    "   * Compute:\n",
    "\n",
    "     * Lower quartile $Q_1$\n",
    "     * Median $Q_2$\n",
    "     * Upper quartile $Q_3$\n",
    "     * Global minimum and maximum dates.\n",
    "\n",
    "   * By default (`default_tmin_tmax=True`):\n",
    "\n",
    "     * $t_{\\min} = Q_1$\n",
    "     * $t_{\\max} = \\max(\\text{decision date})$ over citing cases.\n",
    "\n",
    "2. **Page through Case nodes**\n",
    "\n",
    "   * Use `Q_PAGE_CASES` to page `Case` nodes by `id(c)` in batches of `_CASE_BATCH_SIZE` (default 200).\n",
    "   * Only include cases where:\n",
    "\n",
    "     * `force=True`, or\n",
    "     * `c.case_label IS NULL`.\n",
    "\n",
    "3. **Collect incoming citations per case**\n",
    "\n",
    "   * For each case:\n",
    "\n",
    "     ```cypher\n",
    "     MATCH (src:Case)-[r:CITES_TO]->(tgt:Case {id:$case_id})\n",
    "     OPTIONAL MATCH (src)-[:HEARD_IN]->(ct:Court)\n",
    "     OPTIONAL MATCH (src)-[:UNDER_JURISDICTION]->(j:Jurisdiction)\n",
    "     RETURN r.treatment_label   AS label,\n",
    "            src.decision_date   AS decision_date,\n",
    "            ct.court_level      AS court_level,\n",
    "            j.jurisdiction_name AS jurisdiction_name\n",
    "     ```\n",
    "\n",
    "   * Group edges by `court_level` (1–5).\n",
    "   * Skip edges with missing or invalid `court_level`.\n",
    "\n",
    "4. **Compute metrics per court level**\n",
    "\n",
    "   For each court level $L$:\n",
    "\n",
    "   * Normalize `r.treatment_label` into one of:\n",
    "\n",
    "     * `\"Positive\"`, `\"Negative\"`, `\"Neutral\"`, `\"Unknown\"`\n",
    "\n",
    "   * For each edge $e$ at level $L$:\n",
    "\n",
    "     * Compute a **continuous time weight** $\\alpha(e)$ based on its citing `decision_date`\n",
    "       (and any configured jurisdiction weight for its `jurisdiction_name`).\n",
    "     * Add that weight to the appropriate label’s bucket.\n",
    "\n",
    "   * Compute, per label:\n",
    "\n",
    "     * `counts[label]` = number of edges\n",
    "     * `weights[label]` = sum of time + jurisdiction weights\n",
    "     * `proportions[label]` = weighted share in the denominator\n",
    "\n",
    "   * Store in:\n",
    "\n",
    "     * `per_level_metrics[level]` (counts, weights, proportions, denom)\n",
    "     * `per_level_counts[level]` (raw counts, total)\n",
    "\n",
    "5. **Decide a label for each court level**\n",
    "\n",
    "   * For each level, compare **proportions** to **configured thresholds**:\n",
    "\n",
    "     * `Pos_p` for `\"Positive\"`\n",
    "     * `Neg_p` for `\"Negative\"`\n",
    "     * `Neu_p` for `\"Neutral\"`\n",
    "     * `Unk_p` for `\"Unknown\"` (if `include_unknown=True`)\n",
    "\n",
    "   * A label becomes a **candidate** if:\n",
    "\n",
    "     $$ p_{\\text{label}} ;\\ge; \\text{threshold}_{\\text{label}} $$\n",
    "\n",
    "   * If multiple labels are candidates, apply **label priority** to pick a single **driver label**:\n",
    "\n",
    "     * Default priority: `Unknown > Negative > Neutral > Positive`\n",
    "     * Or a custom order such as `[\"pos\", \"neg\", \"neu\", \"unk\"]`.\n",
    "\n",
    "   * Map the driver label to a **case label**:\n",
    "\n",
    "     * `\"Positive\"` $\\to$ `\"Good\"`\n",
    "     * `\"Negative\"` $\\to$ `\"Bad\"`\n",
    "     * `\"Neutral\"`  $\\to$ `\"Moderate\"`\n",
    "     * `\"Unknown\"`  $\\to$ `\"Unknown\"`\n",
    "\n",
    "6. **Walk court levels (highest to lowest)**\n",
    "\n",
    "   * Identify all levels that have any citations.\n",
    "   * Determine the **highest level with citations** (numerically smallest).\n",
    "   * If `lower_level_court=False`:\n",
    "\n",
    "     * Only use the highest level.\n",
    "     * If no label at that level passes its threshold, classify as `\"Moderate\"`.\n",
    "   * If `lower_level_court=True`:\n",
    "\n",
    "     * Walk levels from **highest to lowest**.\n",
    "     * At each level, try to decide a label.\n",
    "     * Stop at the first level where a label passes its threshold.\n",
    "     * If no level yields a label, default to `\"Moderate\"` at the lowest level with citations.\n",
    "\n",
    "7. **Build a human-readable rationale**\n",
    "\n",
    "   For each case, `label_rationale` includes:\n",
    "\n",
    "   * Case name + **total number of incoming citations**.\n",
    "   * A **per-court-level citation count** summary, e.g.\n",
    "     `Supreme Court: X, Court of Appeals: Y, …`.\n",
    "   * The **decision court level** and a breakdown at that level:\n",
    "\n",
    "     * Weighted proportions (Positive / Negative / Neutral, and optionally Unknown).\n",
    "     * Raw counts for each label.\n",
    "   * A short explanation of **why** that label was chosen:\n",
    "\n",
    "     * Which label was dominant and met its threshold.\n",
    "     * Or that no label met thresholds, so the case is `\"Moderate\"`.\n",
    "   * If a lower court was used:\n",
    "\n",
    "     * A note that the highest level had **mixed / balanced signals**, so the decision moved down.\n",
    "\n",
    "   The rationale also reports:\n",
    "\n",
    "   * The **configured thresholds** $(\\text{Pos}_p, \\text{Neg}_p, \\text{Neu}_p, \\text{Unk}_p)$.\n",
    "   * The **label priority order** used at the decision level.\n",
    "\n",
    "   When `include_unknown=False`:\n",
    "\n",
    "   * Unknown is **not** included in the main share and count breakdown.\n",
    "   * Threshold explanation focuses on Positive / Negative / Neutral.\n",
    "\n",
    "8. **Write results back to Neo4j**\n",
    "\n",
    "   * For each case, run:\n",
    "\n",
    "     ```cypher\n",
    "     MATCH (c:Case {id:$case_id})\n",
    "     SET c.case_label                      = $case_label,\n",
    "         c.court_level_case_label_decision = $decision_level,\n",
    "         c.label_rationale                 = $label_rationale,\n",
    "         c.updated_at_utc                  = datetime()\n",
    "     RETURN c.id AS case_id\n",
    "     ```\n",
    "\n",
    "   * Fields:\n",
    "\n",
    "     * `c.case_label` ∈ `\"Good\" | \"Bad\" | \"Moderate\" | \"Unknown\"`.\n",
    "     * `c.court_level_case_label_decision` = **human-readable court name**\n",
    "       (e.g. `\"Supreme Court\"`, `\"Court of Appeals\"`) where the decision was made,\n",
    "       or `null`/empty if there is no decision court.\n",
    "     * `c.label_rationale` = long explanatory text.\n",
    "     * `c.updated_at_utc` = timestamp.\n",
    "\n",
    "9. **Optional CSV export**\n",
    "\n",
    "   * If `results_csv=True`, build one row per case with:\n",
    "\n",
    "     * Case metadata (`Case ID`, `Case Name`, `Total Number of Citations`).\n",
    "     * For each court level 1–5:\n",
    "\n",
    "       * Counts per label.\n",
    "       * Time + jurisdiction weighted sums per label.\n",
    "       * Proportions per label.\n",
    "     * `Court Level Decision`\n",
    "     * `Case Label`\n",
    "     * `Rationale`\n",
    "\n",
    "   * Written to `results_csv_filename` (default `\"case_labeled_results.csv\"`).\n",
    "\n",
    "---\n",
    "\n",
    "## Continuous time weighting\n",
    "\n",
    "### Recency window\n",
    "\n",
    "Let $t_e$ be the citing decision date (converted to ordinal) for edge $e$.\n",
    "\n",
    "The code defines a **recency window** $[t_{\\min}, t_{\\max}]$.\n",
    "\n",
    "* **Default** (`default_tmin_tmax=True`):\n",
    "\n",
    "  * $t_{\\min}$ = lower quartile $Q_1$ of all citing decision dates\n",
    "  * $t_{\\max}$ = maximum citing decision date in the dataset\n",
    "\n",
    "* **Custom** (`default_tmin_tmax=False`):\n",
    "\n",
    "  * You pass `tmin_tmax = [t_{min}, t_{max}]`\n",
    "  * Both are parsed into ordinals; must satisfy $t_{\\max} > t_{\\min}$.\n",
    "\n",
    "### Normalized recency\n",
    "\n",
    "For an edge $e$, with ordinal date $t_e$, the code computes a **normalized recency**:\n",
    "\n",
    "$$\n",
    "r_e = \\frac{t_e - t_{\\min}}{t_{\\max} - t_{\\min}}\n",
    "$$\n",
    "\n",
    "Then it clamps $r_e$ to the range $[0, 1]$:\n",
    "\n",
    "* If $t_e \\le t_{\\min}$, then $r_e = 0$\n",
    "* If $t_e \\ge t_{\\max}$, then $r_e = 1$\n",
    "\n",
    "If the recency window is not valid (missing dates or $t_{\\max} \\le t_{\\min}$), it falls back to no recency effect, i.e. all edges get base weight $1$ (before any jurisdiction weight).\n",
    "\n",
    "### Time weight range\n",
    "\n",
    "The **base time-weight component** is always in the range $[1, \\text{MAX WEIGHT}]$.\n",
    "\n",
    "* **Default** (`default_time_weight=True`):\n",
    "\n",
    "  $$ \\text{MAX WEIGHT} = 2.5 $$\n",
    "\n",
    "  So the base component is $\\in [1, 2.5]$, and a **fully recent** edge ($r_e = 1$) has base weight $2.5$.\n",
    "\n",
    "* **Custom** (`default_time_weight=False`):\n",
    "\n",
    "  * You pass:\n",
    "\n",
    "    ```python\n",
    "    time_weight = [1.0, MAX_WEIGHT]\n",
    "    ```\n",
    "\n",
    "  * The first element must be $1.0$ (minimum base weight).\n",
    "\n",
    "  * The second element sets $\\text{MAX WEIGHT} \\ge 1.0$.\n",
    "\n",
    "When **no jurisdiction weights** are configured, the final $\\alpha(e)$ equals this base component.  \n",
    "When jurisdiction weights are configured, $\\alpha(e)$ can be **larger than** `MAX_WEIGHT` (see below).\n",
    "\n",
    "### Linear vs non-linear recency\n",
    "\n",
    "Let $\\text{MAX WEIGHT}$ be the maximum base weight.\n",
    "\n",
    "1. **Linear recency** (`non_linear_recency_effect=False`, default)\n",
    "\n",
    "   Base component:\n",
    "\n",
    "   $$ \\alpha_{\\text{base}}(e) = 1 + (\\text{MAX WEIGHT} - 1)\\, r_e $$\n",
    "\n",
    "2. **Quadratic recency** (`non_linear_recency_effect=True`)\n",
    "\n",
    "   Base component:\n",
    "\n",
    "   $$ \\alpha_{\\text{base}}(e) = 1 + (\\text{MAX WEIGHT} - 1)\\, r_e^2 $$\n",
    "\n",
    "   For $0 < r_e < 1$, $r_e^2 < r_e$, so mid-range dates get **less** boost and very recent edges get a **stronger relative boost**.\n",
    "\n",
    "In both cases, the final weight is:\n",
    "\n",
    "$$\n",
    "\\alpha(e) = \\alpha_{\\text{base}}(e) + J_i\n",
    "$$\n",
    "\n",
    "where $J_i$ is an optional **jurisdiction weight** (default $J_i = 0$).  \n",
    "If the recency window is invalid or the date is missing, the code uses $\\alpha_{\\text{base}}(e) = 1$ for that edge (so $\\alpha(e) = 1 + J_i$).\n",
    "\n",
    "---\n",
    "\n",
    "## Weights and proportions (per court level)\n",
    "\n",
    "Fix a single court level $L$. Let:\n",
    "\n",
    "* $\\mathcal{E}_{\\text{Pos}}$, $\\mathcal{E}_{\\text{Neg}}$, $\\mathcal{E}_{\\text{Neu}}$, $\\mathcal{E}_{\\text{Unk}}$\n",
    "  be the sets of incoming `:CITES_TO` edges at level $L$ with labels\n",
    "  **Positive**, **Negative**, **Neutral**, **Unknown**, respectively.\n",
    "\n",
    "Let $\\alpha(e)$ be the full weight for edge $e$ (time + jurisdiction).\n",
    "\n",
    "### Weighted sums\n",
    "\n",
    "For each label:\n",
    "\n",
    "$$\n",
    "w_{\\text{Pos}} = \\sum_{e \\in \\mathcal{E}_{\\text{Pos}}} \\alpha(e), \\qquad\n",
    "w_{\\text{Neg}} = \\sum_{e \\in \\mathcal{E}_{\\text{Neg}}} \\alpha(e),\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{\\text{Neu}} = \\sum_{e \\in \\mathcal{E}_{\\text{Neu}}} \\alpha(e), \\qquad\n",
    "w_{\\text{Unk}} = \\sum_{e \\in \\mathcal{E}_{\\text{Unk}}} \\alpha(e).\n",
    "$$\n",
    "\n",
    "If `include_unknown=False`, the code **still** counts Unknown edges, but **does not** add their weights into the denominator.\n",
    "\n",
    "### Proportions\n",
    "\n",
    "If `include_unknown=True`, the denominator is:\n",
    "\n",
    "$$\n",
    "D = w_{\\text{Pos}} + w_{\\text{Neg}} + w_{\\text{Neu}} + w_{\\text{Unk}},\n",
    "$$\n",
    "\n",
    "and the proportions are:\n",
    "\n",
    "$$\n",
    "p_{\\text{Pos}} = \\frac{w_{\\text{Pos}}}{D}, \\quad\n",
    "p_{\\text{Neg}} = \\frac{w_{\\text{Neg}}}{D}, \\quad\n",
    "p_{\\text{Neu}} = \\frac{w_{\\text{Neu}}}{D}, \\quad\n",
    "p_{\\text{Unk}} = \\frac{w_{\\text{Unk}}}{D}.\n",
    "$$\n",
    "\n",
    "If `include_unknown=False`, the denominator excludes Unknown:\n",
    "\n",
    "$$\n",
    "D = w_{\\text{Pos}} + w_{\\text{Neg}} + w_{\\text{Neu}},\n",
    "$$\n",
    "\n",
    "and the proportions are:\n",
    "\n",
    "$$\n",
    "p_{\\text{Pos}} = \\frac{w_{\\text{Pos}}}{D}, \\quad\n",
    "p_{\\text{Neg}} = \\frac{w_{\\text{Neg}}}{D}, \\quad\n",
    "p_{\\text{Neu}} = \\frac{w_{\\text{Neu}}}{D},\n",
    "$$\n",
    "\n",
    "while $p_{\\text{Unk}}$ is treated as $0$ for scoring (though Unknown counts are still reported in CSV and rationale when relevant).\n",
    "\n",
    "---\n",
    "\n",
    "## Label thresholds and priorities\n",
    "\n",
    "### Thresholds\n",
    "\n",
    "Configured via `label_thresholds`:\n",
    "\n",
    "```python\n",
    "label_thresholds = {\n",
    "    \"Pos_p\": 0.55,  # Positive proportion threshold\n",
    "    \"Neg_p\": 0.55,  # Negative proportion threshold\n",
    "    \"Neu_p\": 0.55,  # Neutral proportion threshold\n",
    "    \"Unk_p\": 0.55,  # Unknown proportion threshold\n",
    "}\n",
    "```\n",
    "\n",
    "* Default (if not provided): $0.55$ for all four.\n",
    "* Per court level:\n",
    "\n",
    "  * Compute $(p_{\\text{Pos}}, p_{\\text{Neg}}, p_{\\text{Neu}}, p_{\\text{Unk}})$.\n",
    "  * A label is a **candidate** if $p_{\\text{label}} \\ge \\text{threshold}_{\\text{label}}$.\n",
    "\n",
    "### Label priority\n",
    "\n",
    "Used to break ties when **multiple labels pass their thresholds** at the same level.\n",
    "\n",
    "**Default** (`default_label_priority=True`):\n",
    "\n",
    "* Priority (from highest to lowest):\n",
    "\n",
    "  ```text\n",
    "  Unknown > Negative > Neutral > Positive\n",
    "  ```\n",
    "\n",
    "* Internally:\n",
    "\n",
    "  ```python\n",
    "  [\"unk\", \"neg\", \"neu\", \"pos\"]\n",
    "  ```\n",
    "\n",
    "* If `include_unknown=False`, `\"Unknown\"` is removed from the effective order.\n",
    "\n",
    "**Custom** (`default_label_priority=False`):\n",
    "\n",
    "* You provide `label_priority`, e.g.:\n",
    "\n",
    "  ```python\n",
    "  label_priority = [\"pos\", \"neg\", \"neu\", \"unk\"]\n",
    "  ```\n",
    "\n",
    "* Accepted values (case-insensitive):\n",
    "\n",
    "  * `pos`, `positive`, `good`\n",
    "  * `neg`, `negative`, `bad`\n",
    "  * `neu`, `neutral`, `moderate`, `mod`\n",
    "  * `unk`, `unknown`\n",
    "\n",
    "* These are mapped to canonical labels:\n",
    "\n",
    "  * `\"Positive\"`, `\"Negative\"`, `\"Neutral\"`, `\"Unknown\"`\n",
    "\n",
    "The script then:\n",
    "\n",
    "* Removes duplicates.\n",
    "* Drops `\"Unknown\"` if `include_unknown=False`.\n",
    "* Uses this ordered list to pick **one driver label** from the candidate set at each level.\n",
    "\n",
    "---\n",
    "\n",
    "## Jurisdiction weights (optional)\n",
    "\n",
    "You can give extra weight to citations from specific jurisdictions via the `jurisdictions` argument:\n",
    "\n",
    "```python\n",
    "jurisdictions = {\n",
    "    \"California\": 1.0,      # fixed extra weight J_i\n",
    "    \"Alabama\":   \"Default\", # interpreted as MAX_WEIGHT / 2\n",
    "}\n",
    "```\n",
    "\n",
    "* Keys must match `j.jurisdiction_name` in Neo4j.\n",
    "* Values can be:\n",
    "\n",
    "  * A float (extra weight $J_i$ added to every edge from that jurisdiction), or\n",
    "  * The string `\"Default\"`, which the script interprets as `MAX_WEIGHT / 2`.\n",
    "\n",
    "For each edge $e$ from a jurisdiction with configured weight $J_i$:\n",
    "\n",
    "* The final weight is $\\alpha(e) = \\alpha_{\\text{base}}(e) + J_i$.\n",
    "\n",
    "If `jurisdictions` is `None` or empty, all $J_i = 0$ and weights are purely time-based.\n",
    "\n",
    "---\n",
    "\n",
    "## Key parameters (continuous time version)\n",
    "\n",
    "```python\n",
    "label_all_cases(\n",
    "    *,\n",
    "    force: bool = False,\n",
    "    echo: bool = False,\n",
    "    lower_level_court: bool = True,\n",
    "    include_unknown: bool = True,\n",
    "    label_thresholds: Optional[Dict[str, float]] = None,\n",
    "    default_label_priority: bool = True,\n",
    "    label_priority: Optional[List[str]] = None,\n",
    "    default_tmin_tmax: bool = True,\n",
    "    tmin_tmax: Optional[List[Any]] = None,\n",
    "    default_time_weight: bool = True,\n",
    "    time_weight: Optional[List[float]] = None,\n",
    "    non_linear_recency_effect: bool = False,\n",
    "    jurisdictions: Optional[Dict[str, Any]] = None,\n",
    "    results_csv: bool = False,\n",
    "    results_csv_filename: str = \"case_labeled_results.csv\",\n",
    ")\n",
    "```\n",
    "\n",
    "* **Recency window**\n",
    "\n",
    "  * `default_tmin_tmax=True`:\n",
    "\n",
    "    * Uses $t_{\\min} = Q_1$, $t_{\\max} = \\max(\\text{decision date})$.\n",
    "  * `default_tmin_tmax=False`:\n",
    "\n",
    "    * You must set `tmin_tmax = [tmin, tmax]`.\n",
    "\n",
    "* **Time weights**\n",
    "\n",
    "  * `default_time_weight=True`:\n",
    "\n",
    "    * Uses base $\\alpha_{\\text{base}}(e) \\in [1, 2.5]`.\n",
    "  * `default_time_weight=False`:\n",
    "\n",
    "    * You must set `time_weight = [1.0, MAX_WEIGHT]`.\n",
    "\n",
    "* **Shape of recency effect**\n",
    "\n",
    "  * `non_linear_recency_effect=False`:\n",
    "\n",
    "    * Linear: $\\alpha_{\\text{base}}(e) = 1 + (\\text{MAX WEIGHT} - 1), r_e$.\n",
    "  * `non_linear_recency_effect=True`:\n",
    "\n",
    "    * Quadratic: $\\alpha_{\\text{base}}(e) = 1 + (\\text{MAX WEIGHT} - 1), r_e^2$.\n",
    "\n",
    "* **Jurisdiction weights**\n",
    "\n",
    "  * `jurisdictions=None`:\n",
    "\n",
    "    * No jurisdiction boost; $\\alpha(e) = \\alpha_{\\text{base}}(e)$.\n",
    "  * `jurisdictions={...}`:\n",
    "\n",
    "    * Adds $J_i$ per jurisdiction: $\\alpha(e) = \\alpha_{\\text{base}}(e) + J_i$.\n",
    "\n",
    "The rest of the behavior (thresholding, priority, lower court walk, rationale, CSV) matches the described logic, using these continuous, date-based weights (and optional jurisdiction boosts) instead of discrete time buckets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed0c037-eb88-4627-9fff-59ca7fd5bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in /opt/conda/lib/python3.12/site-packages (5.28.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from neo4j) (2024.2)\n"
     ]
    }
   ],
   "source": [
    "# Install (if applicable)\n",
    "! pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0338c85-1b9c-4d92-ab3d-dbbd7a823338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import date, datetime\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Quiet noisy logs (incl. Neo4j notifications/deprecations)\n",
    "for _n in (\"neo4j\", \"neo4j.notifications\", \"neo4j.work.simple\"):\n",
    "    logging.getLogger(_n).setLevel(logging.ERROR)\n",
    "os.environ.setdefault(\"NEO4J_DRIVER_LOG_LEVEL\", \"ERROR\")\n",
    "\n",
    "# =========================\n",
    "# Config / ENV\n",
    "# =========================\n",
    "# .env is always one level up from this notebook/script\n",
    "load_dotenv(\"../.env\", override=True)\n",
    "NEO4J_URI       = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME  = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD  = os.getenv(\"NEO4J_PASSWORD\")\n",
    "NEO4J_DATABASE  = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "\n",
    "if not (NEO4J_URI and NEO4J_USERNAME and NEO4J_PASSWORD):\n",
    "    raise RuntimeError(\n",
    "        \"Missing Neo4j connection settings. \"\n",
    "        \"Check ../.env for NEO4J_URI / NEO4J_USERNAME / NEO4J_PASSWORD.\"\n",
    "    )\n",
    "\n",
    "# Internal batch sizes\n",
    "_CASE_BATCH_SIZE = 200\n",
    "_EDGE_BATCH_SIZE = 2000  # for paging CITES_TO edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a90b3e31-f9a7-4222-a8e3-60eb6cd02efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Jurisdictions and Court Levels\n",
    "# =========================\n",
    "\n",
    "VALID_JURISDICTIONS = {\n",
    "    \"Alabama\",\n",
    "    \"Alaska\",\n",
    "    \"Alaska Court of Appeal\",\n",
    "    \"Arizona\",\n",
    "    \"Arizona Court of Appeal\",\n",
    "    \"Arkansas\",\n",
    "    \"Arkansas Court of Appeal\",\n",
    "    \"Board of Immigration Appeals\",\n",
    "    \"California\",\n",
    "    \"California Court of Appeal\",\n",
    "    \"Colorado\",\n",
    "    \"Colorado Court of Appeal\",\n",
    "    \"Connecticut\",\n",
    "    \"Delaware\",\n",
    "    \"Federal Supreme Court\",\n",
    "    \"Florida\",\n",
    "    \"Florida Court of Appeal\",\n",
    "    \"Georgia\",\n",
    "    \"Georgia Court of Appeal\",\n",
    "    \"Hawaii\",\n",
    "    \"Idaho\",\n",
    "    \"Idaho Court of Appeal\",\n",
    "    \"Illinois\",\n",
    "    \"Indiana\",\n",
    "    \"Indiana Court of Appeal\",\n",
    "    \"Iowa\",\n",
    "    \"Iowa Court of Appeal\",\n",
    "    \"Kansas\",\n",
    "    \"Kansas Court of Appeal\",\n",
    "    \"Kentucky\",\n",
    "    \"Kentucky Court of Appeal\",\n",
    "    \"Louisiana\",\n",
    "    \"Louisiana Court of Appeal\",\n",
    "    \"Maine\",\n",
    "    \"Maryland\",\n",
    "    \"Massachusetts\",\n",
    "    \"Merit Systems Protection Board\",\n",
    "    \"Michigan\",\n",
    "    \"Michigan Court of Appeal\",\n",
    "    \"Minnesota\",\n",
    "    \"Minnesota Court of Appeal\",\n",
    "    \"Mississippi\",\n",
    "    \"Mississippi Court of Appeal\",\n",
    "    \"Missouri\",\n",
    "    \"Missouri Court of Appeal\",\n",
    "    \"Montana\",\n",
    "    \"Nebraska\",\n",
    "    \"Nebraska Court of Appeal\",\n",
    "    \"Nevada\",\n",
    "    \"New Hampshire\",\n",
    "    \"New Jersey\",\n",
    "    \"New Jersey Court of Appeal\",\n",
    "    \"New Mexico\",\n",
    "    \"New Mexico Court of Appeal\",\n",
    "    \"New York\",\n",
    "    \"North Carolina\",\n",
    "    \"North Carolina Court of Appeal\",\n",
    "    \"North Dakota\",\n",
    "    \"Northern Mariana Islands\",\n",
    "    \"Office of Legal Counsel\",\n",
    "    \"Ohio\",\n",
    "    \"Ohio Court of Appeal\",\n",
    "    \"Oklahoma\",\n",
    "    \"Oregon\",\n",
    "    \"Oregon Court of Appeal\",\n",
    "    \"Pennsylvania\",\n",
    "    \"Puerto Rico\",\n",
    "    \"Rhode Island\",\n",
    "    \"South Carolina\",\n",
    "    \"South Carolina Court of Appeal\",\n",
    "    \"South Dakota\",\n",
    "    \"Tennessee\",\n",
    "    \"Tennessee Court of Appeal\",\n",
    "    \"Texas\",\n",
    "    \"U.S. Court of Appeals for the Armed Forces\",\n",
    "    \"U.S. Court of Appeals for the D.C. Circuit\",\n",
    "    \"U.S. Court of Appeals for the Eighth Circuit\",\n",
    "    \"U.S. Court of Appeals for the Eleventh Circuit\",\n",
    "    \"U.S. Court of Appeals for the Federal Circuit\",\n",
    "    \"U.S. Court of Appeals for the Fifth Circuit\",\n",
    "    \"U.S. Court of Appeals for the First Circuit\",\n",
    "    \"U.S. Court of Appeals for the Fourth Circuit\",\n",
    "    \"U.S. Court of Appeals for the Ninth Circuit\",\n",
    "    \"U.S. Court of Appeals for the Second Circuit\",\n",
    "    \"U.S. Court of Appeals for the Seventh Circuit\",\n",
    "    \"U.S. Court of Appeals for the Sixth Circuit\",\n",
    "    \"U.S. Court of Appeals for the Tenth Circuit\",\n",
    "    \"U.S. Court of Appeals for the Third Circuit\",\n",
    "    \"U.S. Court of Appeals for Veterans Claims\",\n",
    "    \"U.S. Court of Federal Claims\",\n",
    "    \"U.S. Court of International Trade\",\n",
    "    \"U.S. District Court for the Central District of California\",\n",
    "    \"U.S. District Court for the District of Colorado\",\n",
    "    \"U.S. District Court for the District of Columbia\",\n",
    "    \"U.S. District Court for the District of Hawaii\",\n",
    "    \"U.S. District Court for the District of Maryland\",\n",
    "    \"U.S. District Court for the District of Massachusetts\",\n",
    "    \"U.S. District Court for the District of Minnesota\",\n",
    "    \"U.S. District Court for the District of New Jersey\",\n",
    "    \"U.S. District Court for the District of Oregon\",\n",
    "    \"U.S. District Court for the District of the Virgin Islands\",\n",
    "    \"U.S. District Court for the Eastern District of California\",\n",
    "    \"U.S. District Court for the Eastern District of New York\",\n",
    "    \"U.S. District Court for the Middle District of Louisiana\",\n",
    "    \"U.S. District Court for the Middle District of Pennsylvania\",\n",
    "    \"U.S. District Court for the Northern District of California\",\n",
    "    \"U.S. District Court for the Southern District of California\",\n",
    "    \"U.S. District Court for the Southern District of New York\",\n",
    "    \"U.S. Tax Court\",\n",
    "    \"Unknown\",\n",
    "    \"Utah\",\n",
    "    \"Utah Court of Appeal\",\n",
    "    \"Vermont\",\n",
    "    \"Virginia\",\n",
    "    \"Virginia Court of Appeal\",\n",
    "    \"Washington\",\n",
    "    \"Washington Court of Appeal\",\n",
    "    \"Wisconsin\",\n",
    "    \"Wisconsin Court of Appeal\",\n",
    "    \"Wyoming\",\n",
    "}\n",
    "\n",
    "COURT_LEVEL_NAMES: Dict[int, str] = {\n",
    "    1: \"Supreme Court\",\n",
    "    2: \"Court of Appeals\",\n",
    "    3: \"District Court\",\n",
    "    4: \"State Court\",\n",
    "    5: \"Unknown Court\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeefa7e-8a35-47b1-bc10-390996e2999d",
   "metadata": {},
   "source": [
    "## Cypher Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79cac707-0836-4e3b-9740-fba9856bce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cypher Queries\n",
    "# =========================\n",
    "\n",
    "# All decision dates for computing global time stats (using citing cases)\n",
    "Q_GET_TIME_DATES = \"\"\"\n",
    "MATCH (s:Case)-[:CITES_TO]->(:Case)\n",
    "WHERE s.decision_date IS NOT NULL\n",
    "RETURN DISTINCT s.decision_date AS decision_date\n",
    "\"\"\"\n",
    "\n",
    "# Count cases that need labeling (respecting `force`)\n",
    "Q_COUNT_CASES = \"\"\"\n",
    "MATCH (c:Case)\n",
    "WHERE $force = true OR c.case_label IS NULL\n",
    "RETURN count(c) AS n\n",
    "\"\"\"\n",
    "\n",
    "# Page through Case nodes using internal id(c)\n",
    "Q_PAGE_CASES = \"\"\"\n",
    "MATCH (c:Case)\n",
    "WHERE id(c) > $after_id\n",
    "  AND ($force = true OR c.case_label IS NULL)\n",
    "RETURN id(c) AS neo_id,\n",
    "       c.id   AS case_id,\n",
    "       coalesce(c.name,'') AS case_name\n",
    "ORDER BY neo_id\n",
    "LIMIT $limit\n",
    "\"\"\"\n",
    "\n",
    "# Incoming citations for a given Case, with court level, citing decision date, and jurisdiction\n",
    "# Assumes:\n",
    "#   (src:Case)-[:HEARD_IN]->(ct:Court {level: 1..5})\n",
    "#   (src:Case)-[:UNDER_JURISDICTION]->(j:Jurisdiction {jurisdiction_name: ...})\n",
    "Q_INCOMING_EDGES_FOR_CASE = \"\"\"\n",
    "MATCH (src:Case)-[r:CITES_TO]->(tgt:Case {id:$case_id})\n",
    "OPTIONAL MATCH (src)-[:HEARD_IN]->(ct:Court)\n",
    "OPTIONAL MATCH (src)-[:UNDER_JURISDICTION]->(j:Jurisdiction)\n",
    "RETURN r.treatment_label      AS label,\n",
    "       src.decision_date      AS decision_date,\n",
    "       ct.court_level         AS court_level,\n",
    "       j.jurisdiction_name    AS jurisdiction_name\n",
    "\"\"\"\n",
    "\n",
    "# Write final label back to the Case node\n",
    "Q_WRITE_CASE_LABEL = \"\"\"\n",
    "MATCH (c:Case {id:$case_id})\n",
    "SET c.case_label                       = $case_label,\n",
    "    c.court_level_case_label_decision  = $decision_level,\n",
    "    c.label_rationale                  = $label_rationale,\n",
    "    c.updated_at_utc                   = datetime()\n",
    "RETURN c.id AS case_id\n",
    "\"\"\"\n",
    "\n",
    "# Count CITES_TO edges relevant for this run (only into cases we are labeling,\n",
    "# unless force=True where we use all edges).\n",
    "Q_COUNT_CITES_EDGES = \"\"\"\n",
    "MATCH (src:Case)-[r:CITES_TO]->(tgt:Case)\n",
    "WHERE $force = true OR tgt.case_label IS NULL\n",
    "RETURN count(r) AS n\n",
    "\"\"\"\n",
    "\n",
    "# Page through CITES_TO edges to compute and store recency/alpha scores.\n",
    "# Only edges into cases we are labeling are included.\n",
    "Q_PAGE_CITES_EDGES = \"\"\"\n",
    "MATCH (src:Case)-[r:CITES_TO]->(tgt:Case)\n",
    "WHERE id(r) > $after_id\n",
    "  AND ($force = true OR tgt.case_label IS NULL)\n",
    "OPTIONAL MATCH (src)-[:UNDER_JURISDICTION]->(j:Jurisdiction)\n",
    "RETURN id(r)                AS rel_id,\n",
    "       src.decision_date    AS decision_date,\n",
    "       j.jurisdiction_name  AS jurisdiction_name\n",
    "ORDER BY rel_id\n",
    "LIMIT $limit\n",
    "\"\"\"\n",
    "\n",
    "# Batch-write recency_re and influence_score_alpha onto edges\n",
    "Q_WRITE_EDGE_SCORES = \"\"\"\n",
    "UNWIND $rows AS row\n",
    "MATCH ()-[r:CITES_TO]->()\n",
    "WHERE id(r) = row.rel_id\n",
    "SET r.recency_re            = row.recency_re,\n",
    "    r.influence_score_alpha = row.influence_score_alpha\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c73d6-cc41-45bb-9a43-0a08e25bba1b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78fd3923-c572-4119-8006-e66064093d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helper Functions\n",
    "# =========================\n",
    "\n",
    "def _to_ordinal(date_value: Any) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Convert a Neo4j date / datetime / ISO string to a Python ordinal (int days).\n",
    "    Returns None if the value cannot be parsed.\n",
    "    \"\"\"\n",
    "    if date_value is None:\n",
    "        return None\n",
    "\n",
    "    # Already a date or datetime\n",
    "    if isinstance(date_value, date) and not isinstance(date_value, datetime):\n",
    "        return date_value.toordinal()\n",
    "    if isinstance(date_value, datetime):\n",
    "        return date_value.date().toordinal()\n",
    "\n",
    "    # Fallback: assume ISO string \"YYYY-MM-DD\" or full datetime\n",
    "    try:\n",
    "        txt = str(date_value)\n",
    "        if \"T\" in txt:\n",
    "            return datetime.fromisoformat(txt).date().toordinal()\n",
    "        return datetime.fromisoformat(txt).toordinal()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _compute_time_quartiles(session) -> Dict[str, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Compute global Q1, Q2 (median), Q3, and min/max over decision_date of citing cases.\n",
    "    Returns ordinals in a dict:\n",
    "      {\"q1\": int|None, \"q2\": int|None, \"q3\": int|None,\n",
    "       \"min\": int|None, \"max\": int|None}.\n",
    "    \"\"\"\n",
    "    rows = session.run(Q_GET_TIME_DATES).data()\n",
    "    ordinals: List[int] = []\n",
    "    for row in rows:\n",
    "        ordv = _to_ordinal(row.get(\"decision_date\"))\n",
    "        if ordv is not None:\n",
    "            ordinals.append(ordv)\n",
    "\n",
    "    if not ordinals:\n",
    "        # No dates available\n",
    "        return {\"q1\": None, \"q2\": None, \"q3\": None, \"min\": None, \"max\": None}\n",
    "\n",
    "    s = pd.Series(ordinals, dtype=\"float\")\n",
    "    q1 = int(round(s.quantile(0.25)))\n",
    "    q2 = int(round(s.quantile(0.50)))\n",
    "    q3 = int(round(s.quantile(0.75)))\n",
    "    dmin = int(s.min())\n",
    "    dmax = int(s.max())\n",
    "    return {\"q1\": q1, \"q2\": q2, \"q3\": q3, \"min\": dmin, \"max\": dmax}\n",
    "\n",
    "\n",
    "def _normalize_edge_label(raw: Any) -> str:\n",
    "    \"\"\"\n",
    "    Normalize edge treatment labels into one of:\n",
    "    \"Positive\", \"Neutral\", \"Negative\", \"Unknown\".\n",
    "    \"\"\"\n",
    "    if raw is None:\n",
    "        return \"Unknown\"\n",
    "    txt = str(raw).strip().lower()\n",
    "    if txt == \"positive\":\n",
    "        return \"Positive\"\n",
    "    if txt == \"negative\":\n",
    "        return \"Negative\"\n",
    "    if txt in (\"neutral\", \"moderate\"):\n",
    "        # Edge-level label \"Neutral\" is treated as Neutral; if \"Moderate\" appears, map to Neutral.\n",
    "        return \"Neutral\"\n",
    "    if txt == \"unknown\":\n",
    "        return \"Unknown\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def _court_level_to_name(level: Optional[int]) -> str:\n",
    "    \"\"\"\n",
    "    Map a numeric court level (1–5) to a human-readable court name.\n",
    "    Returns \"\" for None.\n",
    "    \"\"\"\n",
    "    if level is None:\n",
    "        return \"\"\n",
    "    return COURT_LEVEL_NAMES.get(level, f\"Court level {level}\")\n",
    "\n",
    "\n",
    "def _compute_normalized_recency(\n",
    "    decision_date: Any,\n",
    "    tmin_ord: Optional[int],\n",
    "    tmax_ord: Optional[int],\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute normalized recency r_e in [0, 1] based on decision_date\n",
    "    and the global window [tmin_ord, tmax_ord].\n",
    "\n",
    "    Returns:\n",
    "      - float in [0,1] if recency can be computed\n",
    "      - None if there is no valid date or window.\n",
    "    \"\"\"\n",
    "    ordv = _to_ordinal(decision_date)\n",
    "    if (\n",
    "        ordv is None\n",
    "        or tmin_ord is None\n",
    "        or tmax_ord is None\n",
    "        or tmax_ord <= tmin_ord\n",
    "    ):\n",
    "        return None\n",
    "\n",
    "    span = float(tmax_ord - tmin_ord)\n",
    "    r = (ordv - tmin_ord) / span\n",
    "    if r <= 0.0:\n",
    "        return 0.0\n",
    "    if r >= 1.0:\n",
    "        return 1.0\n",
    "    return float(r)\n",
    "\n",
    "\n",
    "def _compute_alpha(\n",
    "    decision_date: Any,\n",
    "    jurisdiction_name: Optional[str],\n",
    "    tmin_ord: Optional[int],\n",
    "    tmax_ord: Optional[int],\n",
    "    max_weight: float,\n",
    "    non_linear_recency_effect: bool,\n",
    "    jurisdiction_weights: Optional[Dict[str, float]],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the weight alpha(e) for one edge:\n",
    "\n",
    "        alpha(e) = 1 + (MAX_WEIGHT - 1) * r_eff (+ optional Ji)\n",
    "\n",
    "    where:\n",
    "      - r_eff = r_e or r_e^2, depending on non_linear_recency_effect\n",
    "      - r_e is the normalized recency in [0, 1]\n",
    "      - Ji is an additional jurisdiction weight if the citing jurisdiction\n",
    "        is in `jurisdiction_weights`. If the user passed \"Default\" for\n",
    "        that jurisdiction, Ji = MAX_WEIGHT / 2 by construction.\n",
    "    \"\"\"\n",
    "    base = 1.0\n",
    "    r = _compute_normalized_recency(decision_date, tmin_ord, tmax_ord)\n",
    "\n",
    "    if r is not None:\n",
    "        if non_linear_recency_effect:\n",
    "            r_eff = r * r\n",
    "        else:\n",
    "            r_eff = r\n",
    "        base = 1.0 + (max_weight - 1.0) * r_eff\n",
    "\n",
    "    # --- jurisdiction component ---\n",
    "    Ji = 0.0\n",
    "    if jurisdiction_weights and jurisdiction_name:\n",
    "        Ji = jurisdiction_weights.get(str(jurisdiction_name), 0.0)\n",
    "\n",
    "    return base + Ji\n",
    "\n",
    "\n",
    "def _compute_level_metrics(\n",
    "    edges: List[Dict[str, Any]],\n",
    "    include_unknown: bool,\n",
    "    tmin_ord: Optional[int],\n",
    "    tmax_ord: Optional[int],\n",
    "    max_weight: float,\n",
    "    non_linear_recency_effect: bool,\n",
    "    jurisdiction_weights: Optional[Dict[str, float]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute counts, time-weighted sums, and proportions for one court level.\n",
    "\n",
    "    Returns a dict:\n",
    "    {\n",
    "      \"counts\":      {label -> int},\n",
    "      \"weights\":     {label -> float},\n",
    "      \"proportions\": {label -> float},\n",
    "      \"denom\":       float\n",
    "    }\n",
    "    where labels are: \"Positive\", \"Neutral\", \"Negative\", \"Unknown\".\n",
    "    \"\"\"\n",
    "    labels = (\"Positive\", \"Neutral\", \"Negative\", \"Unknown\")\n",
    "    counts = {lab: 0 for lab in labels}\n",
    "    weights = {lab: 0.0 for lab in labels}\n",
    "\n",
    "    for e in edges:\n",
    "        lab = _normalize_edge_label(e.get(\"label\"))\n",
    "        counts[lab] += 1\n",
    "\n",
    "        # If we are excluding Unknown from the scoring, skip its weight\n",
    "        if lab == \"Unknown\" and not include_unknown:\n",
    "            continue\n",
    "\n",
    "        w = _compute_alpha(\n",
    "            decision_date=e.get(\"decision_date\"),\n",
    "            jurisdiction_name=e.get(\"jurisdiction_name\"),\n",
    "            tmin_ord=tmin_ord,\n",
    "            tmax_ord=tmax_ord,\n",
    "            max_weight=max_weight,\n",
    "            non_linear_recency_effect=non_linear_recency_effect,\n",
    "            jurisdiction_weights=jurisdiction_weights,\n",
    "        )\n",
    "        weights[lab] += w\n",
    "\n",
    "    if include_unknown:\n",
    "        denom = (\n",
    "            weights[\"Positive\"]\n",
    "            + weights[\"Neutral\"]\n",
    "            + weights[\"Negative\"]\n",
    "            + weights[\"Unknown\"]\n",
    "        )\n",
    "    else:\n",
    "        denom = (\n",
    "            weights[\"Positive\"]\n",
    "            + weights[\"Neutral\"]\n",
    "            + weights[\"Negative\"]\n",
    "        )\n",
    "\n",
    "    if denom > 0.0:\n",
    "        pos_p = weights[\"Positive\"] / denom\n",
    "        neu_p = weights[\"Neutral\"] / denom\n",
    "        neg_p = weights[\"Negative\"] / denom\n",
    "        unk_p = (weights[\"Unknown\"] / denom) if include_unknown else 0.0\n",
    "    else:\n",
    "        pos_p = neu_p = neg_p = unk_p = 0.0\n",
    "\n",
    "    return {\n",
    "        \"counts\": counts,\n",
    "        \"weights\": weights,\n",
    "        \"proportions\": {\n",
    "            \"Positive\": pos_p,\n",
    "            \"Neutral\": neu_p,\n",
    "            \"Negative\": neg_p,\n",
    "            \"Unknown\": unk_p,\n",
    "        },\n",
    "        \"denom\": denom,\n",
    "    }\n",
    "\n",
    "\n",
    "def _normalize_priority_list(\n",
    "    label_priority: List[str],\n",
    "    include_unknown: bool,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Normalize user-specified label priority into canonical labels:\n",
    "    \"Positive\", \"Negative\", \"Neutral\", \"Unknown\".\n",
    "\n",
    "    Removes duplicates and, if include_unknown=False, removes \"Unknown\".\n",
    "    \"\"\"\n",
    "    if not label_priority:\n",
    "        raise ValueError(\"label_priority must be a non-empty list.\")\n",
    "\n",
    "    mapping = {\n",
    "        \"pos\": \"Positive\",\n",
    "        \"positive\": \"Positive\",\n",
    "        \"good\": \"Positive\",\n",
    "\n",
    "        \"neg\": \"Negative\",\n",
    "        \"negative\": \"Negative\",\n",
    "        \"bad\": \"Negative\",\n",
    "\n",
    "        \"neu\": \"Neutral\",\n",
    "        \"neutral\": \"Neutral\",\n",
    "        \"moderate\": \"Neutral\",\n",
    "        \"mod\": \"Neutral\",\n",
    "\n",
    "        \"unk\": \"Unknown\",\n",
    "        \"unknown\": \"Unknown\",\n",
    "    }\n",
    "\n",
    "    result: List[str] = []\n",
    "    for item in label_priority:\n",
    "        if item is None:\n",
    "            continue\n",
    "        key = str(item).strip().lower()\n",
    "        if key not in mapping:\n",
    "            raise ValueError(\n",
    "                f\"Unrecognized label in label_priority: {item!r}. \"\n",
    "                \"Allowed values (case-insensitive) include: \"\n",
    "                \"pos/positive/good, neg/negative/bad, \"\n",
    "                \"neu/neutral/moderate, unk/unknown.\"\n",
    "            )\n",
    "        canon = mapping[key]\n",
    "        if canon not in result:\n",
    "            result.append(canon)\n",
    "\n",
    "    if not include_unknown:\n",
    "        result = [lab for lab in result if lab != \"Unknown\"]\n",
    "\n",
    "    if not result:\n",
    "        raise ValueError(\n",
    "            \"Effective label_priority is empty after applying include_unknown setting.\"\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def _decide_label_from_metrics(\n",
    "    metrics: Dict[str, Any],\n",
    "    include_unknown: bool,\n",
    "    label_thresholds: Dict[str, float],\n",
    "    priority_order: List[str],\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Decide case-level label for a single court level based on proportions.\n",
    "\n",
    "    label_thresholds keys:\n",
    "      - \"Pos_p\"  → threshold for Positive proportion\n",
    "      - \"Neg_p\"  → threshold for Negative proportion\n",
    "      - \"Neu_p\"  → threshold for Neutral proportion\n",
    "      - \"Unk_p\"  → threshold for Unknown proportion\n",
    "\n",
    "    priority_order is a list of canonical labels (subset of):\n",
    "      [\"Positive\", \"Negative\", \"Neutral\", \"Unknown\"]\n",
    "    in the order of preference to break ties when several labels\n",
    "    pass their thresholds.\n",
    "\n",
    "    Returns (case_label, driver_label) where:\n",
    "    - case_label ∈ {\"Good\",\"Bad\",\"Moderate\",\"Unknown\"} or None\n",
    "    - driver_label ∈ {\"Positive\",\"Negative\",\"Neutral\",\"Unknown\"} or None\n",
    "    \"\"\"\n",
    "    if metrics[\"denom\"] <= 0.0:\n",
    "        return None, None\n",
    "\n",
    "    props = metrics[\"proportions\"]\n",
    "\n",
    "    thr_map = {\n",
    "        \"Positive\": label_thresholds[\"Pos_p\"],\n",
    "        \"Negative\": label_thresholds[\"Neg_p\"],\n",
    "        \"Neutral\":  label_thresholds[\"Neu_p\"],\n",
    "        \"Unknown\":  label_thresholds[\"Unk_p\"],\n",
    "    }\n",
    "\n",
    "    # Labels we consider for thresholding, depending on include_unknown\n",
    "    considered_labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "    if include_unknown:\n",
    "        considered_labels.append(\"Unknown\")\n",
    "\n",
    "    # Collect labels that pass their own threshold\n",
    "    candidates = set()\n",
    "    for lab in considered_labels:\n",
    "        if props.get(lab, 0.0) >= thr_map[lab]:\n",
    "            candidates.add(lab)\n",
    "\n",
    "    if not candidates:\n",
    "        return None, None\n",
    "\n",
    "    # Apply priority order to pick the driver label\n",
    "    chosen: Optional[str] = None\n",
    "    for lab in priority_order:\n",
    "        if lab in candidates:\n",
    "            chosen = lab\n",
    "            break\n",
    "\n",
    "    if chosen is None:\n",
    "        # No overlap between candidates and priority_order → treat as no decision\n",
    "        return None, None\n",
    "\n",
    "    if chosen == \"Positive\":\n",
    "        return \"Good\", \"Positive\"\n",
    "    if chosen == \"Negative\":\n",
    "        return \"Bad\", \"Negative\"\n",
    "    if chosen == \"Neutral\":\n",
    "        return \"Moderate\", \"Neutral\"\n",
    "    # chosen == \"Unknown\"\n",
    "    return \"Unknown\", \"Unknown\"\n",
    "\n",
    "\n",
    "def _build_label_rationale(\n",
    "    case_name: str,\n",
    "    total_cites: int,\n",
    "    per_level_counts: Dict[int, Dict[str, int]],\n",
    "    per_level_metrics: Dict[int, Dict[str, Any]],\n",
    "    decision_level: Optional[int],\n",
    "    case_label: str,\n",
    "    driver_label: Optional[str],\n",
    "    include_unknown: bool,\n",
    "    used_lower_level: bool,\n",
    "    label_thresholds: Dict[str, float],\n",
    "    priority_order: List[str],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build the human-readable rationale string for one case.\n",
    "    Uses human-readable court names instead of numeric court levels.\n",
    "    Designed so that lawyers can understand the reasoning without\n",
    "    needing to know implementation details.\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. Case-level label and edge case with no citations\n",
    "    # -----------------------------\n",
    "    if decision_level is None or total_cites == 0:\n",
    "        lines.append(\n",
    "            f\"The case '{case_name}' is labeled '{case_label}'.\"\n",
    "        )\n",
    "        lines.append(\n",
    "            \"The case has no incoming citations with a known court level, \"\n",
    "            \"so there is no clear precedential signal to tilt it toward a \"\n",
    "            \"favorable or unfavorable treatment.\"\n",
    "        )\n",
    "        return \" \".join(lines)\n",
    "\n",
    "    decision_court_name = COURT_LEVEL_NAMES.get(\n",
    "        decision_level, f\"Court level {decision_level}\"\n",
    "    )\n",
    "\n",
    "    # Short, human-readable summary tied to the deciding court\n",
    "    if driver_label == \"Positive\":\n",
    "        summary_sentence = (\n",
    "            f\"The case '{case_name}' is labeled '{case_label}' based on citations \"\n",
    "            f\"from the {decision_court_name}, where the balance of weighted \"\n",
    "            \"citations is predominantly positive.\"\n",
    "        )\n",
    "    elif driver_label == \"Negative\":\n",
    "        summary_sentence = (\n",
    "            f\"The case '{case_name}' is labeled '{case_label}' based on citations \"\n",
    "            f\"from the {decision_court_name}, where the balance of weighted \"\n",
    "            \"citations is predominantly negative.\"\n",
    "        )\n",
    "    elif driver_label == \"Neutral\":\n",
    "        summary_sentence = (\n",
    "            f\"The case '{case_name}' is labeled '{case_label}' based on citations \"\n",
    "            f\"from the {decision_court_name}, where the weighted signals are \"\n",
    "            \"neither strongly positive nor strongly negative and instead cluster \"\n",
    "            \"around a neutral treatment.\"\n",
    "        )\n",
    "    elif driver_label == \"Unknown\":\n",
    "        summary_sentence = (\n",
    "            f\"The case '{case_name}' is labeled '{case_label}' because most of the \"\n",
    "            f\"weighted citations to this case at the {decision_court_name} are \"\n",
    "            \"labeled as 'Unknown'.\"\n",
    "        )\n",
    "    else:\n",
    "        # No single label met its threshold; treated as balanced\n",
    "        summary_sentence = (\n",
    "            f\"The case '{case_name}' is labeled '{case_label}' because, across the \"\n",
    "            \"courts with citations, no single treatment category clearly exceeds \"\n",
    "            \"its required share; the precedential signals are balanced.\"\n",
    "        )\n",
    "\n",
    "    lines.append(summary_sentence)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. How many citations and from which courts\n",
    "    # -----------------------------\n",
    "    lines.append(\n",
    "        f\"The case has {total_cites} incoming citation(s).\"\n",
    "    )\n",
    "\n",
    "    # Counts by court, using court names\n",
    "    level_fragments = []\n",
    "    for lvl in range(1, 6):\n",
    "        lvl_total = per_level_counts.get(lvl, {}).get(\"total\", 0)\n",
    "        court_name = COURT_LEVEL_NAMES.get(lvl, f\"Court level {lvl}\")\n",
    "        level_fragments.append(f\"{court_name}: {lvl_total}\")\n",
    "    levels_str = \", \".join(level_fragments)\n",
    "    lines.append(\n",
    "        f\"By court, citations are distributed as follows: {levels_str}.\"\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Details at the deciding court\n",
    "    # -----------------------------\n",
    "    dec_metrics = per_level_metrics.get(decision_level, {})\n",
    "    dec_counts = per_level_counts.get(decision_level, {})\n",
    "    dec_props = dec_metrics.get(\"proportions\", {})\n",
    "\n",
    "    dec_total = dec_counts.get(\"total\", 0)\n",
    "    c_pos = dec_counts.get(\"Positive\", 0)\n",
    "    c_neg = dec_counts.get(\"Negative\", 0)\n",
    "    c_neu = dec_counts.get(\"Neutral\", 0)\n",
    "    c_unk = dec_counts.get(\"Unknown\", 0)\n",
    "\n",
    "    p_pos = dec_props.get(\"Positive\", 0.0)\n",
    "    p_neg = dec_props.get(\"Negative\", 0.0)\n",
    "    p_neu = dec_props.get(\"Neutral\", 0.0)\n",
    "    p_unk = dec_props.get(\"Unknown\", 0.0)\n",
    "\n",
    "    if include_unknown:\n",
    "        share_str = (\n",
    "            f\"Positive={p_pos:.2f}, Negative={p_neg:.2f}, \"\n",
    "            f\"Neutral={p_neu:.2f}, Unknown={p_unk:.2f}\"\n",
    "        )\n",
    "        count_str = (\n",
    "            f\"{c_pos} positive, {c_neg} negative, \"\n",
    "            f\"{c_neu} neutral, {c_unk} unknown\"\n",
    "        )\n",
    "    else:\n",
    "        share_str = (\n",
    "            f\"Positive={p_pos:.2f}, Negative={p_neg:.2f}, Neutral={p_neu:.2f}\"\n",
    "        )\n",
    "        count_str = (\n",
    "            f\"{c_pos} positive, {c_neg} negative, {c_neu} neutral\"\n",
    "        )\n",
    "\n",
    "    if driver_label in {\"Positive\", \"Negative\", \"Neutral\", \"Unknown\"}:\n",
    "        decision_clause = (\n",
    "            f\"At the {decision_court_name}, the label is driven by \"\n",
    "            f\"{driver_label.lower()} treatment: based on {dec_total} citation(s) \"\n",
    "            f\"at this court, the weighted proportions are {share_str}, coming from \"\n",
    "            f\"{count_str} citation(s).\"\n",
    "        )\n",
    "    else:\n",
    "        decision_clause = (\n",
    "            f\"At the {decision_court_name}, based on {dec_total} citation(s), \"\n",
    "            f\"the weighted proportions are {share_str}, coming from {count_str} \"\n",
    "            \"citation(s), but no single label reaches its required share, so the \"\n",
    "            \"case is treated as 'Moderate' overall.\"\n",
    "        )\n",
    "\n",
    "    lines.append(decision_clause)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. If we moved down from a higher court, explain why\n",
    "    # -----------------------------\n",
    "    levels_with_cites = [\n",
    "        lvl for lvl in range(1, 6)\n",
    "        if per_level_counts.get(lvl, {}).get(\"total\", 0) > 0\n",
    "    ]\n",
    "\n",
    "    if used_lower_level and levels_with_cites:\n",
    "        highest_level = min(levels_with_cites)\n",
    "        if decision_level != highest_level:\n",
    "            highest_court_name = COURT_LEVEL_NAMES.get(\n",
    "                highest_level, f\"Court level {highest_level}\"\n",
    "            )\n",
    "            hl_metrics = per_level_metrics.get(highest_level, {})\n",
    "            hl_props = hl_metrics.get(\"proportions\", {})\n",
    "            hl_denom = hl_metrics.get(\"denom\", 0.0)\n",
    "\n",
    "            if hl_denom > 0.0:\n",
    "                hl_p_pos = hl_props.get(\"Positive\", 0.0)\n",
    "                hl_p_neg = hl_props.get(\"Negative\", 0.0)\n",
    "                hl_p_neu = hl_props.get(\"Neutral\", 0.0)\n",
    "                hl_p_unk = hl_props.get(\"Unknown\", 0.0)\n",
    "\n",
    "                if include_unknown:\n",
    "                    hl_share_str = (\n",
    "                        f\"Positive={hl_p_pos:.2f}, Negative={hl_p_neg:.2f}, \"\n",
    "                        f\"Neutral={hl_p_neu:.2f}, Unknown={hl_p_unk:.2f}\"\n",
    "                    )\n",
    "                else:\n",
    "                    hl_share_str = (\n",
    "                        f\"Positive={hl_p_pos:.2f}, Negative={hl_p_neg:.2f}, \"\n",
    "                        f\"Neutral={hl_p_neu:.2f}\"\n",
    "                    )\n",
    "\n",
    "                lines.append(\n",
    "                    f\"At the {highest_court_name} level (the highest court that cites this case), \"\n",
    "                    f\"the weighted proportions are {hl_share_str}. \"\n",
    "                    \"Because no single treatment label at that court met its configured threshold, \"\n",
    "                    f\"the algorithm looked to the next lower court and ultimately relied on the \"\n",
    "                    f\"{decision_court_name}, where the distribution shown above provided a clearer \"\n",
    "                    \"signal for the final label.\"\n",
    "                )\n",
    "            else:\n",
    "                lines.append(\n",
    "                    f\"The highest court with citations is the {highest_court_name}, \"\n",
    "                    \"but there were not enough labeled citations at that level to meet any \"\n",
    "                    f\"threshold, so the algorithm instead relied on citations from the \"\n",
    "                    f\"{decision_court_name} to determine the label.\"\n",
    "                )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5. How thresholds and weighting are applied\n",
    "    # -----------------------------\n",
    "    pos_thr = label_thresholds[\"Pos_p\"]\n",
    "    neg_thr = label_thresholds[\"Neg_p\"]\n",
    "    neu_thr = label_thresholds[\"Neu_p\"]\n",
    "    unk_thr = label_thresholds[\"Unk_p\"]\n",
    "\n",
    "    thr_parts = [\n",
    "        f\"Positive >= {pos_thr:.2f}\",\n",
    "        f\"Negative >= {neg_thr:.2f}\",\n",
    "        f\"Neutral >= {neu_thr:.2f}\",\n",
    "    ]\n",
    "    if include_unknown:\n",
    "        thr_parts.append(f\"Unknown >= {unk_thr:.2f}\")\n",
    "    thr_str = \", \".join(thr_parts)\n",
    "\n",
    "    if priority_order:\n",
    "        priority_str = \" > \".join(priority_order)\n",
    "        lines.append(\n",
    "            \"At each court, the model uses time- and jurisdiction-weighted citation counts \"\n",
    "            \"to compute the share of positive, negative, neutral (and, if included, unknown) \"\n",
    "            \"treatment. A label can drive the case outcome at that court only if its weighted \"\n",
    "            f\"share meets its configured threshold. For this run, the share thresholds are: {thr_str}. \"\n",
    "            f\"If more than one label meets its threshold, the priority order \"\n",
    "            f\"({priority_str}) is used to select the controlling label.\"\n",
    "        )\n",
    "    else:\n",
    "        lines.append(\n",
    "            \"At each court, the model uses time- and jurisdiction-weighted citation counts \"\n",
    "            \"to compute the share of positive, negative, and neutral treatment. \"\n",
    "            f\"A label can drive the case outcome at that court only if its weighted share \"\n",
    "            f\"meets its configured threshold. For this run, the share thresholds are: {thr_str}.\"\n",
    "        )\n",
    "\n",
    "    return \" \".join(lines)\n",
    "\n",
    "\n",
    "def _precompute_edge_scores(\n",
    "    session,\n",
    "    tmin_ord: Optional[int],\n",
    "    tmax_ord: Optional[int],\n",
    "    max_weight: float,\n",
    "    non_linear_recency_effect: bool,\n",
    "    jurisdiction_weights: Optional[Dict[str, float]],\n",
    "    force: bool,\n",
    "    echo: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compute and store recency_re and influence_score_alpha for CITES_TO edges\n",
    "    that matter for this run:\n",
    "\n",
    "      - If force=True: all (src:Case)-[r:CITES_TO]->(tgt:Case) edges.\n",
    "      - If force=False: only edges where tgt.case_label IS NULL.\n",
    "\n",
    "    - recency_re: normalized recency r_e in [0,1] based on the citing case decision_date.\n",
    "    - influence_score_alpha: alpha(e) = 1 + (MAX_WEIGHT - 1) * r_eff + Ji,\n",
    "      where r_eff = r_e or r_e^2 (if non_linear_recency_effect=True) and Ji is\n",
    "      the jurisdiction weight if configured.\n",
    "\n",
    "    The loop has an explicit safety stop to avoid unbounded scanning.\n",
    "    \"\"\"\n",
    "    # How many edges should we touch?\n",
    "    edge_count_rows = session.run(\n",
    "        Q_COUNT_CITES_EDGES, {\"force\": bool(force)}\n",
    "    ).data()\n",
    "    total_expected = edge_count_rows[0][\"n\"] if edge_count_rows else 0\n",
    "\n",
    "    if echo:\n",
    "        print(f\"Total CITES_TO edges to score (for this run): {total_expected}\")\n",
    "\n",
    "    if total_expected == 0:\n",
    "        return\n",
    "\n",
    "    after = -1\n",
    "    total_edges = 0\n",
    "    t0 = time.time()\n",
    "    last_print = t0\n",
    "\n",
    "    while True:\n",
    "        # Safety: if for some reason we have already processed as many edges\n",
    "        # as expected (or more), stop.\n",
    "        if total_edges >= total_expected:\n",
    "            break\n",
    "\n",
    "        edge_rows = session.run(\n",
    "            Q_PAGE_CITES_EDGES,\n",
    "            {\n",
    "                \"after_id\": after,\n",
    "                \"limit\": _EDGE_BATCH_SIZE,\n",
    "                \"force\": bool(force),\n",
    "            },\n",
    "        ).data()\n",
    "\n",
    "        if not edge_rows:\n",
    "            break\n",
    "\n",
    "        rows_to_write: List[Dict[str, Any]] = []\n",
    "        for er in edge_rows:\n",
    "            rel_id = er[\"rel_id\"]\n",
    "            decision_date = er.get(\"decision_date\")\n",
    "            jurisdiction_name = er.get(\"jurisdiction_name\")\n",
    "\n",
    "            # recency_re\n",
    "            r = _compute_normalized_recency(decision_date, tmin_ord, tmax_ord)\n",
    "\n",
    "            # alpha using the same helper as the labeling logic\n",
    "            alpha = _compute_alpha(\n",
    "                decision_date=decision_date,\n",
    "                jurisdiction_name=jurisdiction_name,\n",
    "                tmin_ord=tmin_ord,\n",
    "                tmax_ord=tmax_ord,\n",
    "                max_weight=max_weight,\n",
    "                non_linear_recency_effect=non_linear_recency_effect,\n",
    "                jurisdiction_weights=jurisdiction_weights,\n",
    "            )\n",
    "\n",
    "            rows_to_write.append(\n",
    "                {\n",
    "                    \"rel_id\": rel_id,\n",
    "                    \"recency_re\": float(r) if r is not None else None,\n",
    "                    \"influence_score_alpha\": float(alpha),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if rows_to_write:\n",
    "            session.run(Q_WRITE_EDGE_SCORES, {\"rows\": rows_to_write})\n",
    "\n",
    "        total_edges += len(rows_to_write)\n",
    "        after = edge_rows[-1][\"rel_id\"]\n",
    "\n",
    "        if echo and (time.time() - last_print >= 5.0):\n",
    "            print(\n",
    "                f\"Computed citation scores for {total_edges} CITES_TO edge(s)...\"\n",
    "            )\n",
    "            last_print = time.time()\n",
    "\n",
    "    if echo:\n",
    "        elapsed_min = (time.time() - t0) / 60.0\n",
    "        print(\n",
    "            f\"Finished computing citation scores for {total_edges} CITES_TO edge(s) \"\n",
    "            f\"in {elapsed_min:.1f} minutes.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb30ce80-2c60-467c-aef5-8488140b47ce",
   "metadata": {},
   "source": [
    "## Label All Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f236e3-2b90-47c5-b913-e0aeadd5fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Core function: label_all_cases\n",
    "# =========================\n",
    "\n",
    "def label_all_cases(\n",
    "    *,\n",
    "    force: bool = False,\n",
    "    echo: bool = False,\n",
    "    lower_level_court: bool = True,\n",
    "    include_unknown: bool = True,\n",
    "    label_thresholds: Optional[Dict[str, float]] = None,\n",
    "    default_label_priority: bool = True,\n",
    "    label_priority: Optional[List[str]] = None,\n",
    "    default_tmin_tmax: bool = True,\n",
    "    tmin_tmax: Optional[List[Any]] = None,\n",
    "    default_time_weight: bool = True,\n",
    "    time_weight: Optional[List[float]] = None,\n",
    "    non_linear_recency_effect: bool = False,\n",
    "    jurisdictions: Optional[Dict[str, Any]] = None,\n",
    "    results_csv: bool = False,\n",
    "    results_csv_filename: str = \"case_labeled_results.csv\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Label Case nodes as \"Good\", \"Bad\", \"Moderate\", or \"Unknown\" based on incoming\n",
    "    CITES_TO edges, court levels, continuous time-weighted citation counts, and\n",
    "    optional jurisdiction weights.\n",
    "\n",
    "    At the start of each run, this function also computes citation scores for\n",
    "    the relevant CITES_TO edges and stores:\n",
    "      - r.recency_re            (normalized recency r_e in [0,1])\n",
    "      - r.influence_score_alpha (alpha(e) consistent with the labeling logic).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    force : bool\n",
    "        If True, overwrite existing case_label values and precompute scores for\n",
    "        all case->case CITES_TO edges.\n",
    "        If False, only label cases where case_label is null/missing and only\n",
    "        precompute scores for edges into those cases.\n",
    "    echo : bool\n",
    "        If True, print progress information.\n",
    "    lower_level_court : bool\n",
    "        If False: only use the highest court level with citations; if no label\n",
    "        reaches its threshold there, classify as \"Moderate\" immediately.\n",
    "        If True: if the highest court level is mixed (no threshold), repeat the\n",
    "        procedure at the next lower court level, and so on.\n",
    "    include_unknown : bool\n",
    "        If True: include edges with treatment_label=\"Unknown\" in the weights and\n",
    "        proportions.\n",
    "        If False: ignore Unknown edges in the weighting and proportions.\n",
    "    label_thresholds : dict or None\n",
    "        Per-label thresholds for the proportions. Must contain:\n",
    "        {\"Pos_p\",\"Neg_p\",\"Neu_p\",\"Unk_p\"}.\n",
    "    default_label_priority : bool\n",
    "        If True, use the default label priority:\n",
    "            Unknown > Negative > Neutral > Positive\n",
    "        to break ties when multiple labels pass their thresholds.\n",
    "    label_priority : list of str or None\n",
    "        Only used when default_label_priority=False.\n",
    "    default_tmin_tmax : bool\n",
    "        If True (default): use dataset-based defaults for the recency window:\n",
    "            t_min = Q1, t_max = max decision date.\n",
    "    tmin_tmax : list or None\n",
    "        Only used when default_tmin_tmax=False.\n",
    "    default_time_weight : bool\n",
    "        If True (default): use alpha(e) in [1, 2.5].\n",
    "    time_weight : list of float or None\n",
    "        Only used when default_time_weight=False.\n",
    "    non_linear_recency_effect : bool\n",
    "        If False (default): use linear recency.\n",
    "        If True: use quadratic recency (r_e^2).\n",
    "    jurisdictions : dict or None\n",
    "        Optional dictionary of jurisdictions that should receive extra weight.\n",
    "        Keys are jurisdiction names (must match VALID_JURISDICTIONS exactly).\n",
    "        Values can be:\n",
    "          - a specific float Ji, or\n",
    "          - the string \"Default\", which is interpreted as Ji = MAX_WEIGHT / 2.\n",
    "    results_csv : bool\n",
    "        If True: write a CSV file with per-case, per-court-level metrics.\n",
    "    results_csv_filename : str\n",
    "        Name of the CSV file to write when results_csv=True.\n",
    "    \"\"\"\n",
    "    # --- label threshold configuration ---\n",
    "    if label_thresholds is None:\n",
    "        label_thresholds = {\n",
    "            \"Pos_p\": 0.55,\n",
    "            \"Neg_p\": 0.55,\n",
    "            \"Neu_p\": 0.55,\n",
    "            \"Unk_p\": 0.55,\n",
    "        }\n",
    "    else:\n",
    "        required_thr = {\"Pos_p\", \"Neg_p\", \"Neu_p\", \"Unk_p\"}\n",
    "        missing_thr = required_thr.difference(label_thresholds.keys())\n",
    "        if missing_thr:\n",
    "            raise ValueError(\n",
    "                f\"label_thresholds is missing required keys: {sorted(missing_thr)}\"\n",
    "            )\n",
    "        # cast to float\n",
    "        label_thresholds = {\n",
    "            k: float(label_thresholds[k]) for k in (\"Pos_p\", \"Neg_p\", \"Neu_p\", \"Unk_p\")\n",
    "        }\n",
    "\n",
    "    # --- label priority configuration ---\n",
    "    if default_label_priority and label_priority is not None:\n",
    "        raise ValueError(\n",
    "            \"You provided label_priority but default_label_priority=True. \"\n",
    "            \"Set default_label_priority=False to use custom label_priority.\"\n",
    "        )\n",
    "\n",
    "    if default_label_priority:\n",
    "        base_priority = [\"unk\", \"neg\", \"neu\", \"pos\"]  # Unknown > Negative > Neutral > Positive\n",
    "    else:\n",
    "        if label_priority is None:\n",
    "            raise ValueError(\n",
    "                \"label_priority must be provided when default_label_priority=False.\"\n",
    "            )\n",
    "        base_priority = label_priority\n",
    "\n",
    "    # --- tmin/tmax configuration (recency window) ---\n",
    "    if default_tmin_tmax and tmin_tmax is not None:\n",
    "        raise ValueError(\n",
    "            \"You provided tmin_tmax but default_tmin_tmax=True. \"\n",
    "            \"Set default_tmin_tmax=False to use custom tmin_tmax.\"\n",
    "        )\n",
    "\n",
    "    # --- time weight range configuration ---\n",
    "    if default_time_weight and time_weight is not None:\n",
    "        raise ValueError(\n",
    "            \"You provided time_weight but default_time_weight=True. \"\n",
    "            \"Set default_time_weight=False to use custom time_weight.\"\n",
    "        )\n",
    "\n",
    "    if default_time_weight:\n",
    "        max_weight = 2.5  # alpha(e) in [1, 2.5]\n",
    "    else:\n",
    "        if time_weight is None or len(time_weight) != 2:\n",
    "            raise ValueError(\n",
    "                \"time_weight must be a 2-element list [1.0, MAX_WEIGHT] \"\n",
    "                \"when default_time_weight=False.\"\n",
    "            )\n",
    "        min_w, max_w = time_weight\n",
    "        min_w = float(min_w)\n",
    "        max_w = float(max_w)\n",
    "        if abs(min_w - 1.0) > 1e-8:\n",
    "            raise ValueError(\n",
    "                f\"time_weight[0] must be 1.0 (got {min_w}). \"\n",
    "                \"The minimum alpha(e) is fixed at 1.0.\"\n",
    "            )\n",
    "        if max_w < 1.0:\n",
    "            raise ValueError(\n",
    "                f\"time_weight[1] must be >= 1.0 (got {max_w}).\"\n",
    "            )\n",
    "        max_weight = max_w\n",
    "\n",
    "    # --- jurisdiction weights configuration ---\n",
    "    jurisdiction_weights: Optional[Dict[str, float]] = None\n",
    "    if jurisdictions is not None:\n",
    "        if not isinstance(jurisdictions, dict):\n",
    "            raise ValueError(\n",
    "                \"jurisdictions must be a dict of {jurisdiction_name: value}.\"\n",
    "            )\n",
    "\n",
    "        invalid = [name for name in jurisdictions.keys() if name not in VALID_JURISDICTIONS]\n",
    "        if invalid:\n",
    "            raise ValueError(\n",
    "                \"Invalid jurisdiction name(s) in 'jurisdictions': \"\n",
    "                + \", \".join(sorted(invalid))\n",
    "            )\n",
    "\n",
    "        jurisdiction_weights = {}\n",
    "        for name, val in jurisdictions.items():\n",
    "            if isinstance(val, str) and val.strip().lower() == \"default\":\n",
    "                w = max_weight / 2.0\n",
    "            else:\n",
    "                try:\n",
    "                    w = float(val)\n",
    "                except (TypeError, ValueError):\n",
    "                    raise ValueError(\n",
    "                        f\"Jurisdiction weight for {name!r} must be a float or 'Default'.\"\n",
    "                    )\n",
    "            jurisdiction_weights[name] = w\n",
    "\n",
    "    # --- connect to Neo4j ---\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "    results_rows: List[Dict[str, Any]] = []\n",
    "    case_label_counts = {\"Good\": 0, \"Bad\": 0, \"Moderate\": 0, \"Unknown\": 0}\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            session = driver.session(\n",
    "                database=NEO4J_DATABASE,\n",
    "                notifications_min_severity=\"OFF\",\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Older drivers may not support notifications_min_severity\n",
    "            session = driver.session(database=NEO4J_DATABASE)\n",
    "\n",
    "        with session as s:\n",
    "            # --- global time stats (for default tmin/tmax) ---\n",
    "            time_stats = _compute_time_quartiles(s)\n",
    "\n",
    "            if default_tmin_tmax:\n",
    "                q1_ord = time_stats.get(\"q1\")\n",
    "                tmax_global = time_stats.get(\"max\")\n",
    "                if q1_ord is None or tmax_global is None or tmax_global <= q1_ord:\n",
    "                    # Degenerate or missing → no recency effect\n",
    "                    tmin_ord = None\n",
    "                    tmax_ord = None\n",
    "                else:\n",
    "                    tmin_ord = q1_ord\n",
    "                    tmax_ord = tmax_global\n",
    "            else:\n",
    "                if tmin_tmax is None or len(tmin_tmax) != 2:\n",
    "                    raise ValueError(\n",
    "                        \"tmin_tmax must be a 2-element list [tmin, tmax] \"\n",
    "                        \"when default_tmin_tmax=False.\"\n",
    "                    )\n",
    "                tmin_raw, tmax_raw = tmin_tmax\n",
    "                tmin_ord = _to_ordinal(tmin_raw)\n",
    "                tmax_ord = _to_ordinal(tmax_raw)\n",
    "                if tmin_ord is None or tmax_ord is None:\n",
    "                    raise ValueError(\n",
    "                        \"Could not parse tmin or tmax into valid dates. \"\n",
    "                        \"They can be date/datetime objects or ISO date strings.\"\n",
    "                    )\n",
    "                if tmax_ord <= tmin_ord:\n",
    "                    raise ValueError(\n",
    "                        f\"tmax must be strictly greater than tmin (got tmin={tmin_raw}, tmax={tmax_raw}).\"\n",
    "                    )\n",
    "\n",
    "            # --- normalize label priority now that include_unknown is known ---\n",
    "            priority_order = _normalize_priority_list(\n",
    "                base_priority,\n",
    "                include_unknown=include_unknown,\n",
    "            )\n",
    "\n",
    "            # --- count total cases to label ---\n",
    "            total_cases = s.run(\n",
    "                Q_COUNT_CASES,\n",
    "                {\"force\": bool(force)},\n",
    "            ).data()[0][\"n\"]\n",
    "\n",
    "            if echo:\n",
    "                print(f\"Total cases to label: {total_cases}\")\n",
    "                if tmin_ord is not None and tmax_ord is not None:\n",
    "                    tmin_dt = datetime.fromordinal(tmin_ord).date()\n",
    "                    tmax_dt = datetime.fromordinal(tmax_ord).date()\n",
    "                    print(\n",
    "                        f\"Using recency window tmin={tmin_dt.isoformat()}, \"\n",
    "                        f\"tmax={tmax_dt.isoformat()}, max_weight={max_weight}, \"\n",
    "                        f\"non_linear_recency_effect={non_linear_recency_effect}.\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"No valid recency window; all edges will have alpha(e) = 1.0 \"\n",
    "                        \"(before any jurisdiction weight Ji).\"\n",
    "                    )\n",
    "                if jurisdiction_weights:\n",
    "                    print(\n",
    "                        f\"Using jurisdiction weights for {len(jurisdiction_weights)} jurisdiction(s).\"\n",
    "                    )\n",
    "\n",
    "            # --- precompute citation scores on relevant CITES_TO edges ---\n",
    "            _precompute_edge_scores(\n",
    "                session=s,\n",
    "                tmin_ord=tmin_ord,\n",
    "                tmax_ord=tmax_ord,\n",
    "                max_weight=max_weight,\n",
    "                non_linear_recency_effect=non_linear_recency_effect,\n",
    "                jurisdiction_weights=jurisdiction_weights,\n",
    "                force=bool(force),\n",
    "                echo=echo,\n",
    "            )\n",
    "\n",
    "            # --- main paging loop over Case nodes ---\n",
    "            after = -1\n",
    "            processed_cases = 0\n",
    "            t0 = time.time()\n",
    "            last_print = time.time()\n",
    "\n",
    "            while True:\n",
    "                case_rows = s.run(\n",
    "                    Q_PAGE_CASES,\n",
    "                    {\n",
    "                        \"after_id\": after,\n",
    "                        \"limit\": _CASE_BATCH_SIZE,\n",
    "                        \"force\": bool(force),\n",
    "                    },\n",
    "                ).data()\n",
    "\n",
    "                if not case_rows:\n",
    "                    break\n",
    "\n",
    "                for c_row in case_rows:\n",
    "                    after = c_row[\"neo_id\"]\n",
    "                    case_id = c_row[\"case_id\"]\n",
    "                    case_name = c_row[\"case_name\"] or \"\"\n",
    "\n",
    "                    # --- fetch incoming CITES_TO edges for this case ---\n",
    "                    edge_rows = s.run(\n",
    "                        Q_INCOMING_EDGES_FOR_CASE,\n",
    "                        {\"case_id\": case_id},\n",
    "                    ).data()\n",
    "\n",
    "                    edges_by_level: Dict[int, List[Dict[str, Any]]] = defaultdict(list)\n",
    "\n",
    "                    for er in edge_rows:\n",
    "                        lvl = er.get(\"court_level\")\n",
    "                        if lvl is None:\n",
    "                            # If no court level, skip this edge for scoring\n",
    "                            continue\n",
    "                        try:\n",
    "                            lvl_int = int(lvl)\n",
    "                        except (TypeError, ValueError):\n",
    "                            continue\n",
    "                        if not (1 <= lvl_int <= 5):\n",
    "                            continue\n",
    "\n",
    "                        edges_by_level[lvl_int].append(\n",
    "                            {\n",
    "                                \"label\": er.get(\"label\"),\n",
    "                                \"decision_date\": er.get(\"decision_date\"),\n",
    "                                \"jurisdiction_name\": er.get(\"jurisdiction_name\"),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    # --- compute per-level metrics (counts, weights, proportions) ---\n",
    "                    per_level_metrics: Dict[int, Dict[str, Any]] = {}\n",
    "                    per_level_counts: Dict[int, Dict[str, int]] = {}\n",
    "\n",
    "                    total_cites = 0\n",
    "\n",
    "                    for lvl in range(1, 6):\n",
    "                        lvl_edges = edges_by_level.get(lvl, [])\n",
    "                        metrics = _compute_level_metrics(\n",
    "                            lvl_edges,\n",
    "                            include_unknown=include_unknown,\n",
    "                            tmin_ord=tmin_ord,\n",
    "                            tmax_ord=tmax_ord,\n",
    "                            max_weight=max_weight,\n",
    "                            non_linear_recency_effect=non_linear_recency_effect,\n",
    "                            jurisdiction_weights=jurisdiction_weights,\n",
    "                        )\n",
    "                        per_level_metrics[lvl] = metrics\n",
    "\n",
    "                        counts = metrics[\"counts\"]\n",
    "                        lvl_total = (\n",
    "                            counts[\"Positive\"]\n",
    "                            + counts[\"Neutral\"]\n",
    "                            + counts[\"Negative\"]\n",
    "                            + counts[\"Unknown\"]\n",
    "                        )\n",
    "                        per_level_counts[lvl] = {\n",
    "                            \"total\": lvl_total,\n",
    "                            \"Positive\": counts[\"Positive\"],\n",
    "                            \"Neutral\": counts[\"Neutral\"],\n",
    "                            \"Negative\": counts[\"Negative\"],\n",
    "                            \"Unknown\": counts[\"Unknown\"],\n",
    "                        }\n",
    "                        total_cites += lvl_total\n",
    "\n",
    "                    levels_with_cites = [\n",
    "                        lvl for lvl in range(1, 6)\n",
    "                        if per_level_counts[lvl][\"total\"] > 0\n",
    "                    ]\n",
    "\n",
    "                    # --- decide label for this case ---\n",
    "                    used_lower_level = False\n",
    "                    decision_level: Optional[int] = None  # numeric level 1–5\n",
    "                    case_label: str\n",
    "                    driver_label: Optional[str] = None\n",
    "\n",
    "                    if total_cites == 0:\n",
    "                        # No signal at all → label as \"Unknown\" at the case level\n",
    "                        case_label = \"Unknown\"\n",
    "                        decision_level = None\n",
    "                        driver_label = None\n",
    "                    else:\n",
    "                        if not levels_with_cites:\n",
    "                            # Should not happen if total_cites > 0, but be safe.\n",
    "                            case_label = \"Unknown\"\n",
    "                            decision_level = None\n",
    "                            driver_label = None\n",
    "                        else:\n",
    "                            highest_level = min(levels_with_cites)\n",
    "\n",
    "                            if not lower_level_court:\n",
    "                                # Only consider highest-level court; if no threshold,\n",
    "                                # classify as Moderate immediately.\n",
    "                                metrics_h = per_level_metrics[highest_level]\n",
    "                                label_h, driver_h = _decide_label_from_metrics(\n",
    "                                    metrics_h,\n",
    "                                    include_unknown=include_unknown,\n",
    "                                    label_thresholds=label_thresholds,\n",
    "                                    priority_order=priority_order,\n",
    "                                )\n",
    "                                if label_h is None:\n",
    "                                    case_label = \"Moderate\"\n",
    "                                    decision_level = highest_level\n",
    "                                    driver_label = None\n",
    "                                else:\n",
    "                                    case_label = label_h\n",
    "                                    decision_level = highest_level\n",
    "                                    driver_label = driver_h\n",
    "                            else:\n",
    "                                # Walk from highest level down to lowest level\n",
    "                                case_label = \"Moderate\"\n",
    "                                decision_level = levels_with_cites[-1]  # default\n",
    "                                driver_label = None\n",
    "\n",
    "                                for idx, lvl in enumerate(sorted(levels_with_cites)):\n",
    "                                    lvl_metrics = per_level_metrics[lvl]\n",
    "                                    lvl_label, lvl_driver = _decide_label_from_metrics(\n",
    "                                        lvl_metrics,\n",
    "                                        include_unknown=include_unknown,\n",
    "                                        label_thresholds=label_thresholds,\n",
    "                                        priority_order=priority_order,\n",
    "                                    )\n",
    "                                    if lvl_label is not None:\n",
    "                                        case_label = lvl_label\n",
    "                                        decision_level = lvl\n",
    "                                        driver_label = lvl_driver\n",
    "                                        # If idx > 0, we used a lower level than the highest\n",
    "                                        used_lower_level = (idx > 0)\n",
    "                                        break\n",
    "                                else:\n",
    "                                    # No level reached its threshold; keep \"Moderate\"\n",
    "                                    # Set decision_level to the lowest level we checked\n",
    "                                    decision_level = sorted(levels_with_cites)[-1]\n",
    "                                    driver_label = None\n",
    "                                    used_lower_level = (len(levels_with_cites) > 1)\n",
    "\n",
    "                    # --- build rationale text ---\n",
    "                    label_rationale = _build_label_rationale(\n",
    "                        case_name=case_name,\n",
    "                        total_cites=total_cites,\n",
    "                        per_level_counts=per_level_counts,\n",
    "                        per_level_metrics=per_level_metrics,\n",
    "                        decision_level=decision_level,\n",
    "                        case_label=case_label,\n",
    "                        driver_label=driver_label,\n",
    "                        include_unknown=include_unknown,\n",
    "                        used_lower_level=used_lower_level,\n",
    "                        label_thresholds=label_thresholds,\n",
    "                        priority_order=priority_order,\n",
    "                    )\n",
    "\n",
    "                    # --- convert decision level to court name for storage ---\n",
    "                    decision_level_name = (\n",
    "                        _court_level_to_name(decision_level)\n",
    "                        if decision_level is not None\n",
    "                        else None\n",
    "                    )\n",
    "\n",
    "                    # --- write back to Neo4j ---\n",
    "                    s.run(\n",
    "                        Q_WRITE_CASE_LABEL,\n",
    "                        {\n",
    "                            \"case_id\": case_id,\n",
    "                            \"case_label\": case_label,\n",
    "                            \"decision_level\": decision_level_name,\n",
    "                            \"label_rationale\": label_rationale,\n",
    "                        },\n",
    "                    )\n",
    "\n",
    "                    # --- update counters ---\n",
    "                    if case_label in case_label_counts:\n",
    "                        case_label_counts[case_label] += 1\n",
    "                    else:\n",
    "                        case_label_counts[case_label] = 1\n",
    "\n",
    "                    # --- accumulate CSV row ---\n",
    "                    if results_csv:\n",
    "                        row_data: Dict[str, Any] = {}\n",
    "                        row_data[\"Case ID\"] = case_id\n",
    "                        row_data[\"Case Name\"] = case_name\n",
    "                        row_data[\"Total Number of Citations\"] = total_cites\n",
    "\n",
    "                        for lvl in range(1, 6):\n",
    "                            counts = per_level_counts[lvl]\n",
    "                            metrics = per_level_metrics[lvl]\n",
    "                            weights = metrics[\"weights\"]\n",
    "                            props = metrics[\"proportions\"]\n",
    "\n",
    "                            # Counts\n",
    "                            row_data[\n",
    "                                f\"Number of Citations from Court Level {lvl}\"\n",
    "                            ] = counts[\"total\"]\n",
    "                            row_data[\n",
    "                                f\"Number of Positive Citations from Court Level {lvl}\"\n",
    "                            ] = counts[\"Positive\"]\n",
    "                            row_data[\n",
    "                                f\"Number of Neutral Citations from Court Level {lvl}\"\n",
    "                            ] = counts[\"Neutral\"]\n",
    "                            row_data[\n",
    "                                f\"Number of Negative Citations from Court Level {lvl}\"\n",
    "                            ] = counts[\"Negative\"]\n",
    "                            row_data[\n",
    "                                f\"Number of Unknown Citations from Court Level {lvl}\"\n",
    "                            ] = counts[\"Unknown\"]\n",
    "\n",
    "                            # Weights\n",
    "                            row_data[\n",
    "                                f\"Positive Weight from Court Level {lvl}\"\n",
    "                            ] = float(weights[\"Positive\"])\n",
    "                            row_data[\n",
    "                                f\"Neutral Weight from Court Level {lvl}\"\n",
    "                            ] = float(weights[\"Neutral\"])\n",
    "                            row_data[\n",
    "                                f\"Negative Weight from Court Level {lvl}\"\n",
    "                            ] = float(weights[\"Negative\"])\n",
    "                            # For Unknown, if include_unknown=False this will be 0.0\n",
    "                            row_data[\n",
    "                                f\"Unknown Weight from Court Level {lvl}\"\n",
    "                            ] = float(weights[\"Unknown\"])\n",
    "\n",
    "                            # Proportions\n",
    "                            row_data[\n",
    "                                f\"Positive Proportion from Court Level {lvl}\"\n",
    "                            ] = float(props[\"Positive\"])\n",
    "                            row_data[\n",
    "                                f\"Neutral Proportion from Court Level {lvl}\"\n",
    "                            ] = float(props[\"Neutral\"])\n",
    "                            row_data[\n",
    "                                f\"Negative Proportion from Court Level {lvl}\"\n",
    "                            ] = float(props[\"Negative\"])\n",
    "                            row_data[\n",
    "                                f\"Unknown Proportion from Court Level {lvl}\"\n",
    "                            ] = float(props[\"Unknown\"])\n",
    "\n",
    "                        row_data[\"Court Level Decision\"] = (\n",
    "                            decision_level_name if decision_level_name is not None else \"\"\n",
    "                        )\n",
    "                        row_data[\"Case Label\"] = case_label\n",
    "                        row_data[\"Rationale\"] = label_rationale\n",
    "\n",
    "                        results_rows.append(row_data)\n",
    "\n",
    "                    processed_cases += 1\n",
    "\n",
    "                    # Progress printing every ~5 seconds\n",
    "                    now = time.time()\n",
    "                    if echo and (now - last_print >= 5.0):\n",
    "                        print(\n",
    "                            f\"Labeled {processed_cases} / {total_cases} cases \"\n",
    "                            f\"({(processed_cases / max(total_cases, 1)) * 100:.1f}%).\"\n",
    "                        )\n",
    "                        last_print = now\n",
    "\n",
    "            # --- final summary ---\n",
    "            if echo:\n",
    "                elapsed_min = (time.time() - t0) / 60.0\n",
    "                print(f\"\\nCompleted labeling in {elapsed_min:.1f} minutes.\")\n",
    "                print(\"Case label counts:\")\n",
    "                print(f\"  Good     : {case_label_counts.get('Good', 0)}\")\n",
    "                print(f\"  Bad      : {case_label_counts.get('Bad', 0)}\")\n",
    "                print(f\"  Moderate : {case_label_counts.get('Moderate', 0)}\")\n",
    "                print(f\"  Unknown  : {case_label_counts.get('Unknown', 0)}\")\n",
    "\n",
    "            # --- write CSV if requested ---\n",
    "            if results_csv:\n",
    "                df = pd.DataFrame(results_rows)\n",
    "\n",
    "                # Build explicit column order to match your spec\n",
    "                columns: List[str] = [\n",
    "                    \"Case ID\",\n",
    "                    \"Case Name\",\n",
    "                    \"Total Number of Citations\",\n",
    "                ]\n",
    "                for lvl in range(1, 6):\n",
    "                    columns.extend(\n",
    "                        [\n",
    "                            f\"Number of Citations from Court Level {lvl}\",\n",
    "                            f\"Number of Positive Citations from Court Level {lvl}\",\n",
    "                            f\"Number of Neutral Citations from Court Level {lvl}\",\n",
    "                            f\"Number of Negative Citations from Court Level {lvl}\",\n",
    "                            f\"Number of Unknown Citations from Court Level {lvl}\",\n",
    "                            f\"Positive Weight from Court Level {lvl}\",\n",
    "                            f\"Neutral Weight from Court Level {lvl}\",\n",
    "                            f\"Negative Weight from Court Level {lvl}\",\n",
    "                            f\"Unknown Weight from Court Level {lvl}\",\n",
    "                            f\"Positive Proportion from Court Level {lvl}\",\n",
    "                            f\"Neutral Proportion from Court Level {lvl}\",\n",
    "                            f\"Negative Proportion from Court Level {lvl}\",\n",
    "                            f\"Unknown Proportion from Court Level {lvl}\",\n",
    "                        ]\n",
    "                    )\n",
    "                columns.extend(\n",
    "                    [\n",
    "                        \"Court Level Decision\",\n",
    "                        \"Case Label\",\n",
    "                        \"Rationale\",\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Ensure all columns exist\n",
    "                for col in columns:\n",
    "                    if col not in df.columns:\n",
    "                        df[col] = \"\"\n",
    "\n",
    "                df = df[columns]\n",
    "                df.to_csv(results_csv_filename, index=False)\n",
    "                if echo:\n",
    "                    print(f\"\\nWrote case-level results CSV → {results_csv_filename}\")\n",
    "\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "# =========================\n",
    "# Example call (commented)\n",
    "# =========================\n",
    "# label_all_cases(\n",
    "#     force=False,\n",
    "#     echo=True,\n",
    "#     lower_level_court=True,\n",
    "#     include_unknown=True,\n",
    "#     label_thresholds=None,\n",
    "#     default_label_priority=True,\n",
    "#     label_priority=None,\n",
    "#     default_tmin_tmax=True,\n",
    "#     tmin_tmax=None,\n",
    "#     default_time_weight=True,\n",
    "#     time_weight=None,\n",
    "#     non_linear_recency_effect=False,\n",
    "#     jurisdictions={\n",
    "#         \"Alabama\": \"Default\",   # Ji = MAX_WEIGHT / 2\n",
    "#         \"California\": 1.0,      # explicit Ji\n",
    "#     },\n",
    "#     results_csv=True,\n",
    "#     results_csv_filename=\"case_labeled_results_continuous_time_with_jurisdictions.csv\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837e75d-5387-4520-9b68-74872319d4eb",
   "metadata": {},
   "source": [
    "## Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90390b2-7993-4b49-95a3-be70c2142c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases to label: 3648\n",
      "Using recency window tmin=2001-06-14, tmax=2025-10-08, max_weight=2.5, non_linear_recency_effect=False.\n",
      "Total CITES_TO edges to score (for this run): 5491\n",
      "Finished computing citation scores for 5491 CITES_TO edge(s) in 0.0 minutes.\n",
      "Labeled 50 / 3648 cases (1.4%).\n",
      "Labeled 113 / 3648 cases (3.1%).\n",
      "Labeled 182 / 3648 cases (5.0%).\n",
      "Labeled 252 / 3648 cases (6.9%).\n",
      "Labeled 324 / 3648 cases (8.9%).\n",
      "Labeled 396 / 3648 cases (10.9%).\n",
      "Labeled 467 / 3648 cases (12.8%).\n",
      "Labeled 537 / 3648 cases (14.7%).\n",
      "Labeled 606 / 3648 cases (16.6%).\n",
      "Labeled 678 / 3648 cases (18.6%).\n",
      "Labeled 749 / 3648 cases (20.5%).\n",
      "Labeled 819 / 3648 cases (22.5%).\n",
      "Labeled 891 / 3648 cases (24.4%).\n",
      "Labeled 963 / 3648 cases (26.4%).\n",
      "Labeled 1036 / 3648 cases (28.4%).\n",
      "Labeled 1109 / 3648 cases (30.4%).\n",
      "Labeled 1181 / 3648 cases (32.4%).\n",
      "Labeled 1251 / 3648 cases (34.3%).\n",
      "Labeled 1323 / 3648 cases (36.3%).\n",
      "Labeled 1394 / 3648 cases (38.2%).\n",
      "Labeled 1465 / 3648 cases (40.2%).\n",
      "Labeled 1537 / 3648 cases (42.1%).\n",
      "Labeled 1608 / 3648 cases (44.1%).\n",
      "Labeled 1678 / 3648 cases (46.0%).\n",
      "Labeled 1749 / 3648 cases (47.9%).\n",
      "Labeled 1819 / 3648 cases (49.9%).\n",
      "Labeled 1891 / 3648 cases (51.8%).\n",
      "Labeled 1962 / 3648 cases (53.8%).\n",
      "Labeled 2033 / 3648 cases (55.7%).\n",
      "Labeled 2106 / 3648 cases (57.7%).\n",
      "Labeled 2180 / 3648 cases (59.8%).\n",
      "Labeled 2253 / 3648 cases (61.8%).\n",
      "Labeled 2327 / 3648 cases (63.8%).\n",
      "Labeled 2400 / 3648 cases (65.8%).\n",
      "Labeled 2472 / 3648 cases (67.8%).\n",
      "Labeled 2544 / 3648 cases (69.7%).\n",
      "Labeled 2617 / 3648 cases (71.7%).\n",
      "Labeled 2691 / 3648 cases (73.8%).\n",
      "Labeled 2765 / 3648 cases (75.8%).\n",
      "Labeled 2838 / 3648 cases (77.8%).\n",
      "Labeled 2912 / 3648 cases (79.8%).\n",
      "Labeled 2986 / 3648 cases (81.9%).\n",
      "Labeled 3060 / 3648 cases (83.9%).\n",
      "Labeled 3135 / 3648 cases (85.9%).\n",
      "Labeled 3208 / 3648 cases (87.9%).\n",
      "Labeled 3281 / 3648 cases (89.9%).\n",
      "Labeled 3354 / 3648 cases (91.9%).\n",
      "Labeled 3425 / 3648 cases (93.9%).\n",
      "Labeled 3497 / 3648 cases (95.9%).\n",
      "Labeled 3570 / 3648 cases (97.9%).\n",
      "Labeled 3643 / 3648 cases (99.9%).\n",
      "\n",
      "Completed labeling in 4.3 minutes.\n",
      "Case label counts:\n",
      "  Good     : 869\n",
      "  Bad      : 19\n",
      "  Moderate : 147\n",
      "  Unknown  : 2613\n"
     ]
    }
   ],
   "source": [
    "label_all_cases(\n",
    "    force=True,\n",
    "    echo=True,\n",
    "    lower_level_court=True,\n",
    "    include_unknown=True,\n",
    "    label_thresholds=None,\n",
    "    default_label_priority=True,\n",
    "    label_priority=None,\n",
    "    default_tmin_tmax=True,\n",
    "    tmin_tmax=None,\n",
    "    default_time_weight=True,\n",
    "    time_weight=None,\n",
    "    non_linear_recency_effect=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be670531-4a6e-4856-8a33-144cc813dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_all_cases(\n",
    "#     force=True,\n",
    "#     echo=True,\n",
    "#     lower_level_court=True,\n",
    "#     include_unknown=True,\n",
    "#     label_thresholds=None,\n",
    "#     default_label_priority=True,\n",
    "#     label_priority=None,\n",
    "#     default_tmin_tmax=True,\n",
    "#     tmin_tmax=None,\n",
    "#     default_time_weight=True,\n",
    "#     time_weight=None,\n",
    "#     non_linear_recency_effect=False,\n",
    "#     jurisdictions={\n",
    "#         \"Alabama\": \"Default\",   # Ji = MAX_WEIGHT / 2\n",
    "#         \"U.S. Court of Appeals for the Sixth Circuit\": \"Default\",      # explicit Ji\n",
    "#     },\n",
    "#     results_csv=True,\n",
    "#     results_csv_filename=\"case_labeled_results_continuous_time.csv\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f374f3-eafd-476b-b14c-fcb2271c8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds = {\n",
    "#     \"Pos_p\": 0.55,  \n",
    "#     \"Neg_p\": 0.75,  \n",
    "#     \"Neu_p\": 0.75,  \n",
    "#     \"Unk_p\": 0.55,  \n",
    "# }\n",
    "\n",
    "# label_all_cases(\n",
    "#     force=True,\n",
    "#     echo=True,\n",
    "#     lower_level_court=True,\n",
    "#     include_unknown=True,\n",
    "#     label_thresholds=thresholds,\n",
    "#     default_label_priority=True,\n",
    "#     label_priority=None,\n",
    "#     default_tmin_tmax=True,\n",
    "#     tmin_tmax=None,\n",
    "#     default_time_weight=True,\n",
    "#     time_weight=None,\n",
    "#     non_linear_recency_effect=False,\n",
    "#     jurisdictions={\n",
    "#         \"Alabama\": \"Default\",   # Ji = MAX_WEIGHT / 2\n",
    "#         \"U.S. Court of Appeals for the Sixth Circuit\": \"Default\",      # explicit Ji\n",
    "#     },\n",
    "#     results_csv=True,\n",
    "#     results_csv_filename=\"case_labeled_results_continuous_time.csv\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
