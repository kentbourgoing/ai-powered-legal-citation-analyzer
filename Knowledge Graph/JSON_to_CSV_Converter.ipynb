{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d04ec5e",
   "metadata": {},
   "source": [
    "# How to use `json_to_csv`\n",
    "\n",
    "## What this does\n",
    "\n",
    "`json_to_csv` converts **ADAH legal case JSON** into CSVs ready for **Neo4j Aura Data Importer**.\n",
    "\n",
    "It expects **ADAH-style objects** (each case has at least:\n",
    "\n",
    "* `ID` (int-like)\n",
    "* `opinion` (string)\n",
    "\n",
    "You can choose how to store opinion text:\n",
    "\n",
    "* **`opinion_storage=\"chunks\"` (default):** sentence-aware chunking into `OpinionChunk` rows (linked back to `Case`).\n",
    "* **`opinion_storage=\"inline\"`:** full opinion text stored as a single property on the `Case` row (no chunk rows are produced).\n",
    "\n",
    "---\n",
    "\n",
    "## Current behavior & data shaping\n",
    "\n",
    "* **Scope: ADAH only**\n",
    "\n",
    "  * The converter checks that the JSON array contains ADAH-style cases (objects with `ID` and `opinion`).\n",
    "  * If no ADAH cases are found, it raises a `ValueError`.\n",
    "\n",
    "* **ADAH field handling**\n",
    "\n",
    "  * `ID` → written to **`id`** in `cases.csv`.\n",
    "  * `date_filed` → written to **`decision_date`**.\n",
    "  * `docket_number` or **`docker_number`** (ADAH typo) → written to **`docket_number`**.\n",
    "  * `case_full_name` is preserved on ADAH rows; literal `\"NULL\"` becomes `\"N/A\"`.\n",
    "\n",
    "* **Unified citation column**\n",
    "\n",
    "  * ADAH `citation` can be a string or list.\n",
    "  * All values are joined into **`citation_pipe`** (joined with `\" | \"`).\n",
    "\n",
    "* **Stable ADAH `court_id`**\n",
    "\n",
    "  * `court_id` is a deterministic integer based on ADAH court name + abbreviation:\n",
    "\n",
    "    ```text\n",
    "    court_id = stable_crc32(\"adah-court:{court_name}|{court_abbrev}\")\n",
    "    ```\n",
    "\n",
    "* **Court level inference (`court_level`)**\n",
    "\n",
    "  * For each case, the court’s level is inferred from **`jurisdiction_inferred`**.\n",
    "  * Levels are integers **1–5** (1 = highest, 5 = lowest).\n",
    "  * Unknown or unmatched jurisdictions default to **5**.\n",
    "  * If the same court appears with multiple levels, the **smallest** level seen is kept.\n",
    "\n",
    "* **Jurisdictions output**\n",
    "\n",
    "  * Jurisdictions are built from ADAH `jurisdiction_inferred`.\n",
    "  * Final `jurisdictions.csv` is in **minimal ADAH form** with columns:\n",
    "\n",
    "    * `id` (synthetic, deterministic int per jurisdiction name)\n",
    "    * `jurisdiction_name`\n",
    "\n",
    "* **Optional CourtListener URL**\n",
    "\n",
    "  * Set **`include_url=True`** to add **`court_listener_url`** to `cases.csv`.\n",
    "  * URL pattern: `https://www.courtlistener.com/opinion/{ID}/{slug}/`\n",
    "    * `slug` is taken from the tail of `absolute_url` in the ADAH JSON.\n",
    "  * `cites_to.csv` **always** includes CourtListener URLs for source/target cases, independent of `include_url`.\n",
    "\n",
    "* **Filtering behavior**\n",
    "\n",
    "  * By default, the converter keeps:\n",
    "\n",
    "    * cases with `Citing_Relationship = \"adah\"` (the ADAH seed cases), and  \n",
    "    * cases with `Citing_Relationship = \"citing_adah\"` (cases that cite ADAH).\n",
    "\n",
    "  * **`subset`** (model-dev sampler):\n",
    "\n",
    "    * When `subset` is set:\n",
    "      1. Select up to `subset` ADAH cases where `Citing_Relationship = \"adah\"`.\n",
    "      2. Then add **all cases that cite any of those ADAH cases** (via `cite_to`).\n",
    "    * This gives a smaller, but still connected, ADAH subgraph.\n",
    "\n",
    "  * **`only_case_ids=[...]`**:\n",
    "\n",
    "    * Optional hard filter applied after the above logic.\n",
    "    * Accepts strings or ints; duplicates are allowed.\n",
    "    * Internally converted to a set of valid int IDs, and intersected with the allowed cases.\n",
    "\n",
    "* **Opinion handling and chunking**\n",
    "\n",
    "  * Opinions are cleaned before storage:\n",
    "\n",
    "    * Control characters removed.\n",
    "    * Hyphenation across line breaks healed.\n",
    "    * Standalone line-number lines dropped.\n",
    "    * Whitespace collapsed to single spaces.\n",
    "\n",
    "  * **Chunked mode (`opinion_storage=\"chunks\"`):**\n",
    "    * The ADAH `opinion` string is compacted and then split into sentence-aware chunks of about `semantic_chunk_size` characters.\n",
    "    * Each chunk becomes an **OpinionChunk** row with fields:\n",
    "      * `id` (string `\"{case_id}:0:{chunk_index}\"`)\n",
    "      * `case_id`\n",
    "      * `chunk_index`\n",
    "      * `opinion_type` (from `opinion_type`)\n",
    "      * `opinion_author` (from `judge` + optional `panel_names`)\n",
    "      * `text` (cleaned chunk text)\n",
    "\n",
    "  * **Inline mode (`opinion_storage=\"inline\"`):**\n",
    "    * The full compacted opinion goes into `cases.csv` under `opinion_property` (default `opinion_text`).\n",
    "    * No opinion chunk files are produced.\n",
    "\n",
    "  * **`max_opinion_chars`**:\n",
    "    * If set and the (inline) opinion string is longer than this limit, it is truncated to that many characters.\n",
    "\n",
    "* **Opinion splits**\n",
    "\n",
    "  * When `opinion_splits > 1` and `opinion_storage=\"chunks\"`:\n",
    "    * Opinion chunks are split into several folders named `Opinion Chunks/split_XX`.\n",
    "    * Splits are **case-aligned** (a case never gets split across multiple files).\n",
    "    * The function tries to balance the number of chunks per split.\n",
    "\n",
    "---\n",
    "\n",
    "## What gets produced\n",
    "\n",
    "Subject to filters and options, the converter writes:\n",
    "\n",
    "* `cases.csv` — ADAH case metadata\n",
    "\n",
    "  * Key columns include:\n",
    "\n",
    "    * `id`\n",
    "    * `name`\n",
    "    * `case_full_name`\n",
    "    * `decision_date`\n",
    "    * `docket_number`\n",
    "    * `court_id`\n",
    "    * `jurisdiction_id`\n",
    "    * `court_name_abbreviation`\n",
    "    * `court_name`\n",
    "    * `jurisdiction_name`\n",
    "    * `citation_pipe`\n",
    "    * `file_name`\n",
    "    * `adah_case` (Boolean-like; `True` for `Citing_Relationship=\"adah\"`)\n",
    "    * optional `court_listener_url`\n",
    "    * optional **inline opinion column** (name = `opinion_property`, e.g. `opinion_text`)\n",
    "\n",
    "* `courts.csv` — court reference table\n",
    "\n",
    "  * Columns:\n",
    "    * `id`\n",
    "    * `name`\n",
    "    * `name_abbreviation`\n",
    "    * `court_level` (1–5; default 5 when unknown)\n",
    "\n",
    "* `jurisdictions.csv` — ADAH jurisdiction table\n",
    "\n",
    "  * Columns:\n",
    "    * `id`\n",
    "    * `jurisdiction_name`\n",
    "\n",
    "* `cites_to.csv` — **resolved** case-to-case citations\n",
    "\n",
    "  * Columns:\n",
    "    * `src_case_id`\n",
    "    * `src_case_name`\n",
    "    * `tgt_case_id`\n",
    "    * `tgt_case_name`\n",
    "    * `src_case_court_listener_url`\n",
    "    * `tgt_case_court_listener_url`\n",
    "\n",
    "* If `opinion_storage=\"chunks\"`:\n",
    "\n",
    "  * `opinion_chunks.csv` — opinion chunk nodes\n",
    "\n",
    "    * Columns:\n",
    "      * `id`\n",
    "      * `case_id`\n",
    "      * `chunk_index`\n",
    "      * `opinion_type`\n",
    "      * `opinion_author`\n",
    "      * `text`\n",
    "\n",
    "  * `case_opinion_edges.csv` — mapping from Case to OpinionChunk\n",
    "\n",
    "    * Columns:\n",
    "      * `case_id`\n",
    "      * `chunk_id`\n",
    "\n",
    "* If `opinion_storage=\"inline\"`:\n",
    "\n",
    "  * No opinion chunk files are produced.\n",
    "  * Full opinion text is stored on the Case rows in `cases.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters (high-level)\n",
    "\n",
    "```python\n",
    "json_to_csv(\n",
    "    json_path: str | None = None,      # local JSON array file\n",
    "    outdir: str | None = None,         # output folder (defaults to \"<json_stem>_csv_out\")\n",
    "    # OR load from S3\n",
    "    s3_bucket: str | None = None,\n",
    "    s3_key: str | None = None,\n",
    "\n",
    "    # opinions\n",
    "    semantic_chunk_size: int = 4000,   # ~target chars per chunk\n",
    "    opinion_splits: int = 1,           # if >1, writes balanced, case-aligned splits\n",
    "    opinion_storage: str = \"chunks\",   # \"chunks\" | \"inline\"\n",
    "    opinion_property: str = \"opinion_text\",\n",
    "    max_opinion_chars: int | None = None,\n",
    "\n",
    "    # ADAH sampling / filtering\n",
    "    subset: int | None = None,         # ADAH seed sample size (adah + its citers)\n",
    "    only_case_ids: Iterable[int|str] | None = None,\n",
    "\n",
    "    # extras\n",
    "    include_url: bool = False          # adds \"court_listener_url\" to cases.csv only\n",
    ")\n",
    "````\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Exactly one of `json_path` or (`s3_bucket` + `s3_key`) must be provided.\n",
    "* The input must be a JSON **array** of ADAH case objects.\n",
    "\n",
    "---\n",
    "\n",
    "## Examples\n",
    "\n",
    "### 1) Default: ADAH + citing_ADAH, chunked opinions\n",
    "\n",
    "```python\n",
    "json_to_csv(\"full_adah_set.json\")\n",
    "```\n",
    "\n",
    "* Keeps all cases with:\n",
    "\n",
    "  * `Citing_Relationship = \"adah\"` or `\"citing_adah\"`.\n",
    "* Writes chunked opinions to `opinion_chunks.csv` and `case_opinion_edges.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Inline full opinions on Case nodes\n",
    "\n",
    "```python\n",
    "json_to_csv(\n",
    "    \"full_adah_set.json\",\n",
    "    opinion_storage=\"inline\",\n",
    "    opinion_property=\"full_opinion_text\"  # column name in cases.csv\n",
    ")\n",
    "```\n",
    "\n",
    "* Puts the full (cleaned) opinion text into `cases.csv.full_opinion_text`.\n",
    "* Does not create opinion chunk CSVs.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Split opinion chunks into three balanced folders\n",
    "\n",
    "```python\n",
    "json_to_csv(\"full_adah_set.json\", opinion_splits=3)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "* Base CSVs in `full_adah_set_csv_out/`:\n",
    "\n",
    "  * `cases.csv`\n",
    "  * `courts.csv`\n",
    "  * `jurisdictions.csv`\n",
    "  * `cites_to.csv`\n",
    "\n",
    "* Opinion chunks split into folders:\n",
    "\n",
    "  * `Opinion Chunks/split_01/opinion_chunks.csv`\n",
    "  * `Opinion Chunks/split_01/case_opinion_edges.csv`\n",
    "  * `Opinion Chunks/split_02/...`\n",
    "  * `Opinion Chunks/split_03/...`\n",
    "\n",
    "Each case appears in **exactly one** split.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) ADAH model-dev subset: ~100 ADAH cases + all citers of those\n",
    "\n",
    "```python\n",
    "json_to_csv(\"full_adah_set.json\", subset=100)\n",
    "```\n",
    "\n",
    "* Step 1: picks up to 100 cases with `Citing_Relationship=\"adah\"`.\n",
    "* Step 2: adds all cases that cite any of those 100 cases.\n",
    "* Applies the same opinion and URL options as usual.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Restrict to a specific set of case IDs\n",
    "\n",
    "```python\n",
    "json_to_csv(\n",
    "    \"full_adah_set.json\",\n",
    "    only_case_ids=[\"2113004\", \"174213\", \"174213\", 2461771]\n",
    ")\n",
    "```\n",
    "\n",
    "* Filters the default ADAH + citing_ADAH set to the provided IDs.\n",
    "* Duplicate values are safe; they are deduplicated internally.\n",
    "\n",
    "---\n",
    "\n",
    "### 6) Include CourtListener URLs in `cases.csv`\n",
    "\n",
    "```python\n",
    "json_to_csv(\"full_adah_set.json\", include_url=True)\n",
    "```\n",
    "\n",
    "* Adds `court_listener_url` to **`cases.csv`**.\n",
    "* `cites_to.csv` always includes URLs regardless of this flag.\n",
    "\n",
    "---\n",
    "\n",
    "### 7) Load from S3 instead of local\n",
    "\n",
    "```python\n",
    "json_to_csv(\n",
    "    s3_bucket=\"my-bucket\",\n",
    "    s3_key=\"exports/full_adah_set.json\",\n",
    "    outdir=\"adah_csv_out\"\n",
    ")\n",
    "```\n",
    "\n",
    "* Uses `boto3` to read the JSON array from S3.\n",
    "* Writes all CSVs into `adah_csv_out/`.\n",
    "\n",
    "---\n",
    "\n",
    "## Importing into Neo4j Aura Data Importer\n",
    "\n",
    "### A) Base CSVs\n",
    "\n",
    "Upload from the output folder:\n",
    "\n",
    "* `cases.csv`\n",
    "* `courts.csv`\n",
    "* `jurisdictions.csv`\n",
    "* `cites_to.csv`\n",
    "\n",
    "**Node mappings**\n",
    "\n",
    "* **Case** ← `cases.csv`\n",
    "\n",
    "  * Key: `id` (Integer, Unique)\n",
    "  * Useful properties:\n",
    "\n",
    "    * `name`\n",
    "    * `case_full_name`\n",
    "    * `decision_date`\n",
    "    * `docket_number`\n",
    "    * `citation_pipe`\n",
    "    * `file_name`\n",
    "    * `adah_case` (Boolean)\n",
    "    * `court_listener_url` (optional)\n",
    "    * `opinion_text` or your custom `opinion_property` (if using inline mode)\n",
    "\n",
    "* **Court** ← `courts.csv`\n",
    "\n",
    "  * Key: `id` (Integer, Unique)\n",
    "  * Properties:\n",
    "\n",
    "    * `name`\n",
    "    * `name_abbreviation`\n",
    "    * `court_level` (Integer)\n",
    "\n",
    "* **Jurisdiction** ← `jurisdictions.csv`\n",
    "\n",
    "  * Key: `id` (Integer, Unique)\n",
    "  * Map `jurisdiction_name` → node property `name`.\n",
    "\n",
    "**Relationship mappings**\n",
    "\n",
    "* `(:Case)-[:HEARD_IN]->(:Court)` from `cases.csv`\n",
    "\n",
    "  * Source: `cases.id` → Case.id\n",
    "  * Target: `cases.court_id` → Court.id\n",
    "\n",
    "* `(:Case)-[:UNDER_JURISDICTION]->(:Jurisdiction)` from `cases.csv`\n",
    "\n",
    "  * Source: `cases.id` → Case.id\n",
    "  * Target: `cases.jurisdiction_id` → Jurisdiction.id\n",
    "\n",
    "* `(:Case)-[:CITES_TO]->(:Case)` from `cites_to.csv`\n",
    "\n",
    "  * Source: `src_case_id` → Case.id\n",
    "  * Target: `tgt_case_id` → Case.id\n",
    "  * You can keep the `src_case_name`, `tgt_case_name`, and URL fields as edge properties for easier debugging and visualization.\n",
    "\n",
    "---\n",
    "\n",
    "### B) Opinions (if `opinion_storage=\"chunks\"`)\n",
    "\n",
    "1. **Nodes:** `opinion_chunks.csv` → **OpinionChunk**\n",
    "\n",
    "   * Key: `id` (String, Unique)\n",
    "   * Properties:\n",
    "\n",
    "     * `case_id` (Integer)\n",
    "     * `chunk_index` (Integer)\n",
    "     * `opinion_type` (String)\n",
    "     * `opinion_author` (String)\n",
    "     * `text` (String)\n",
    "\n",
    "2. **Edges:** `case_opinion_edges.csv` → `(:Case)-[:HAS_OPINION_CHUNK]->(:OpinionChunk)`\n",
    "\n",
    "   * Source: `case_id` → Case.id\n",
    "   * Target: `chunk_id` → OpinionChunk.id\n",
    "\n",
    "If you enabled `opinion_splits > 1`, repeat the node and edge mappings for each `Opinion Chunks/split_XX` folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c132e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# JSON → CSV converter for ADAH cases\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import zlib\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "try:\n",
    "    import boto3  # optional for S3 mode\n",
    "except Exception:\n",
    "    boto3 = None  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714937b-1e50-4cce-92e4-6e2cedf67341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Court level mapping\n",
    "# =========================\n",
    "COURT_LEVEL_MAP: Dict[int, List[str]] = {\n",
    "    1: [\n",
    "        \"Federal Supreme Court\",\n",
    "        \"U.S. Court of International Trade\",\n",
    "        \"U.S. Tax Court\",\n",
    "        \"Merit Systems Protection Board\",\n",
    "        \"Office of Legal Counsel\",\n",
    "        \"U.S. Bankruptcy Court, D of Penn\",\n",
    "        \"Board of Immigration Appeals\",\n",
    "        \"Armed Services Board of Contract Appeals\",\n",
    "        \"U.S. Court of Federal Claims\",\n",
    "    ],\n",
    "    2: [\n",
    "        \"U.S. Court of Appeals for the First Circuit\",\n",
    "        \"U.S. Court of Appeals for the Second Circuit\",\n",
    "        \"U.S. Court of Appeals for the Third Circuit\",\n",
    "        \"U.S. Court of Appeals for the Fourth Circuit\",\n",
    "        \"U.S. Court of Appeals for the Fifth Circuit\",\n",
    "        \"U.S. Court of Appeals for the Sixth Circuit\",\n",
    "        \"U.S. Court of Appeals for the Seventh Circuit\",\n",
    "        \"U.S. Court of Appeals for the Eighth Circuit\",\n",
    "        \"U.S. Court of Appeals for the Ninth Circuit\",\n",
    "        \"U.S. Court of Appeals for the Tenth Circuit\",\n",
    "        \"U.S. Court of Appeals for the Eleventh Circuit\",\n",
    "        \"U.S. Court of Appeals for the D.C. Circuit\",\n",
    "        \"U.S. Court of Appeals for the Federal Circuit\",\n",
    "        \"U.S. Court of Appeals for Veterans Claims\",\n",
    "        \"U.S. Court of Appeals for the Armed Forces\",\n",
    "        \"U.S. Army Court of Criminal Appeals\",\n",
    "    ],\n",
    "    3: [\n",
    "        \"U.S. District Court for the Northern District of California\",\n",
    "        \"U.S. District Court for the Central District of California\",\n",
    "        \"U.S. District Court for the Southern District of California\",\n",
    "        \"U.S. District Court for the Eastern District of California\",\n",
    "        \"U.S. District Court for the District of Massachusetts\",\n",
    "        \"U.S. District Court for the District of Maryland\",\n",
    "        \"U.S. District Court for the Middle District of Louisiana\",\n",
    "        \"U.S. District Court for the District of Colorado\",\n",
    "        \"U.S. District Court for the District of Oregon\",\n",
    "        \"U.S. District Court for the District of Hawaii\",\n",
    "        \"U.S. District Court for the Southern District of New York\",\n",
    "        \"U.S. District Court for the Eastern District of New York\",\n",
    "        \"U.S. District Court for the District of New Jersey\",\n",
    "        \"U.S. District Court for the District of Columbia\",\n",
    "        \"U.S. District Court for the District of the Virgin Islands\",\n",
    "        \"U.S. District Court for the District of Minnesota\",\n",
    "        \"U.S. District Court for the Middle District of Pennsylvania\",\n",
    "    ],\n",
    "    4: [\n",
    "        \"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\n",
    "        \"Hawaii\",\"Idaho\",\"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
    "        \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\"Nevada\",\n",
    "        \"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\n",
    "        \"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
    "        \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\",\"Virgin Islands\",\"Puerto Rico\",\n",
    "        \"Guam\",\"Northern Mariana Islands\",\n",
    "    ],\n",
    "    5: [\"Unknown\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa410f9-de41-4655-8414-379bf9425a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lowercase lookup: jurisdiction_name -> level\n",
    "_JURIS_TO_LEVEL: Dict[str, int] = {}\n",
    "for lvl, names in COURT_LEVEL_MAP.items():\n",
    "    for nm in names:\n",
    "        key = (nm or \"\").strip().lower()\n",
    "        if key:\n",
    "            # keep the smallest level if duplicates appear\n",
    "            _JURIS_TO_LEVEL[key] = min(lvl, _JURIS_TO_LEVEL.get(key, lvl))\n",
    "\n",
    "\n",
    "def court_level_from_jurisdiction(name: Optional[str]) -> int:\n",
    "    if not name:\n",
    "        return 5\n",
    "    return _JURIS_TO_LEVEL.get(name.strip().lower(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6158e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "\n",
    "def clean_text_basic(s: str) -> str:\n",
    "    \"\"\"Remove control chars, collapse whitespace; safe for CSV/DB.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    TRANSLATE = {\n",
    "        0x00A0: 0x20, 0x0085: 0x20, 0x2028: 0x20, 0x2029: 0x20,\n",
    "        0x200E: 0x20, 0x200F: 0x20, 0x202A: 0x20, 0x202B: 0x20,\n",
    "        0x202C: 0x20, 0x202D: 0x20, 0x202E: 0x20, 0x2066: 0x20,\n",
    "        0x2067: 0x20, 0x2068: 0x20, 0x2069: 0x20, 0x0009: 0x20,\n",
    "        0x000A: 0x20, 0x000D: 0x20,\n",
    "    }\n",
    "    s = s.translate(TRANSLATE)\n",
    "    s = re.sub(r\"[\\x00-\\x1F\\x7F-\\x9F]\", \" \", s)\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def wcsv(path: Path, rows: Iterable[Dict[str, Any]], header: List[str]):\n",
    "    \"\"\"Write CSV with UTF-8 BOM and CRLF; create parent dirs.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    rows = list(rows)\n",
    "    if not rows:\n",
    "        return\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header, quoting=csv.QUOTE_ALL, lineterminator=\"\\r\\n\")\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k, \"\") for k in header})\n",
    "\n",
    "\n",
    "def infer_header(rows: List[Dict[str, Any]], preferred_order: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"Create a stable header from the union of keys across all rows.\"\"\"\n",
    "    keys: List[str] = []\n",
    "    seen = set()\n",
    "    if preferred_order:\n",
    "        for k in preferred_order:\n",
    "            if any(k in r for r in rows):\n",
    "                keys.append(k)\n",
    "                seen.add(k)\n",
    "    for r in rows:\n",
    "        for k in r.keys():\n",
    "            if k not in seen:\n",
    "                keys.append(k)\n",
    "                seen.add(k)\n",
    "    return keys\n",
    "\n",
    "\n",
    "def join_or_blank(items, sep=\" | \"):\n",
    "    if not items:\n",
    "        return \"\"\n",
    "    return sep.join(str(x) for x in items if x)\n",
    "\n",
    "\n",
    "def _stable_id(s: str) -> int:\n",
    "    \"\"\"Deterministic positive 31-bit int from a string (for synthetic IDs).\"\"\"\n",
    "    return zlib.crc32(s.encode(\"utf-8\")) & 0x7FFFFFFF\n",
    "\n",
    "\n",
    "def _tail_slug(absolute_url: Optional[str]) -> str:\n",
    "    if not absolute_url:\n",
    "        return \"\"\n",
    "    parts = absolute_url.strip(\"/\").split(\"/\")\n",
    "    return parts[-1] if parts else \"\"\n",
    "\n",
    "\n",
    "def _norm_rel(val: Any) -> str:\n",
    "    \"\"\"Normalize Citing_Relationship values to lowercase strings.\"\"\"\n",
    "    return (str(val).strip().lower() if val is not None else \"\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Text compaction (opinion)\n",
    "# =========================\n",
    "\n",
    "def compact_snippet(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize opinion text/snippet into a single readable paragraph:\n",
    "      - remove form feeds\n",
    "      - heal hyphenations across line breaks\n",
    "      - drop standalone line-number lines\n",
    "      - collapse whitespace to single spaces\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    s = text.replace(\"\\r\", \"\\n\").replace(\"\\x0c\", \" \")\n",
    "    s = re.sub(r'(\\w)-\\s*\\n\\s*(\\w)', r'\\1\\2', s)  # heal hyphenation\n",
    "    lines = []\n",
    "    for ln in s.splitlines():\n",
    "        if re.match(r'^\\s*\\d{1,3}\\s*$', ln):\n",
    "            continue\n",
    "        lines.append(ln)\n",
    "    s = \"\\n\".join(lines)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = re.sub(r'\\s+([,.;:!?])', r'\\1', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def _compact_for_csv(s: str) -> str:\n",
    "    \"\"\"Compact first (line numbers, hyphenation), then scrub control chars.\"\"\"\n",
    "    return clean_text_basic(compact_snippet(s))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Sentence-aware chunking\n",
    "# =========================\n",
    "\n",
    "_CLOSERS_AFTER_SPACE = set(['\"', \"'\", \")\", \"]\", \"”\", \"’\", \"»\"])\n",
    "_TERMINATORS = {'.', '!', '?'}\n",
    "\n",
    "\n",
    "def _precompute_paren_depths(text: str) -> List[int]:\n",
    "    depths = [0] * len(text)\n",
    "    depth = 0\n",
    "    for i, ch in enumerate(text):\n",
    "        depths[i] = depth\n",
    "        if ch == '(':\n",
    "            depth += 1\n",
    "        elif ch == ')' and depth > 0:\n",
    "            depth -= 1\n",
    "    return depths\n",
    "\n",
    "\n",
    "def _is_ellipsis_dot(text: str, idx: int) -> bool:\n",
    "    if idx < 0 or idx >= len(text) or text[idx] != '.':\n",
    "        return False\n",
    "\n",
    "    def count_forward(i: int) -> int:\n",
    "        cnt = 0\n",
    "        j = i\n",
    "        if j < len(text) and text[j] == '.':\n",
    "            cnt += 1\n",
    "            j += 1\n",
    "        while j < len(text):\n",
    "            if text[j].isspace():\n",
    "                j += 1\n",
    "                continue\n",
    "            if text[j] == '.':\n",
    "                cnt += 1\n",
    "                j += 1\n",
    "                continue\n",
    "            break\n",
    "        return cnt\n",
    "\n",
    "    def count_backward(i: int) -> int:\n",
    "        cnt = 0\n",
    "        j = i\n",
    "        if j >= 0 and text[j] == '.':\n",
    "            cnt += 1\n",
    "            j -= 1\n",
    "        while j >= 0:\n",
    "            if text[j].isspace():\n",
    "                j -= 1\n",
    "                continue\n",
    "            if text[j] == '.':\n",
    "                cnt += 1\n",
    "                j -= 1\n",
    "                continue\n",
    "            break\n",
    "        return cnt\n",
    "\n",
    "    total = 1\n",
    "    total += count_forward(idx + 1)\n",
    "    total += count_backward(idx - 1)\n",
    "    return total >= 3\n",
    "\n",
    "\n",
    "def _is_good_boundary_next_char_rule(text: str, abs_pos: int) -> bool:\n",
    "    ch = text[abs_pos]\n",
    "    if ch == '.' and _is_ellipsis_dot(text, abs_pos):\n",
    "        return False\n",
    "    k = abs_pos + 1\n",
    "    n = len(text)\n",
    "    while k < n and text[k].isspace():\n",
    "        k += 1\n",
    "    if k >= n:\n",
    "        return True\n",
    "    nxt = text[k]\n",
    "    if nxt in _CLOSERS_AFTER_SPACE:\n",
    "        return False\n",
    "    if nxt.isalpha():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def sentence_chunks(text: str, target_size: int) -> List[str]:\n",
    "    if not text:\n",
    "        return [\"\"]\n",
    "\n",
    "    n = len(text)\n",
    "    out: List[str] = []\n",
    "    start = 0\n",
    "    min_len = max(1, int(target_size * 0.6))\n",
    "    paren_depths = _precompute_paren_depths(text)\n",
    "\n",
    "    while start < n:\n",
    "        max_end = min(start + target_size, n)\n",
    "        window = text[start:max_end]\n",
    "        candidates = [i for i, ch in enumerate(window) if ch in _TERMINATORS]\n",
    "        chosen_end = None\n",
    "\n",
    "        for i in reversed(candidates):\n",
    "            abs_i = start + i\n",
    "            if paren_depths[abs_i] > 0:\n",
    "                continue\n",
    "            if window[i] == '.' and _is_ellipsis_dot(text, abs_i):\n",
    "                continue\n",
    "            if i + 1 >= min_len and _is_good_boundary_next_char_rule(text, abs_i):\n",
    "                chosen_end = abs_i + 1\n",
    "                break\n",
    "\n",
    "        if chosen_end is None:\n",
    "            ws = window.rfind(\" \")\n",
    "            if ws != -1 and ws + 1 >= min_len:\n",
    "                chosen_end = start + ws\n",
    "            else:\n",
    "                chosen_end = max_end\n",
    "\n",
    "        piece = text[start:chosen_end].strip()\n",
    "        if piece:\n",
    "            out.append(piece)\n",
    "        start = chosen_end\n",
    "\n",
    "    return out if out else [\"\"]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilities for splitting\n",
    "# =========================\n",
    "\n",
    "def _build_case_splits(\n",
    "    case_id_order: List[int],\n",
    "    case_chunk_counts: Dict[int, int],\n",
    "    total_chunks: int,\n",
    "    num_splits: int,\n",
    ") -> Dict[int, int]:\n",
    "    \"\"\"\n",
    "    Return case_id -> split_index (0..num_splits-1), approximating equal\n",
    "    opinion_chunk rows per split while never splitting a case across files.\n",
    "    \"\"\"\n",
    "    if num_splits <= 1:\n",
    "        return {cid: 0 for cid in case_id_order}\n",
    "    target = int(math.ceil(total_chunks / num_splits))\n",
    "    mapping: Dict[int, int] = {}\n",
    "    split_idx = 0\n",
    "    running = 0\n",
    "    for cid in case_id_order:\n",
    "        cnt = case_chunk_counts.get(cid, 0)\n",
    "        if split_idx < num_splits - 1 and running > 0 and running + cnt > target:\n",
    "            split_idx += 1\n",
    "            running = 0\n",
    "        mapping[cid] = split_idx\n",
    "        running += cnt\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Type check\n",
    "# =========================\n",
    "\n",
    "def _is_adah_case(obj: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Heuristic: ADAH cases have 'ID' (int) and 'opinion' (string) keys.\"\"\"\n",
    "    return (\"ID\" in obj) and (\"opinion\" in obj)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# S3 / Local loader\n",
    "# =========================\n",
    "\n",
    "def _load_json_from_local_or_s3(\n",
    "    json_path: Optional[str],\n",
    "    s3_bucket: Optional[str],\n",
    "    s3_key: Optional[str],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load a JSON array either from local path or from S3 (via boto3).\n",
    "    Exactly one of (json_path) or (s3_bucket+s3_key) must be provided.\n",
    "    \"\"\"\n",
    "    if json_path and (s3_bucket or s3_key):\n",
    "        raise ValueError(\"Provide either json_path OR (s3_bucket and s3_key), not both.\")\n",
    "    if not json_path and not (s3_bucket and s3_key):\n",
    "        raise ValueError(\"You must provide json_path OR (s3_bucket and s3_key).\")\n",
    "\n",
    "    if json_path:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    if boto3 is None:\n",
    "        raise RuntimeError(\"boto3 is required for S3 mode\")\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    obj = s3.get_object(Bucket=s3_bucket, Key=s3_key)  # type: ignore[arg-type]\n",
    "    with io.TextIOWrapper(obj[\"Body\"], encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _default_outdir_name(json_path: Optional[str], s3_key: Optional[str]) -> str:\n",
    "    \"\"\"Derive a default output directory name from local json filename or S3 key.\"\"\"\n",
    "    if json_path:\n",
    "        stem = Path(json_path).stem\n",
    "        return f\"{stem}_csv_out\"\n",
    "    if s3_key:\n",
    "        stem = Path(s3_key).stem\n",
    "        return f\"{stem}_csv_out\"\n",
    "    return \"csv_out\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Small helpers for IDs & edges (ADAH-only)\n",
    "# =========================\n",
    "\n",
    "def _get_case_id(obj: Dict[str, Any]) -> Optional[int]:\n",
    "    \"\"\"Return canonical int case id from an ADAH object, else None.\"\"\"\n",
    "    try:\n",
    "        return int(obj[\"ID\"])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _get_cited_ids(obj: Dict[str, Any]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Return list of resolved target ids that this ADAH object cites.\n",
    "    Uses top-level 'cite_to' (list of ints/strings).\n",
    "    \"\"\"\n",
    "    out: List[int] = []\n",
    "    for tid in (obj.get(\"cite_to\") or []):\n",
    "        try:\n",
    "            out.append(int(tid))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "\n",
    "def _compute_case_meta_for_allowed(\n",
    "    data: List[Dict[str, Any]],\n",
    "    allowed_case_ids: set[int],\n",
    ") -> Dict[int, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Build id → {name, file_slug, url} for all allowed ADAH cases, so cites_to rows can\n",
    "    include names and CourtListener URLs regardless of processing order.\n",
    "    \"\"\"\n",
    "    meta: Dict[int, Dict[str, str]] = {}\n",
    "    for c in data:\n",
    "        if not _is_adah_case(c):\n",
    "            continue\n",
    "        cid = _get_case_id(c)\n",
    "        if cid is None or cid not in allowed_case_ids:\n",
    "            continue\n",
    "        name = c.get(\"case_name\") or \"\"\n",
    "        file_slug = clean_text_basic(_tail_slug(c.get(\"absolute_url\")))\n",
    "        url = f\"https://www.courtlistener.com/opinion/{cid}/{file_slug}/\" if file_slug else \"\"\n",
    "        meta[cid] = {\n",
    "            \"name\": clean_text_basic(name),\n",
    "            \"file_slug\": file_slug,\n",
    "            \"url\": url,\n",
    "        }\n",
    "    return meta\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main conversion\n",
    "# =========================\n",
    "\n",
    "def json_to_csv(\n",
    "    json_path: Optional[str] = None,\n",
    "    outdir: Optional[str] = None,\n",
    "    *,\n",
    "    # ALSO SUPPORT S3\n",
    "    s3_bucket: Optional[str] = None,\n",
    "    s3_key: Optional[str] = None,\n",
    "    # opinion handling\n",
    "    semantic_chunk_size: int = 4000,\n",
    "    opinion_splits: int = 1,          # used only when opinion_storage=\"chunks\"\n",
    "    opinion_storage: str = \"chunks\",  # \"chunks\" | \"inline\"\n",
    "    opinion_property: str = \"opinion_text\",\n",
    "    max_opinion_chars: Optional[int] = None,\n",
    "    # ADAH model-dev subsetting:\n",
    "    # keep up to `subset` ADAH cases (Citing_Relationship='adah')\n",
    "    # plus all cases that cite these ADAH cases\n",
    "    subset: Optional[int] = None,\n",
    "    # Optional hard filter to keep only these case IDs (strings or ints; deduped)\n",
    "    only_case_ids: Optional[Iterable[Union[str, int]]] = None,\n",
    "    # Optional: include URLs in cases.csv (cites_to.csv always includes URLs)\n",
    "    include_url: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert ADAH JSON cases into CSVs for Neo4j Aura Data Importer.\n",
    "\n",
    "    Behavior:\n",
    "      - Always keeps ADAH cases (Citing_Relationship='adah') and cases that cite ADAH\n",
    "        (Citing_Relationship='citing_adah').\n",
    "      - If `subset` is given, first selects up to `subset` ADAH cases, then adds all\n",
    "        cases that cite any of those ADAH cases.\n",
    "      - cites_to.csv outputs: src_case_id, src_case_name, tgt_case_id, tgt_case_name,\n",
    "        src_case_court_listener_url, tgt_case_court_listener_url.\n",
    "      - courts.csv includes `court_level` derived from jurisdiction.\n",
    "      - include_url toggles URLs in cases.csv only (cites_to.csv always includes URLs).\n",
    "\n",
    "    This converter only supports ADAH-style JSON objects (with keys 'ID' and 'opinion').\n",
    "    \"\"\"\n",
    "    # ---------- load payload ----------\n",
    "    data = _load_json_from_local_or_s3(json_path, s3_bucket, s3_key)\n",
    "\n",
    "    # Check that this looks like ADAH data\n",
    "    if not any(_is_adah_case(obj) for obj in data):\n",
    "        raise ValueError(\n",
    "            \"This converter only supports ADAH-format cases (objects with 'ID' and 'opinion'). \"\n",
    "            \"The provided JSON does not appear to contain ADAH cases.\"\n",
    "        )\n",
    "\n",
    "    # Normalize only_case_ids to a set of ints, if provided\n",
    "    only_ids_set: Optional[set[int]] = None\n",
    "    if only_case_ids is not None:\n",
    "        only_ids_set = set()\n",
    "        for v in only_case_ids:\n",
    "            try:\n",
    "                only_ids_set.add(int(v))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # ---------- setup ----------\n",
    "    if outdir is None:\n",
    "        outdir = _default_outdir_name(json_path, s3_key)\n",
    "    OUTDIR = Path(outdir)\n",
    "    OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Build allowed_case_ids ----\n",
    "    # Always using Citing_Relationship values \"adah\" and \"citing_adah\"\n",
    "    allowed_case_ids: set[int] = set()\n",
    "    REL_ADAH = \"adah\"\n",
    "    REL_CITING_ADAH = \"citing_adah\"\n",
    "\n",
    "    if subset:\n",
    "        # Step 1: pick up to `subset` ADAH cases\n",
    "        selected_adah_ids: List[int] = []\n",
    "        for obj in data:\n",
    "            if not _is_adah_case(obj):\n",
    "                continue\n",
    "            if _norm_rel(obj.get(\"Citing_Relationship\")) == REL_ADAH:\n",
    "                cid = _get_case_id(obj)\n",
    "                if cid is None:\n",
    "                    continue\n",
    "                selected_adah_ids.append(cid)\n",
    "                if len(selected_adah_ids) >= subset:\n",
    "                    break\n",
    "\n",
    "        selected_adah_ids_set = set(selected_adah_ids)\n",
    "        allowed_case_ids.update(selected_adah_ids_set)\n",
    "\n",
    "        # Step 2: include all cases that cite any selected ADAH case\n",
    "        for obj in data:\n",
    "            if not _is_adah_case(obj):\n",
    "                continue\n",
    "            cid = _get_case_id(obj)\n",
    "            if cid is None:\n",
    "                continue\n",
    "            cited = _get_cited_ids(obj)\n",
    "            if any(t in selected_adah_ids_set for t in cited):\n",
    "                allowed_case_ids.add(cid)\n",
    "\n",
    "    else:\n",
    "        # Default: keep all ADAH and citing-ADAH cases\n",
    "        for obj in data:\n",
    "            if not _is_adah_case(obj):\n",
    "                continue\n",
    "            rel = _norm_rel(obj.get(\"Citing_Relationship\"))\n",
    "            if rel in (REL_ADAH, REL_CITING_ADAH):\n",
    "                cid = _get_case_id(obj)\n",
    "                if cid is not None:\n",
    "                    allowed_case_ids.add(cid)\n",
    "\n",
    "    if only_ids_set is not None:\n",
    "        allowed_case_ids &= only_ids_set\n",
    "\n",
    "    # ---- Precompute meta (names, URLs) for all allowed cases ----\n",
    "    case_meta = _compute_case_meta_for_allowed(data, allowed_case_ids)\n",
    "\n",
    "    # ---- Root-level tables ----\n",
    "    cases_rows: List[Dict[str, Any]] = []\n",
    "    courts: Dict[Any, Dict[str, Any]] = {}\n",
    "    jurisdictions: Dict[Any, Dict[str, Any]] = {}\n",
    "\n",
    "    cites_to: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Opinion (chunks mode)\n",
    "    opinion_chunks: List[Dict[str, Any]] = []\n",
    "    case_opinion_edges: List[Dict[str, Any]] = []\n",
    "\n",
    "    case_id_order_with_opinions: List[int] = []\n",
    "    per_case_chunk_count: Dict[int, int] = defaultdict(int)\n",
    "\n",
    "    # Synthetic jurisdiction id mapping for ADAH (by name)\n",
    "    adah_juris_name_to_id: Dict[str, int] = {}\n",
    "\n",
    "    def chunk_id(case_id, op_idx, ch_idx):\n",
    "        return f\"{case_id}:{op_idx}:{ch_idx}\"\n",
    "\n",
    "    def maybe_cap(s: str) -> str:\n",
    "        if s is None:\n",
    "            return \"\"\n",
    "        if max_opinion_chars is not None and len(s) > max_opinion_chars:\n",
    "            return s[:max_opinion_chars]\n",
    "        return s\n",
    "\n",
    "    # Helper to upsert courts with court_level (min level wins if repeated)\n",
    "    def upsert_court(court_key, name, abbrev, level: int):\n",
    "        cur = courts.get(court_key)\n",
    "        if not cur:\n",
    "            courts[court_key] = {\n",
    "                \"id\": court_key,\n",
    "                \"name\": name,\n",
    "                \"name_abbreviation\": abbrev,\n",
    "                \"court_level\": int(level) if level else 5,\n",
    "            }\n",
    "        else:\n",
    "            cur[\"name\"] = cur.get(\"name\") or name\n",
    "            cur[\"name_abbreviation\"] = cur.get(\"name_abbreviation\") or abbrev\n",
    "            prev = cur.get(\"court_level\")\n",
    "            if prev is None:\n",
    "                cur[\"court_level\"] = int(level) if level else 5\n",
    "            else:\n",
    "                cur[\"court_level\"] = min(int(prev), int(level) if level else 5)\n",
    "\n",
    "    # ---- Transform loop (ADAH-only) ----\n",
    "    for c in data:\n",
    "        if not _is_adah_case(c):\n",
    "            continue\n",
    "        cid = _get_case_id(c)\n",
    "        if cid is None or cid not in allowed_case_ids:\n",
    "            continue\n",
    "\n",
    "        rel = _norm_rel(c.get(\"Citing_Relationship\"))\n",
    "\n",
    "        # Base ADAH metadata\n",
    "        name = c.get(\"case_name\") or \"\"\n",
    "        decision_date = c.get(\"date_filed\")\n",
    "        docket_number = c.get(\"docket_number\") or c.get(\"docker_number\")\n",
    "\n",
    "        # Court & jurisdiction (ADAH)\n",
    "        court_name = (c.get(\"court\") or \"\").strip()\n",
    "        court_abbrev = (c.get(\"court_citation_string\") or \"\").strip() or \"\"\n",
    "        court_id = _stable_id(f\"adah-court:{court_name}|{court_abbrev}\")\n",
    "\n",
    "        juris_name = c.get(\"jurisdiction_inferred\") or \"\"\n",
    "        jurisdiction_id = None\n",
    "        if juris_name:\n",
    "            if juris_name not in adah_juris_name_to_id:\n",
    "                adah_juris_name_to_id[juris_name] = _stable_id(f\"adah-juris:{juris_name}\")\n",
    "            jurisdiction_id = adah_juris_name_to_id[juris_name]\n",
    "            jurisdictions[jurisdiction_id] = {\n",
    "                \"id\": jurisdiction_id,\n",
    "                \"name\": juris_name,\n",
    "                \"name_long\": \"\",\n",
    "            }\n",
    "\n",
    "        # Court level from jurisdiction (ADAH)\n",
    "        lvl = court_level_from_jurisdiction(juris_name)\n",
    "        upsert_court(court_id, court_name, court_abbrev, lvl)\n",
    "\n",
    "        # case_full_name (tolerate literal \"NULL\")\n",
    "        cf_raw = c.get(\"case_full_name\")\n",
    "        case_full_name = (\n",
    "            \"N/A\"\n",
    "            if (isinstance(cf_raw, str) and cf_raw.strip().upper() == \"NULL\")\n",
    "            else (clean_text_basic(cf_raw) if cf_raw else \"\")\n",
    "        )\n",
    "\n",
    "        # citation pipe (single or list)\n",
    "        cit_val = c.get(\"citation\")\n",
    "        if isinstance(cit_val, list):\n",
    "            citation_pipe = join_or_blank([str(x).strip() for x in cit_val if x], sep=\" | \")\n",
    "        else:\n",
    "            citation_pipe = str(cit_val).strip() if cit_val else \"\"\n",
    "\n",
    "        # file slug + optional CourtListener URL for cases.csv\n",
    "        file_name_val = case_meta.get(cid, {}).get(\"file_slug\", \"\")\n",
    "        cl_url = case_meta.get(cid, {}).get(\"url\", \"\")\n",
    "\n",
    "        # Base case row (ADAH)\n",
    "        base = {\n",
    "            \"id\": cid,\n",
    "            \"name\": name,\n",
    "            \"case_full_name\": case_full_name,\n",
    "            \"decision_date\": decision_date,\n",
    "            \"docket_number\": docket_number,\n",
    "            \"court_id\": court_id,\n",
    "            \"jurisdiction_id\": jurisdiction_id,\n",
    "            \"court_name_abbreviation\": court_abbrev,\n",
    "            \"court_name\": court_name,\n",
    "            \"jurisdiction_name\": juris_name,\n",
    "            \"citation_pipe\": citation_pipe,\n",
    "            \"file_name\": file_name_val,\n",
    "            \"adah_case\": (rel == \"adah\"),\n",
    "        }\n",
    "        if include_url:\n",
    "            base[\"court_listener_url\"] = cl_url\n",
    "        if opinion_storage == \"inline\":\n",
    "            base[opinion_property] = \"\"\n",
    "\n",
    "        # Opinions (ADAH is one long string)\n",
    "        otype = (c.get(\"opinion_type\") or \"\") or \"\"\n",
    "        author = (c.get(\"judge\") or \"\") or \"\"\n",
    "        panel = c.get(\"panel_names\") or []\n",
    "        if author and panel:\n",
    "            author = f\"{author} (panel: {', '.join(panel)})\"\n",
    "        otext = (c.get(\"opinion\") or \"\") or \"\"\n",
    "\n",
    "        if opinion_storage == \"inline\":\n",
    "            full_txt = _compact_for_csv(otext)\n",
    "            base[opinion_property] = maybe_cap(full_txt)\n",
    "        else:\n",
    "            if otext.strip():\n",
    "                otext_comp = _compact_for_csv(otext)\n",
    "                sem_chunks = sentence_chunks(otext_comp, semantic_chunk_size)\n",
    "                if cid not in per_case_chunk_count:\n",
    "                    case_id_order_with_opinions.append(cid)\n",
    "                for ch_idx, ch_text in enumerate(sem_chunks):\n",
    "                    oid = chunk_id(cid, 0, ch_idx)\n",
    "                    opinion_chunks.append(\n",
    "                        {\n",
    "                            \"id\": oid,\n",
    "                            \"case_id\": cid,\n",
    "                            \"chunk_index\": ch_idx,\n",
    "                            \"opinion_type\": otype,\n",
    "                            \"opinion_author\": author,\n",
    "                            \"text\": ch_text,\n",
    "                        }\n",
    "                    )\n",
    "                    per_case_chunk_count[cid] += 1\n",
    "                    case_opinion_edges.append({\"case_id\": cid, \"chunk_id\": oid})\n",
    "\n",
    "        cases_rows.append(base)\n",
    "\n",
    "        # cites_to (ADAH → numeric IDs)\n",
    "        for tid_raw in (c.get(\"cite_to\") or []):\n",
    "            try:\n",
    "                tid = int(tid_raw)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if tid in allowed_case_ids:\n",
    "                cites_to.append(\n",
    "                    {\n",
    "                        \"src_case_id\": cid,\n",
    "                        \"src_case_name\": case_meta.get(cid, {}).get(\"name\", \"\"),\n",
    "                        \"tgt_case_id\": tid,\n",
    "                        \"tgt_case_name\": case_meta.get(tid, {}).get(\"name\", \"\"),\n",
    "                        \"src_case_court_listener_url\": case_meta.get(cid, {}).get(\"url\", \"\"),\n",
    "                        \"tgt_case_court_listener_url\": case_meta.get(tid, {}).get(\"url\", \"\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # ---------- write CSVs ----------\n",
    "    if cases_rows:\n",
    "        preferred = [\n",
    "            \"id\",\n",
    "            \"name\",\n",
    "            \"case_full_name\",\n",
    "            \"decision_date\",\n",
    "            \"docket_number\",\n",
    "            \"court_id\",\n",
    "            \"jurisdiction_id\",\n",
    "            \"court_name_abbreviation\",\n",
    "            \"court_name\",\n",
    "            \"jurisdiction_name\",\n",
    "            \"citation_pipe\",\n",
    "            \"file_name\",\n",
    "            \"court_listener_url\",\n",
    "            \"adah_case\",\n",
    "        ]\n",
    "        if any(opinion_property in r for r in cases_rows):\n",
    "            preferred.append(opinion_property)\n",
    "        header = infer_header(cases_rows, preferred_order=preferred)\n",
    "        wcsv(Path(OUTDIR / \"cases.csv\"), cases_rows, header)\n",
    "\n",
    "    if courts:\n",
    "        # ensure every court row has court_level (default 5 if somehow missing)\n",
    "        for v in courts.values():\n",
    "            if \"court_level\" not in v or v[\"court_level\"] in (None, \"\"):\n",
    "                v[\"court_level\"] = 5\n",
    "        wcsv(\n",
    "            Path(OUTDIR / \"courts.csv\"),\n",
    "            courts.values(),\n",
    "            [\"id\", \"name\", \"name_abbreviation\", \"court_level\"],\n",
    "        )\n",
    "\n",
    "    # jurisdictions.csv (two passes; final file is ADAH minimal rows)\n",
    "    if jurisdictions:\n",
    "        adah_ids = set(adah_juris_name_to_id.values()) if adah_juris_name_to_id else set()\n",
    "        cap_jur_rows = [r for r in jurisdictions.values() if r.get(\"id\") not in adah_ids]\n",
    "        if cap_jur_rows:\n",
    "            # no-op in pure ADAH case, but kept for compatibility\n",
    "            wcsv(Path(OUTDIR / \"jurisdictions.csv\"), cap_jur_rows, [\"id\", \"name\", \"name_long\"])\n",
    "\n",
    "    if adah_juris_name_to_id:\n",
    "        minimal_rows = [\n",
    "            {\"id\": jid, \"jurisdiction_name\": name}\n",
    "            for name, jid in sorted(adah_juris_name_to_id.items(), key=lambda x: x[0].lower())\n",
    "        ]\n",
    "        wcsv(Path(OUTDIR / \"jurisdictions.csv\"), minimal_rows, [\"id\", \"jurisdiction_name\"])\n",
    "\n",
    "    if cites_to:\n",
    "        wcsv(\n",
    "            Path(OUTDIR / \"cites_to.csv\"),\n",
    "            cites_to,\n",
    "            [\n",
    "                \"src_case_id\",\n",
    "                \"src_case_name\",\n",
    "                \"tgt_case_id\",\n",
    "                \"tgt_case_name\",\n",
    "                \"src_case_court_listener_url\",\n",
    "                \"tgt_case_court_listener_url\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    # ---- Opinion output ----\n",
    "    if opinion_storage == \"inline\":\n",
    "        print(\"Done (inline opinions). Files in:\", OUTDIR.resolve())\n",
    "        return\n",
    "\n",
    "    if not opinion_chunks:\n",
    "        print(\"Done. Files in:\", OUTDIR.resolve())\n",
    "        return\n",
    "\n",
    "    # Single-file opinion chunks\n",
    "    if opinion_splits <= 1:\n",
    "        oc_pref = [\n",
    "            \"id\",\n",
    "            \"case_id\",\n",
    "            \"chunk_index\",\n",
    "            \"opinion_type\",\n",
    "            \"opinion_author\",\n",
    "            \"text\",\n",
    "        ]\n",
    "        oc_header = infer_header(opinion_chunks, preferred_order=oc_pref)\n",
    "        wcsv(Path(OUTDIR / \"opinion_chunks.csv\"), opinion_chunks, oc_header)\n",
    "\n",
    "        wcsv(Path(OUTDIR / \"case_opinion_edges.csv\"), case_opinion_edges, [\"case_id\", \"chunk_id\"])\n",
    "        print(\"Done. Files in:\", OUTDIR.resolve())\n",
    "        return\n",
    "\n",
    "    # Multi-file split by case boundaries, balancing chunk counts\n",
    "    total_chunks = len(opinion_chunks)\n",
    "    case_to_split = _build_case_splits(\n",
    "        case_id_order_with_opinions, per_case_chunk_count, total_chunks, opinion_splits\n",
    "    )\n",
    "\n",
    "    base = OUTDIR / \"Opinion Chunks\"\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for k in range(opinion_splits):\n",
    "        split_dir = base / f\"split_{k+1:02d}\"\n",
    "        split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        cases_in_split = {cid for cid, sidx in case_to_split.items() if sidx == k}\n",
    "\n",
    "        oc_rows = [r for r in opinion_chunks if r[\"case_id\"] in cases_in_split]\n",
    "        coe_rows = [r for r in case_opinion_edges if r[\"case_id\"] in cases_in_split]\n",
    "\n",
    "        if oc_rows:\n",
    "            oc_pref = [\n",
    "                \"id\",\n",
    "                \"case_id\",\n",
    "                \"chunk_index\",\n",
    "                \"opinion_type\",\n",
    "                \"opinion_author\",\n",
    "                \"text\",\n",
    "            ]\n",
    "            oc_header = infer_header(oc_rows, preferred_order=oc_pref)\n",
    "            wcsv(split_dir / \"opinion_chunks.csv\", oc_rows, oc_header)\n",
    "        if coe_rows:\n",
    "            wcsv(split_dir / \"case_opinion_edges.csv\", coe_rows, [\"case_id\", \"chunk_id\"])\n",
    "\n",
    "    print(\"Done. Files in:\", OUTDIR.resolve())\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Example calls\n",
    "# ---------------------------\n",
    "\n",
    "# 1) Default “production” filter: keep ADAH + citing_ADAH, chunk opinions into single CSVs\n",
    "# json_to_csv(\"adah_cases_full.json\")\n",
    "\n",
    "# 2) Inline full opinions\n",
    "# json_to_csv(\"adah_cases_full.json\", opinion_storage=\"inline\", opinion_property=\"full_opinion_text\")\n",
    "\n",
    "# 3) Chunked opinions, split across 3 folders\n",
    "# json_to_csv(\"adah_cases_full.json\", opinion_splits=3)\n",
    "\n",
    "# 4) ADAH model-dev subset: keep ~100 ADAH cases + ALL cases that cite them\n",
    "# json_to_csv(\"adah_cases_full.json\", subset=100)\n",
    "\n",
    "# 5) Restrict to a specific set of case IDs (strings or ints; duplicates OK)\n",
    "# json_to_csv(\"adah_cases_full.json\", only_case_ids=[\"2113004\", \"174213\", \"174213\", 2461771])\n",
    "\n",
    "# 6) Include CourtListener URLs in cases.csv (cites_to.csv always includes URLs)\n",
    "# json_to_csv(\"adah_cases_full.json\", include_url=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e52e4",
   "metadata": {},
   "source": [
    "## Example of How to Extract a Subset (15 ADAH Cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dda2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Files in: C:\\Users\\kentb\\OneDrive\\Desktop\\Berkeley MIDS\\DATASCI 210\\Post-ADA Dataset Work\\adah_cases_subset_eval\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# json_to_csv(json_path = \"adah_cases_full.json\", outdir = \"adah_cases_subset_eval\", semantic_chunk_size=4000, subset = 15, include_url=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
