{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28458af0-7dba-412f-b8cf-9dd8d76c8435",
   "metadata": {},
   "source": [
    "# Edge Classifier Snippet Method – Claude 3.5 Sonnet\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "* Classify each `(:Case)-[:CITES_TO]->(:Case)` edge as **Positive / Neutral / Negative / Unknown** using:\n",
    "  * **Pre-extracted snippets** on the relation (`snippet_1 … snippet_N`)\n",
    "  * Short **case summaries** for citing and cited cases\n",
    "* Write results back to the relation and (optionally) produce a CSV for analysis and QA.\n",
    "\n",
    "---\n",
    "\n",
    "## What this notebook uses\n",
    "\n",
    "* **Snippets from the relation**  \n",
    "  * `snippet_1, snippet_2, …` already created by the Snippet Retriever.\n",
    "* **Case metadata** (for both source = citing, target = cited):\n",
    "  * `name`, `decision_date`, `citation_pipe`, optional `opinion_summary`\n",
    "* **Short citation**  \n",
    "  * First element from target’s `citation_pipe` (used to anchor the model).\n",
    "* **Model**  \n",
    "  * AWS Bedrock **Claude 3.5 Sonnet** (`anthropic.claude-3-5-sonnet-20240620-v1:0`).\n",
    "\n",
    "---\n",
    "\n",
    "## How it works (high level)\n",
    "\n",
    "1. **Page edges**  \n",
    "   * Use `Q_PAGE_REL` to page through `(:Case)-[r:CITES_TO]->(:Case)` relations in batches by `id(r)`.\n",
    "\n",
    "2. **Gather snippets**  \n",
    "   * Read `properties(r)` and collect keys matching `snippet_\\d+`.\n",
    "   * Sort by numeric suffix and build a labeled block:\n",
    "\n",
    "     ```text\n",
    "     snippet 1:\n",
    "\n",
    "     [text]\n",
    "\n",
    "     snippet 2:\n",
    "\n",
    "     [text]\n",
    "     ```\n",
    "\n",
    "3. **Build the prompt**\n",
    "\n",
    "   Claude sees:\n",
    "\n",
    "   * Citing case **name** and **summary**\n",
    "   * Cited case **name** and **summary**\n",
    "   * **Cited short citation** (first from `citation_pipe`)\n",
    "   * **Snippets block**\n",
    "\n",
    "4. **Model output (single judge)**\n",
    "\n",
    "   Claude is instructed to return strict JSON:\n",
    "\n",
    "   ```json\n",
    "   {\n",
    "     \"classification\": \"Positive|Neutral|Negative\",\n",
    "     \"rationale\": \"Four sentences with one short direct quote.\"\n",
    "   }\n",
    "\n",
    "* If the call succeeds and the JSON can be parsed → classification and rationale are stored.\n",
    "* If the call fails (too long / JSON error / empty response, etc.) → this edge is labeled **`\"Unknown\"`** with an error message recorded in the rationale.\n",
    "\n",
    "5. **Write back to the edge**\n",
    "\n",
    "   For each relation:\n",
    "\n",
    "   * `r.treatment_label`          (Claude’s label or `\"Unknown\"`)\n",
    "   * `r.treatment_rationale`      (four-sentence rationale or failure message)\n",
    "   * `r.treatment_snippet`        (all snippets joined)\n",
    "   * `r.model_used`               (e.g., `\"Claude 3.5 Sonnet (…)\"`)\n",
    "   * `r.updated_at_utc`           (timestamp from `datetime()`)\n",
    "\n",
    "6. **Already-labeled edges**\n",
    "\n",
    "   * If `force=False` and `r.treatment_label` exists and is not `\"Unknown\"`, the edge is **skipped**.\n",
    "   * Set `force=True` if you want to overwrite existing labels.\n",
    "\n",
    "7. **No snippets**\n",
    "\n",
    "   * If the relation has **no `snippet_1..N`**, it is labeled **`\"Unknown\"`** and the rationale explains that there were no snippets to analyze.\n",
    "\n",
    "---\n",
    "\n",
    "## Token budgeting & overflow handling\n",
    "\n",
    "* Uses a shared tokenizer (based on `cl100k_base`) to estimate total tokens for:\n",
    "\n",
    "  * System prompt + user prompt (summaries + snippets).\n",
    "* If the prompt exceeds the context budget:\n",
    "\n",
    "  * Compute the overhead for system + an empty user prompt.\n",
    "  * Use the remaining budget only for **snippets**.\n",
    "  * Trim the snippet block down to the allowed token count (summaries are preserved).\n",
    "* If the prompt is **still too long** after trimming:\n",
    "\n",
    "  * The call is flagged as **`\"too_long\"`**.\n",
    "  * The edge is labeled **`\"Unknown\"`** with a rationale explaining the overflow.\n",
    "\n",
    "---\n",
    "\n",
    "## Robust JSON parsing & retries\n",
    "\n",
    "Each call to Claude:\n",
    "\n",
    "* Retries up to **3 times** with increasing strictness in the system prompt:\n",
    "\n",
    "  1. Normal instructions.\n",
    "  2. “Output JSON only. No prose, no backticks.”\n",
    "  3. “Output ONLY a JSON object with `classification` and `rationale`. Double quotes. No trailing commas. No markdown.”\n",
    "\n",
    "* JSON cleaning and parsing:\n",
    "\n",
    "  * Strip `json / ` fences and extra whitespace.\n",
    "  * Normalize smart quotes.\n",
    "  * Remove trailing commas before `}` or `]`.\n",
    "  * If needed, apply a heuristic to fix **unescaped double quotes** inside the `rationale` string.\n",
    "  * Try parsing the entire output as JSON.\n",
    "  * If that fails, search for the **largest `{...}` block** and parse that.\n",
    "  * Support both:\n",
    "\n",
    "    * Direct JSON object, and\n",
    "    * JSON nested or stringified inside the `rationale` field.\n",
    "\n",
    "* Status codes:\n",
    "\n",
    "  * `\"ok\"`\n",
    "  * `\"json_parse_failed\"`\n",
    "  * `\"bad_keys_or_values\"`\n",
    "  * `\"too_long\"`\n",
    "  * `\"empty_response\"`\n",
    "  * `\"api_error\"`, `\"api_error_throttled\"`, `\"api_response_error\"`\n",
    "\n",
    "If the final status is not `\"ok\"`, the edge is labeled `\"Unknown\"` and the status is described in the rationale.\n",
    "\n",
    "---\n",
    "\n",
    "## Neo4j I/O\n",
    "\n",
    "**Reads**\n",
    "\n",
    "* `Q_PAGE_REL` returns:\n",
    "\n",
    "  * Source case: `id`, `name`, `decision_date`, `citation_pipe`, `opinion_summary`, URL\n",
    "  * Target case: `id`, `name`, `decision_date`, `citation_pipe`, `opinion_summary`\n",
    "  * `properties(r)` (snippets and any previous fields)\n",
    "  * Existing `r.treatment_label` (for `force` behavior)\n",
    "\n",
    "* Snippets are pulled from `properties(r)` as all keys matching `snippet_\\d+` and sorted numerically.\n",
    "\n",
    "**Writes**\n",
    "\n",
    "* `Q_WRITE_REL_ANNOT` sets on the relation:\n",
    "\n",
    "  * `treatment_label`\n",
    "  * `treatment_rationale`\n",
    "  * `treatment_snippet`\n",
    "  * `model_used`\n",
    "  * `updated_at_utc`\n",
    "\n",
    "> Already-labeled edges are **skipped** unless `force=True`.\n",
    "\n",
    "---\n",
    "\n",
    "## CSV outputs (optional)\n",
    "\n",
    "With `results_csv=True`, the notebook writes (by default):\n",
    "\n",
    "* `edge_classifications_claude.csv`\n",
    "\n",
    "Each row includes:\n",
    "\n",
    "* Source / Target:\n",
    "\n",
    "  * IDs, names, decision dates, `citation_pipe`\n",
    "  * Summaries (source and target)\n",
    "* Snippet and label:\n",
    "\n",
    "  * Joined `Opinion Snippet`\n",
    "  * **Citation Evaluation** (Claude label or `\"Unknown\"`)\n",
    "  * **LLM Rationale** (four-sentence explanation or error message)\n",
    "* URL:\n",
    "\n",
    "  * Source case URL (CourtListener if available)\n",
    "\n",
    "### Optional merges\n",
    "\n",
    "* **Append to labeled dataset** (`append_to_labeled_dataset_csv`)\n",
    "  Produces a **model comparison** CSV joining your human-labeled dataset with Claude’s labels and rationales.\n",
    "* **Compare with previous model CSV** (`compare_with_previous_csv`)\n",
    "  Optional side-by-side diff of an older model CSV vs. the current Claude outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## Key parameters\n",
    "\n",
    "```python\n",
    "label_all_citations(\n",
    "    results_csv: bool = False,\n",
    "    results_csv_filename: str = \"edge_classifications_claude.csv\",\n",
    "    batch_size: int = 200,\n",
    "    echo: bool = True,\n",
    "    force: bool = False,\n",
    "    append_to_labeled_dataset_csv: Optional[str] = None,\n",
    "    labeled_output_csv: Optional[str] = None,\n",
    "    compare_with_previous_csv: Optional[str] = None,\n",
    "    comparison_output_csv: Optional[str] = None,\n",
    ")\n",
    "```\n",
    "\n",
    "* `results_csv` / `results_csv_filename`\n",
    "  Control whether and where to write the output CSV.\n",
    "* `batch_size`\n",
    "  Number of edges processed per Neo4j page.\n",
    "* `echo`\n",
    "  If `True`, prints `\"<Source> → <Target>: <Label>\"` plus batch-level summaries.\n",
    "* `force`\n",
    "  If `True`, re-labels edges even if `treatment_label` already exists and is not `\"Unknown\"`.\n",
    "* `append_to_labeled_dataset_csv`, `compare_with_previous_csv`\n",
    "  Optional CSV comparison / benchmarking utilities.\n",
    "\n",
    "---\n",
    "\n",
    "## Environment\n",
    "\n",
    "* Loads `../.env`. Required:\n",
    "\n",
    "  * `NEO4J_URI`, `NEO4J_USERNAME`, `NEO4J_PASSWORD`\n",
    "    (and optionally `NEO4J_DATABASE`, default `\"neo4j\"`)\n",
    "  * `BEDROCK_REGION` (default `\"us-east-1\"`)\n",
    "\n",
    "---\n",
    "\n",
    "## Quick start\n",
    "\n",
    "```python\n",
    "label_all_citations(\n",
    "    force=False,\n",
    "    echo=True,\n",
    "    results_csv=True,\n",
    "    results_csv_filename=\"edge_classifications_claude.csv\",\n",
    ")\n",
    "```\n",
    "\n",
    "This will:\n",
    "\n",
    "* Page through all `CITES_TO` edges that are not already labeled (unless `force=True`).\n",
    "* Call **Claude 3.5 Sonnet** once per edge.\n",
    "* Write `treatment_label` / `treatment_rationale` / `treatment_snippet` and metadata back to Neo4j.\n",
    "* Export `edge_classifications_claude.csv` for review.\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "* **“Unknown” due to missing snippets**\n",
    "  Run the Snippet Retriever so edges have `snippet_1..N`.\n",
    "\n",
    "* **“Unknown (too_long)”**\n",
    "  Snippets are auto-trimmed, but if you still hit token limits:\n",
    "\n",
    "  * Reduce the number or length of snippets per edge, or\n",
    "  * Lower `max_new_tokens` to give more room to the input.\n",
    "\n",
    "* **Frequent JSON parse / key errors**\n",
    "  Retries and JSON cleaning are already enabled. If errors continue:\n",
    "\n",
    "  * Set `echo=True` and inspect raw Claude outputs.\n",
    "  * Consider simplifying summaries or prompts if the model is adding extra prose or nested structures.\n",
    "\n",
    "```\n",
    "::contentReference[oaicite:0]{index=0}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbb61c4-d969-4934-9527-efdf3ab17ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in /opt/conda/lib/python3.12/site-packages (6.0.3)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from neo4j) (2024.2)\n"
     ]
    }
   ],
   "source": [
    "# Install (if applicable)\n",
    "! pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e06ba7-d97f-4154-bf06-c11d92755d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.12.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d94089-4492-45b6-8e6f-4711a7302298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, json, pathlib, logging, datetime as dt, random\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from botocore.exceptions import ClientError, BotoCoreError\n",
    "import tiktoken\n",
    "\n",
    "# Quiet noisy logs (incl. Neo4j notifications/deprecations)\n",
    "for _n in (\"neo4j\", \"neo4j.notifications\", \"neo4j.work.simple\"):\n",
    "    logging.getLogger(_n).setLevel(logging.ERROR)\n",
    "os.environ.setdefault(\"NEO4J_DRIVER_LOG_LEVEL\", \"ERROR\")\n",
    "\n",
    "# =========================\n",
    "# Config / ENV\n",
    "# =========================\n",
    "BEDROCK_REGION   = os.getenv(\"BEDROCK_REGION\", \"us-east-1\")\n",
    "BEDROCK_MODEL_ID = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "# .env is always one level up from this notebook\n",
    "load_dotenv(\"../.env\", override=True)\n",
    "NEO4J_URI       = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME  = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD  = os.getenv(\"NEO4J_PASSWORD\")\n",
    "NEO4J_DATABASE  = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "\n",
    "if not (NEO4J_URI and NEO4J_USERNAME and NEO4J_PASSWORD):\n",
    "    raise RuntimeError(\"Missing Neo4j connection settings. Check ../.env for NEO4J_URI/USERNAME/PASSWORD.\")\n",
    "\n",
    "# =========================\n",
    "# Tokenizer for precise prompt sizing (Claude 3 / 3.5)\n",
    "# =========================\n",
    "_tok = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def _count_tokens(text: str) -> int:\n",
    "    \"\"\"Approximate Claude token count.\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(_tok.encode(text))\n",
    "\n",
    "def _trim_to_tokens(text: str, max_tokens: int) -> str:\n",
    "    \"\"\"Trim text to max_tokens using Claude-like tokenization.\"\"\"\n",
    "    token_ids = _tok.encode(text)\n",
    "    if len(token_ids) <= max_tokens:\n",
    "        return text\n",
    "    return _tok.decode(token_ids[:max_tokens])\n",
    "\n",
    "def _max_ctx_tokens_for_bedrock(max_new_tokens: int) -> int:\n",
    "    \"\"\"\n",
    "    Claude 3.5 Sonnet supports ~200k tokens of context.\n",
    "    Reserve budget for response + overhead.\n",
    "    \"\"\"\n",
    "    max_context = 200_000\n",
    "    buffer_for_overhead = 2_000\n",
    "    reserve_for_output  = max_new_tokens + 128\n",
    "    return max_context - buffer_for_overhead - reserve_for_output\n",
    "\n",
    "# =========================\n",
    "# Bedrock client (lazy)\n",
    "# =========================\n",
    "_bedrock_client = None\n",
    "\n",
    "def _bedrock():\n",
    "    global _bedrock_client\n",
    "    if _bedrock_client is None:\n",
    "        _bedrock_client = boto3.client(\"bedrock-runtime\", region_name=BEDROCK_REGION)\n",
    "    return _bedrock_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584297bb-8e9e-478f-9fd6-b694f7e2fd45",
   "metadata": {},
   "source": [
    "## Edge Classification Prompt (Snippet Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f3012ee-fc67-4dbf-9919-40de462896ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Prompts\n",
    "# =========================\n",
    "SYSTEM_PROMPT = \"\"\"Role: You are an experienced lawyer specializing in legal citation analysis. \n",
    "Goal: Your goal is to classify how a citing case treats a cited case (e.g., Positive, Neutral, Negative) when given:\n",
    "1.        Citing Case Name: the name of the citing case.\n",
    "2.        Citing Case Summary: the summary of the citing case.\n",
    "3.        Cited Case Name: the name of the cited case\n",
    "4.        Cited Case Summary: the summary of the cited case.\n",
    "5.        Snippets (from citing opinion where the cited case appears): List of text snippets where the cited case is cited in the citing opinion text \n",
    "\n",
    "CRITICAL RULES (must follow):\n",
    "1. **If the citing case uses the cited case to establish or support ANY legal rule, doctrine, standard, test, or conclusion, classify as POSITIVE.**\n",
    "   - This includes situations where the case:\n",
    "     • recites a standard from the case,\n",
    "     • cites the case as part of a string cite supporting a legal principle,\n",
    "     • uses the case as an example consistent with its reasoning,\n",
    "     • applies reasoning from the cited case.\n",
    "\n",
    "   - IMPORTANT: The opinion **does NOT need to use words like “follow,” “adopt,” “agree,” or “apply.”**  \n",
    "     Any supportive or explanatory use counts as Positive.\n",
    "\n",
    "2. Classify as NEUTRAL **only when the case is mentioned without being used as authority**.\n",
    "   - Examples:\n",
    "     • descriptive background\n",
    "     • illustrating a factual distinction\n",
    "     • quoting language without using it to support a rule\n",
    "     • noting procedural history\n",
    "\n",
    "3. Classify as NEGATIVE when the citing court:\n",
    "   - criticizes, limits, distinguishes, rejects, or declines to follow the cited case.\n",
    "\n",
    "4. When writing the rationale:\n",
    "   - Include ONE short direct quote from the citing opinion.\n",
    "   - The quote must illustrate why the treatment is Positive, Neutral, or Negative.\n",
    "   - Provide EXACTLY four sentences.\n",
    "\n",
    "OUTPUT FORMAT (strict):\n",
    "{\n",
    "  \"classification\": \"Positive|Neutral|Negative\",\n",
    "  \"rationale\": \"Four sentences with one direct quote.\"\n",
    "}\n",
    "\"\"\"\n",
    "USER_PROMPT_TMPL = \"\"\"Classify how the citing case treats the cited case. Return your answer as a json format: {{classification: \"\", rationale: \"\"}} where label is one of Positive, Neutral, or Negative and rationale is a four-sentence explanation that justifies your classification\n",
    "using evidence from the cited paragraph and case summaries.\n",
    "\n",
    "Input:\n",
    "Citing Case Name: {citing_case_name}\n",
    "Citing Case Summary: {citing_case_summary}\n",
    "Cited Case Name: {cited_case_name}\n",
    "Cited Case Citation: {cited_case_citation}\n",
    "Cited Case Summary: {cited_case_summary}\n",
    "\n",
    "Snippets (from citing opinion where the cited case appears):\n",
    "{snippet_block}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86060f1-a79a-43e9-b9da-e2af511b7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inst(sys: str, usr: str) -> str:\n",
    "    return f\"<s>[INST]{sys}\\n{usr}[/INST]\"\n",
    "\n",
    "def _format_snippet_block_labeled(snippets: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Render as:\n",
    "    snippet 1:\n",
    "\n",
    "    [text]\n",
    "\n",
    "    snippet 2:\n",
    "\n",
    "    [text]\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    for i, s in enumerate(snippets, 1):\n",
    "        ss = (s or \"\").strip()\n",
    "        if not ss:\n",
    "            continue\n",
    "        blocks.append(f\"snippet {i}:\\n\\n{ss}\")\n",
    "    return \"\\n\\n\".join(blocks) if blocks else \"snippet 1:\\n\\n[N/A]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ce592-2f9d-4aa0-b01a-53262af8023f",
   "metadata": {},
   "source": [
    "## Cypher Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bac7d4-77a5-454f-bdf4-d5c0281021ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cypher queries\n",
    "# =========================\n",
    "COL_URL_HEADER = \"Source Case CourtListener URL (may be incorrect for some cases)\"\n",
    "\n",
    "# Paginated fetch of relations for classification\n",
    "Q_PAGE_REL = \"\"\"\n",
    "MATCH (s:Case)-[r:CITES_TO]->(t:Case)\n",
    "WHERE id(r) > $after_id\n",
    "RETURN id(r) AS rel_id,\n",
    "       s.id AS src_id, coalesce(s.name,'') AS src_name, s.decision_date AS src_date, s.citation_pipe AS src_cite,\n",
    "       coalesce(s.court_listener_url, s.url, '') AS src_url,\n",
    "       coalesce(s.opinion_summary,'') AS src_summary,\n",
    "       t.id AS tgt_id, coalesce(t.name,'') AS tgt_name, t.decision_date AS tgt_date, t.citation_pipe AS tgt_cite,\n",
    "       coalesce(t.opinion_summary,'') AS tgt_summary,\n",
    "       properties(r) AS rel_props,\n",
    "       r.treatment_label AS existing_label\n",
    "ORDER BY rel_id\n",
    "LIMIT $limit\n",
    "\"\"\"\n",
    "\n",
    "Q_WRITE_REL_ANNOT = \"\"\"\n",
    "MATCH (s:Case {id:$src_id})-[r:CITES_TO]->(t:Case {id:$tgt_id})\n",
    "SET r.treatment_label     = $label,\n",
    "    r.treatment_rationale = $rationale,\n",
    "    r.treatment_snippet   = $snippet_joined,\n",
    "    r.model_used          = $model_used,\n",
    "    r.updated_at_utc      = datetime()\n",
    "RETURN count(r) AS updated\n",
    "\"\"\"\n",
    "\n",
    "Q_COUNT_LABELS = \"\"\"\n",
    "MATCH ()-[r:CITES_TO]->()\n",
    "RETURN\n",
    "  sum(CASE WHEN r.treatment_label = 'Positive' THEN 1 ELSE 0 END) AS pos_total,\n",
    "  sum(CASE WHEN r.treatment_label = 'Neutral'  THEN 1 ELSE 0 END) AS neu_total,\n",
    "  sum(CASE WHEN r.treatment_label = 'Negative' THEN 1 ELSE 0 END) AS neg_total,\n",
    "  sum(CASE WHEN r.treatment_label = 'Unknown'  THEN 1 ELSE 0 END) AS unk_total\n",
    "\"\"\"\n",
    "\n",
    "# Full dataset of currently labeled edges (for show_all_labels_in_output_csv=True)\n",
    "Q_ALL_LABELED_REL = \"\"\"\n",
    "MATCH (s:Case)-[r:CITES_TO]->(t:Case)\n",
    "WHERE r.treatment_label IS NOT NULL\n",
    "RETURN\n",
    "  s.id AS src_id,\n",
    "  coalesce(s.name,'') AS src_name,\n",
    "  s.decision_date AS src_date,\n",
    "  s.citation_pipe AS src_cite,\n",
    "  coalesce(s.court_listener_url, s.url, '') AS src_url,\n",
    "  coalesce(s.opinion_summary,'') AS src_summary,\n",
    "  t.id AS tgt_id,\n",
    "  coalesce(t.name,'') AS tgt_name,\n",
    "  t.decision_date AS tgt_date,\n",
    "  t.citation_pipe AS tgt_cite,\n",
    "  coalesce(t.opinion_summary,'') AS tgt_summary,\n",
    "  coalesce(r.treatment_snippet,'') AS snippet_joined,\n",
    "  r.treatment_label AS treatment_label,\n",
    "  coalesce(r.treatment_rationale,'') AS treatment_rationale\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0bd5cb-bf54-4db8-a4e2-f768a283d7ec",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0667006-5525-4592-9b77-26cd4f22ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _first_citation(citation_pipe: Optional[str]) -> str:\n",
    "    if not citation_pipe:\n",
    "        return \"\"\n",
    "    parts = [p.strip() for p in re.split(r'[|;]+', citation_pipe) if p.strip()]\n",
    "    return parts[0] if parts else \"\"\n",
    "\n",
    "def _extract_numbered_snippets(rel_props: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Pull snippet_1, snippet_2, ... from properties(r) and return as a list\n",
    "    sorted by the numeric suffix. Empty/whitespace-only are skipped.\n",
    "    \"\"\"\n",
    "    out: List[Tuple[int, str]] = []\n",
    "    for k, v in (rel_props or {}).items():\n",
    "        m = re.fullmatch(r\"snippet_(\\d+)\", k)\n",
    "        if not m:\n",
    "            continue\n",
    "        try:\n",
    "            idx = int(m.group(1))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        text = str(v or \"\").strip()\n",
    "        if text:\n",
    "            out.append((idx, text))\n",
    "    out.sort(key=lambda p: p[0])\n",
    "    return [t for _, t in out]\n",
    "\n",
    "def _read_csv_fallback(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read CSV trying a few common encodings in order.\n",
    "    Returns a DataFrame or raises the last exception.\n",
    "    \"\"\"\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    # As a last resort, decode with cp1252 and replace invalid bytes to avoid hard fail\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"cp1252\", errors=\"replace\") as fh:\n",
    "            return pd.read_csv(fh)\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise last_err if last_err else RuntimeError(\"Failed to read CSV with fallback encodings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4095b89-8d0e-4a2c-acea-93b2e8ac249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# JSON cleaning / normalization\n",
    "# =========================\n",
    "_BACKTICKS_RE = re.compile(r\"^```(?:json)?|```$\", re.MULTILINE)\n",
    "\n",
    "def _clean_json(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize LLM-style JSON:\n",
    "      - strip ```json fences\n",
    "      - collapse newlines into spaces (multi-line strings become single-line)\n",
    "      - normalize smart quotes\n",
    "      - unwrap a single outer quote pair if it surrounds a JSON object/array\n",
    "      - drop dangling commas before } or ]\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "\n",
    "    # Strip markdown fences\n",
    "    s = _BACKTICKS_RE.sub(\"\", s)\n",
    "\n",
    "    # Normalize newlines and carriage returns to spaces so raw line breaks\n",
    "    # inside strings do not break JSON parsing.\n",
    "    s = s.replace(\"\\r\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "\n",
    "    s = s.strip()\n",
    "\n",
    "    # Normalize smart quotes / apostrophes\n",
    "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "\n",
    "    # If the entire payload is quoted as a big JSON string, unwrap it once\n",
    "    if len(s) >= 2 and s[0] == s[-1] == '\"':\n",
    "        inner = s[1:-1].strip()\n",
    "        if (\"{\" in inner and \"}\" in inner) or (\"[\" in inner and \"]\" in inner):\n",
    "            s = inner\n",
    "\n",
    "    # Remove trailing commas before object/array close\n",
    "    s = re.sub(r\",\\s*([}\\]])\", r\"\\1\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def _fix_unescaped_quotes_in_rationale(payload: str) -> str:\n",
    "    \"\"\"\n",
    "    Heuristic: inside the rationale string, replace any *unescaped* \" with '.\n",
    "    This recovers JSON where the model used double quotes for a direct quote\n",
    "    but did not escape them.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        key = '\"rationale\"'\n",
    "        i = payload.find(key)\n",
    "        if i == -1:\n",
    "            return payload\n",
    "\n",
    "        colon = payload.find(\":\", i)\n",
    "        if colon == -1:\n",
    "            return payload\n",
    "\n",
    "        # First \" after the colon starts the rationale string\n",
    "        first_q = payload.find('\"', colon + 1)\n",
    "        # Assume rationale is the *last* string value: last \" in the payload\n",
    "        last_q = payload.rfind('\"')\n",
    "\n",
    "        if first_q == -1 or last_q == -1 or last_q <= first_q:\n",
    "            return payload\n",
    "\n",
    "        body = payload[first_q + 1:last_q]\n",
    "\n",
    "        # Replace any unescaped \" inside the rationale body with '\n",
    "        out_chars = []\n",
    "        for idx, ch in enumerate(body):\n",
    "            if ch == '\"' and (idx == 0 or body[idx - 1] != \"\\\\\"):\n",
    "                out_chars.append(\"'\")\n",
    "            else:\n",
    "                out_chars.append(ch)\n",
    "        fixed_body = \"\".join(out_chars)\n",
    "\n",
    "        return payload[:first_q + 1] + fixed_body + payload[last_q:]\n",
    "    except Exception:\n",
    "        # On any error, return original unchanged\n",
    "        return payload\n",
    "\n",
    "\n",
    "def _normalize_from_js(js: Any) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Given a parsed JSON value (dict or string), extract:\n",
    "      classification ∈ {\"Positive\",\"Neutral\",\"Negative\"}\n",
    "      rationale: non-empty string\n",
    "\n",
    "    Handles nested or stringified JSON inside the 'rationale' field.\n",
    "    \"\"\"\n",
    "    valid_labels = {\"Positive\", \"Neutral\", \"Negative\"}\n",
    "\n",
    "    # Case 1: dict at top level\n",
    "    if isinstance(js, dict):\n",
    "        c = js.get(\"classification\") or js.get(\"label\")\n",
    "        r = js.get(\"rationale\")\n",
    "\n",
    "        # If rationale itself is another dict, try inner fields\n",
    "        if isinstance(r, dict):\n",
    "            inner_c = r.get(\"classification\") or r.get(\"label\")\n",
    "            inner_r = r.get(\"rationale\")\n",
    "            if inner_c in valid_labels and isinstance(inner_r, str) and inner_r.strip():\n",
    "                return inner_c, inner_r.strip()\n",
    "\n",
    "        # If rationale is a string that looks like JSON, try to parse it\n",
    "        if isinstance(r, str):\n",
    "            r_str = r.strip()\n",
    "            if len(r_str) >= 2 and r_str[0] == r_str[-1] == '\"':\n",
    "                r_str = r_str[1:-1].strip()\n",
    "            if r_str.startswith(\"{\") and \"classification\" in r_str and \"rationale\" in r_str:\n",
    "                try:\n",
    "                    inner_js = json.loads(_clean_json(r_str))\n",
    "                    inner_c, inner_r = _normalize_from_js(inner_js)\n",
    "                    if inner_c and inner_r:\n",
    "                        return inner_c, inner_r\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Normal case: classification + plain rationale\n",
    "        if c in valid_labels and isinstance(r, str) and r.strip():\n",
    "            return c, r.strip()\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    # Case 2: top-level is a string that itself contains JSON\n",
    "    if isinstance(js, str):\n",
    "        txt = js.strip()\n",
    "        if len(txt) >= 2 and txt[0] == txt[-1] == '\"':\n",
    "            txt = txt[1:-1].strip()\n",
    "        if txt.startswith(\"{\") and \"classification\" in txt and \"rationale\" in txt:\n",
    "            try:\n",
    "                inner_js = json.loads(_clean_json(txt))\n",
    "                return _normalize_from_js(inner_js)\n",
    "            except Exception:\n",
    "                return None, None\n",
    "\n",
    "    return None, None\n",
    "    \n",
    "def _extract_rationale_between_markers(s: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Heuristic rationale extractor that ignores JSON validity.\n",
    "\n",
    "    Strategy:\n",
    "      - Find `\"rationale\"` key\n",
    "      - From the colon, find the first `\"` that starts the string\n",
    "      - Then take everything up to the LAST `\"` before the closing `}`\n",
    "\n",
    "    This allows inner unescaped \" characters inside the rationale, since\n",
    "    we are not trying to parse the JSON string literally.\n",
    "    \"\"\"\n",
    "    s_lower = s.lower()\n",
    "    key_idx = s_lower.find('\"rationale\"')\n",
    "    if key_idx == -1:\n",
    "        return None\n",
    "\n",
    "    colon_idx = s.find(\":\", key_idx)\n",
    "    if colon_idx == -1:\n",
    "        return None\n",
    "\n",
    "    # First quote after the colon = start of rationale string\n",
    "    first_quote = s.find('\"', colon_idx)\n",
    "    if first_quote == -1:\n",
    "        return None\n",
    "\n",
    "    # Try to limit search to this JSON object: last \" before the last }\n",
    "    last_brace = s.rfind(\"}\")\n",
    "    if last_brace == -1:\n",
    "        search_end = len(s)\n",
    "    else:\n",
    "        search_end = last_brace\n",
    "\n",
    "    last_quote = s.rfind('\"', first_quote + 1, search_end)\n",
    "    if last_quote == -1:\n",
    "        # Fallback: last quote in the whole string\n",
    "        last_quote = s.rfind('\"', first_quote + 1)\n",
    "        if last_quote == -1:\n",
    "            return None\n",
    "\n",
    "    rationale = s[first_quote + 1:last_quote]\n",
    "\n",
    "    # Simple cleanup of common escape sequences\n",
    "    rationale = rationale.replace('\\\\\"', '\"').replace(\"\\\\n\", \" \").replace(\"\\\\r\", \" \")\n",
    "    rationale = rationale.strip()\n",
    "    return rationale or None\n",
    "\n",
    "\n",
    "def _extract_classification_and_rationale(raw_text: str) -> Tuple[Optional[str], Optional[str], str]:\n",
    "    \"\"\"\n",
    "    Extract classification and rationale from Claude output.\n",
    "\n",
    "    New behavior:\n",
    "      - Classification is read via regex from \"classification\" / \"label\".\n",
    "      - Rationale is extracted heuristically as everything between\n",
    "        `\"rationale\": \"` and the closing `}` of the JSON object, so it\n",
    "        is not cut off by inner unescaped double quotes.\n",
    "\n",
    "    Fallback:\n",
    "      - If this fails, we still try a simple json.loads() on the cleaned\n",
    "        text and read classification / rationale from there.\n",
    "    \"\"\"\n",
    "    if not raw_text or not str(raw_text).strip():\n",
    "        return None, None, \"empty_response\"\n",
    "\n",
    "    # Normalize markdown fences, newlines, smart quotes, etc.\n",
    "    s = _clean_json(str(raw_text).strip())\n",
    "    valid_labels = {\"Positive\", \"Neutral\", \"Negative\"}\n",
    "\n",
    "    # ---- 1) Classification via regex (safe; label string has no inner quotes) ----\n",
    "    label_match = re.search(\n",
    "        r'\"(?:classification|label)\"\\s*:\\s*\"(?P<label>Positive|Neutral|Negative)\"',\n",
    "        s,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "    label: Optional[str] = None\n",
    "    if label_match:\n",
    "        label = label_match.group(\"label\").capitalize()\n",
    "        if label not in valid_labels:\n",
    "            label = None\n",
    "\n",
    "    # ---- 2) Rationale via heuristic between \"rationale\" and closing brace ----\n",
    "    rationale = _extract_rationale_between_markers(s)\n",
    "\n",
    "    if label and rationale:\n",
    "        return label, rationale, \"ok\"\n",
    "\n",
    "    # ---- 3) Simple JSON fallback (for weird but valid JSON cases) ----\n",
    "    try:\n",
    "        js = json.loads(s)\n",
    "        c = js.get(\"classification\") or js.get(\"label\")\n",
    "        r = js.get(\"rationale\")\n",
    "        if c in valid_labels and isinstance(r, str) and r.strip():\n",
    "            return c, r.strip(), \"ok\"\n",
    "        return None, None, \"bad_keys_or_values\"\n",
    "    except Exception:\n",
    "        return None, None, \"json_parse_failed\"\n",
    "\n",
    "\n",
    "def _describe_error(status: str) -> str:\n",
    "    \"\"\"\n",
    "    Map internal status codes to a human-readable error description.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"too_long\": \"too_long: prompt exceeded maximum context tokens\",\n",
    "        \"json_parse_failed\": \"json_parse_failed: could not parse LLM output as JSON\",\n",
    "        \"bad_keys_or_values\": \"bad_keys_or_values: missing or invalid classification/rationale in JSON\",\n",
    "        \"api_error_throttled\": \"api_error_throttled: Bedrock throttled the request\",\n",
    "        \"api_error\": \"api_error: generic AWS/Bedrock client or service error\",\n",
    "        \"api_response_error\": \"api_response_error: malformed or unexpected API response\",\n",
    "        \"empty_response\": \"empty_response: model returned empty text\",\n",
    "        \"missing_snippets\": \"missing_snippets: no snippets available on the relation (snippet_1..N).\",\n",
    "        \"ok\": \"ok: successful classification\",\n",
    "    }\n",
    "    return mapping.get(status, status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37ff09-40ff-4e03-9024-2fb72d93e057",
   "metadata": {},
   "source": [
    "## LLM Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da923b8-fce7-446b-8a35-1e6320b7a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LLM classify (Claude 3.5 Sonnet on Bedrock)\n",
    "# =========================\n",
    "def classify_with_bedrock(\n",
    "    *,\n",
    "    citing_case_name: str,\n",
    "    citing_case_summary: str,\n",
    "    cited_case_name: str,\n",
    "    cited_case_citation: str,\n",
    "    cited_case_summary: str,\n",
    "    snippet_block_labeled: str,\n",
    "    max_new_tokens: int = 1024,\n",
    "    retries: int = 3,\n",
    "    echo: bool = False\n",
    ") -> Tuple[\n",
    "    Optional[str], Optional[str], str, str, int,\n",
    "    List[Dict[str, Any]], str\n",
    "]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      classification: Optional[str]\n",
    "      rationale: Optional[str]\n",
    "      status: final status code (\"ok\", \"json_parse_failed\", etc.)\n",
    "      raw_output: final raw output (string)\n",
    "      attempt_used: which attempt index produced the final result (1-based)\n",
    "      attempt_logs: list of per-attempt dicts:\n",
    "          {\n",
    "            \"attempt\": int,\n",
    "            \"status\": str,\n",
    "            \"raw\": str,\n",
    "            \"input\": str  # user prompt string (no system wrapper)\n",
    "          }\n",
    "      user_input: the user prompt string sent to the LLM\n",
    "\n",
    "    status: \"ok\" | \"json_parse_failed\" | \"bad_keys_or_values\" | \"too_long\"\n",
    "            | \"api_error\" | \"api_error_throttled\" | \"api_response_error\" | \"empty_response\"\n",
    "    \"\"\"\n",
    "    client = _bedrock()\n",
    "\n",
    "    user = USER_PROMPT_TMPL.format(\n",
    "        citing_case_name=(citing_case_name or \"\").strip(),\n",
    "        citing_case_summary=(citing_case_summary or \"\").strip(),\n",
    "        cited_case_name=(cited_case_name or \"\").strip(),\n",
    "        cited_case_citation=(cited_case_citation or \"\").strip(),\n",
    "        cited_case_summary=(cited_case_summary or \"\").strip(),\n",
    "        snippet_block=snippet_block_labeled\n",
    "    )\n",
    "\n",
    "    attempt_logs: List[Dict[str, Any]] = []\n",
    "\n",
    "    # -------- Token budget (system + user combined) --------\n",
    "    max_in = _max_ctx_tokens_for_bedrock(max_new_tokens)\n",
    "\n",
    "    # First pass usage\n",
    "    used = _count_tokens(SYSTEM_PROMPT + \"\\n\\n\" + user)\n",
    "\n",
    "    if used > max_in:\n",
    "        # Compute overhead for system + empty-user\n",
    "        empty_user = USER_PROMPT_TMPL.format(\n",
    "            citing_case_name=\"\",\n",
    "            citing_case_summary=\"\",\n",
    "            cited_case_name=\"\",\n",
    "            cited_case_citation=\"\",\n",
    "            cited_case_summary=\"\",\n",
    "            snippet_block=\"\"\n",
    "        )\n",
    "        overhead_tokens = _count_tokens(SYSTEM_PROMPT + \"\\n\\n\" + empty_user)\n",
    "\n",
    "        # Allowance for snippets inside the user prompt\n",
    "        allowance_for_snippets = max(512, max_in - overhead_tokens)\n",
    "\n",
    "        snippet_tokens = _count_tokens(snippet_block_labeled)\n",
    "        target_snippet_tokens = max(128, min(snippet_tokens, allowance_for_snippets))\n",
    "        trimmed = _trim_to_tokens(snippet_block_labeled, target_snippet_tokens)\n",
    "\n",
    "        user = USER_PROMPT_TMPL.format(\n",
    "            citing_case_name=(citing_case_name or \"\").strip(),\n",
    "            citing_case_summary=(citing_case_summary or \"\").strip(),\n",
    "            cited_case_name=(cited_case_name or \"\").strip(),\n",
    "            cited_case_citation=(cited_case_citation or \"\").strip(),\n",
    "            cited_case_summary=(cited_case_summary or \"\").strip(),\n",
    "            snippet_block=trimmed\n",
    "        )\n",
    "\n",
    "        used = _count_tokens(SYSTEM_PROMPT + \"\\n\\n\" + user)\n",
    "        if used > max_in:\n",
    "            if echo:\n",
    "                print(f\"      · prompt still too long after trim ({used} > {max_in}); giving up\")\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": 1,\n",
    "                \"status\": \"too_long\",\n",
    "                \"raw\": \"\",\n",
    "                \"input\": user,\n",
    "            })\n",
    "            return None, None, \"too_long\", \"\", 1, attempt_logs, user\n",
    "\n",
    "    last_err = \"json_parse_failed\"\n",
    "    last_raw = \"\"\n",
    "\n",
    "    # -------- Call Bedrock with retries --------\n",
    "    for attempt in range(1, retries + 1):\n",
    "        # Progressive strictness in system prompt\n",
    "        if attempt == 1:\n",
    "            sys = SYSTEM_PROMPT\n",
    "        elif attempt == 2:\n",
    "            sys = SYSTEM_PROMPT + \"\\n\\nSTRICT: Output JSON only. No prose, no backticks.\"\n",
    "        else:\n",
    "            sys = SYSTEM_PROMPT + (\n",
    "                '\\n\\nSTRICT: Output ONLY a JSON object with keys '\n",
    "                '\"classification\" and \"rationale\". Use double quotes. '\n",
    "                \"No trailing commas. No markdown.\"\n",
    "            )\n",
    "\n",
    "        if echo:\n",
    "            print(f\"      · attempt {attempt}/{retries}\")\n",
    "\n",
    "        # Small base delay to avoid hammering the API\n",
    "        time.sleep(0.75)\n",
    "\n",
    "        try:\n",
    "            resp = client.invoke_model(\n",
    "                modelId=BEDROCK_MODEL_ID,\n",
    "                body=json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": max_new_tokens,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"system\": sys,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": user}\n",
    "                    ]\n",
    "                }),\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "            )\n",
    "\n",
    "            raw_body = resp[\"body\"].read()\n",
    "            try:\n",
    "                body = json.loads(raw_body)\n",
    "            except Exception as e:\n",
    "                if echo:\n",
    "                    print(f\"      · failed to decode response JSON: {e}\")\n",
    "                last_err = \"api_response_error\"\n",
    "                last_raw = (\n",
    "                    raw_body.decode(\"utf-8\", errors=\"replace\")\n",
    "                    if isinstance(raw_body, (bytes, bytearray))\n",
    "                    else str(raw_body)\n",
    "                )\n",
    "\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": last_err,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": user,\n",
    "                })\n",
    "                time.sleep(1.5 * attempt)\n",
    "                continue\n",
    "\n",
    "            # Claude response format\n",
    "            try:\n",
    "                text = body[\"content\"][0][\"text\"]\n",
    "            except (KeyError, IndexError, TypeError) as e:\n",
    "                if echo:\n",
    "                    print(f\"      · unexpected response structure: {e}\")\n",
    "                last_err = \"api_response_error\"\n",
    "                last_raw = json.dumps(body)\n",
    "\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": last_err,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": user,\n",
    "                })\n",
    "                time.sleep(1.5 * attempt)\n",
    "                continue\n",
    "\n",
    "            last_raw = (text or \"\").strip()\n",
    "            if not last_raw:\n",
    "                if echo:\n",
    "                    print(\"      · empty response from model\")\n",
    "                status = \"empty_response\"\n",
    "                last_err = status\n",
    "                attempt_logs.append({\n",
    "                    \"attempt\": attempt,\n",
    "                    \"status\": status,\n",
    "                    \"raw\": last_raw,\n",
    "                    \"input\": user,\n",
    "                })\n",
    "                time.sleep(1.5 * attempt)\n",
    "                continue\n",
    "\n",
    "            # ---- Centralized parsing / normalization ----\n",
    "            c, r, status = _extract_classification_and_rationale(last_raw)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": status,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "\n",
    "            if status == \"ok\" and c and r:\n",
    "                return c, r, \"ok\", last_raw, attempt, attempt_logs, user\n",
    "\n",
    "            # Logical / JSON parsing failure\n",
    "            last_err = status\n",
    "            time.sleep(1.5 * attempt)\n",
    "            continue\n",
    "\n",
    "        except ClientError as e:\n",
    "            # AWS / Bedrock explicit error with code + message\n",
    "            code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
    "            msg = e.response.get(\"Error\", {}).get(\"Message\", \"\")\n",
    "\n",
    "            if echo:\n",
    "                print(f\"      · API ClientError on attempt {attempt}/{retries}: {code} - {msg}\")\n",
    "\n",
    "            msg_lower = msg.lower() if isinstance(msg, str) else \"\"\n",
    "            if \"token\" in msg_lower and (\"length\" in msg_lower or \"limit\" in msg_lower):\n",
    "                last_err = \"too_long\"\n",
    "            elif code in (\"ThrottlingException\", \"TooManyRequestsException\") or \"rate exceeded\" in msg_lower:\n",
    "                last_err = \"api_error_throttled\"\n",
    "            else:\n",
    "                last_err = \"api_error\"\n",
    "\n",
    "            last_raw = msg\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "\n",
    "            # More conservative backoff on throttling: exponential + small jitter\n",
    "            if last_err == \"api_error_throttled\":\n",
    "                backoff = min(60.0, float(2 ** attempt))\n",
    "                jitter = random.uniform(0.0, 0.5)\n",
    "                if echo:\n",
    "                    print(f\"      · throttled, backing off for {backoff + jitter:.2f} seconds\")\n",
    "                time.sleep(backoff + jitter)\n",
    "            else:\n",
    "                time.sleep(1.5 * attempt)\n",
    "            continue\n",
    "\n",
    "        except BotoCoreError as e:\n",
    "            if echo:\n",
    "                print(f\"      · BotoCoreError on attempt {attempt}/{retries}: {e}\")\n",
    "            last_err = \"api_error\"\n",
    "            last_raw = str(e)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "            time.sleep(1.5 * attempt)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            if echo:\n",
    "                print(f\"      · API error on attempt {attempt}/{retries}: {e}\")\n",
    "            last_err = \"api_error\"\n",
    "            last_raw = str(e)\n",
    "            attempt_logs.append({\n",
    "                \"attempt\": attempt,\n",
    "                \"status\": last_err,\n",
    "                \"raw\": last_raw,\n",
    "                \"input\": user,\n",
    "            })\n",
    "            time.sleep(1.5 * attempt)\n",
    "            continue\n",
    "\n",
    "    # After all retries\n",
    "    return None, None, last_err, last_raw, attempt, attempt_logs, user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e41c69-dd79-45a2-a068-637318e03d4e",
   "metadata": {},
   "source": [
    "## Full-dataset fetch for CSV Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c59f042-8ec3-4347-9783-a3ce9abfa89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Full-dataset fetch for CSV outputs\n",
    "# =========================\n",
    "def _fetch_all_labeled_results(driver) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all currently labeled CITES_TO edges from Neo4j\n",
    "    and shape them like rows_out / df_results.\n",
    "    \"\"\"\n",
    "    with driver.session(database=NEO4J_DATABASE) as s_all:\n",
    "        data = s_all.run(Q_ALL_LABELED_REL, {}).data()\n",
    "\n",
    "    rows_all: List[Dict[str, Any]] = []\n",
    "    for row in data:\n",
    "        rows_all.append({\n",
    "            \"Source Case ID\": row[\"src_id\"],\n",
    "            \"Source Case Name\": row[\"src_name\"] or \"\",\n",
    "            \"Source Case Decision Date\": row.get(\"src_date\") or \"\",\n",
    "            \"Source Case citation_pipe\": row.get(\"src_cite\") or \"\",\n",
    "            \"Source Case Summary\": row.get(\"src_summary\") or \"\",\n",
    "            \"Target Case ID\": row[\"tgt_id\"],\n",
    "            \"Target Case Name\": row[\"tgt_name\"] or \"\",\n",
    "            \"Target Case Decision Date\": row.get(\"tgt_date\") or \"\",\n",
    "            \"Target Case citation_pipe\": row.get(\"tgt_cite\") or \"\",\n",
    "            \"Target Case Summary\": row.get(\"tgt_summary\") or \"\",\n",
    "            \"Opinion Snippet\": row.get(\"snippet_joined\") or \"\",\n",
    "            \"Citation Evaluation\": row.get(\"treatment_label\") or \"\",\n",
    "            \"LLM Rationale\": row.get(\"treatment_rationale\") or \"\",\n",
    "            COL_URL_HEADER: row.get(\"src_url\") or \"\",\n",
    "        })\n",
    "    return pd.DataFrame(rows_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46acbe78-2ece-42f6-9989-19c6651ce905",
   "metadata": {},
   "source": [
    "## Batch Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89cb1f62-f95f-4bdb-9358-ec0ba9d3a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Batch driver\n",
    "# =========================\n",
    "def label_all_citations(\n",
    "    *,\n",
    "    results_csv: bool = False,\n",
    "    results_csv_filename: str = \"edge_classifications_claude.csv\",\n",
    "    batch_size: int = 200,\n",
    "    echo: bool = True,\n",
    "    force: bool = False,\n",
    "    append_to_labeled_dataset_csv: Optional[str] = None,\n",
    "    labeled_output_csv: Optional[str] = None,   # default = \"<labeled_csv_stem> - model comparison.csv\"\n",
    "    compare_with_previous_csv: Optional[str] = None,\n",
    "    comparison_output_csv: Optional[str] = None, # default = \"<prev_csv_stem> - model comparison.csv\"\n",
    "    show_all_labels_in_output_csv: bool = False,\n",
    "    failed_csv: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Classifies CITES_TO edges using snippets stored on the relation as snippet_1, snippet_2, ...\n",
    "    Prints only: \"<Source Name> → <Target Name>: <Label>\".\n",
    "\n",
    "    Writes r.treatment_label / r.treatment_rationale / r.treatment_snippet (joined) to the relation.\n",
    "\n",
    "    CSV behavior:\n",
    "      - If show_all_labels_in_output_csv=False (default):\n",
    "          df_results contains ONLY edges touched in this run.\n",
    "      - If show_all_labels_in_output_csv=True:\n",
    "          df_results is replaced with ALL currently labeled edges from Neo4j,\n",
    "          regardless of whether they were modified in this run.\n",
    "\n",
    "    Failed CSV:\n",
    "      - If failed_csv=True:\n",
    "          writes \"failed_citations.csv\" with:\n",
    "            first 11 columns (same as results_csv),\n",
    "            plus \"Attempt\", \"Input to LLM\", \"LLM Output\", \"Type of Error\"\n",
    "          for all edges that ended with label \"Unknown\" in this run,\n",
    "          with one row per LLM attempt (for those edges where the LLM was called).\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "    # Best-effort: disable notifications in the session (Neo4j 5+ driver)\n",
    "    try:\n",
    "        session = driver.session(database=NEO4J_DATABASE, notifications_min_severity=\"OFF\")\n",
    "    except TypeError:\n",
    "        session = driver.session(database=NEO4J_DATABASE)\n",
    "\n",
    "    rows_out: List[Dict[str, Any]] = []\n",
    "    failed_rows: List[Dict[str, Any]] = []\n",
    "    after = -1\n",
    "    processed = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    # diagnostics counters\n",
    "    ok_rel = 0\n",
    "    fail_too_long = fail_json = fail_bad = fail_api = fail_other = 0\n",
    "    missing_snippets = 0\n",
    "\n",
    "    # classification tallies (this run)\n",
    "    pos_cnt = neu_cnt = neg_cnt = unk_cnt = 0\n",
    "\n",
    "    # First 11 columns to reuse in failed_citations.csv\n",
    "    first_11_cols = [\n",
    "        \"Source Case ID\",\n",
    "        \"Source Case Name\",\n",
    "        \"Source Case Decision Date\",\n",
    "        \"Source Case citation_pipe\",\n",
    "        \"Source Case Summary\",\n",
    "        \"Target Case ID\",\n",
    "        \"Target Case Name\",\n",
    "        \"Target Case Decision Date\",\n",
    "        \"Target Case citation_pipe\",\n",
    "        \"Target Case Summary\",\n",
    "        \"Opinion Snippet\",\n",
    "    ]\n",
    "\n",
    "    with session as s:\n",
    "        while True:\n",
    "            rels = s.run(Q_PAGE_REL, {\"after_id\": after, \"limit\": batch_size}).data()\n",
    "            if not rels:\n",
    "                if echo:\n",
    "                    print(\"All done. No more relations.\")\n",
    "                break\n",
    "\n",
    "            if echo:\n",
    "                print(f\"\\nBatch after rel_id {after}: {len(rels)} relation(s)\")\n",
    "\n",
    "            for row in rels:\n",
    "                after = row[\"rel_id\"]\n",
    "\n",
    "                # --- force=False: skip ONLY edges that already have a non-Unknown label ---\n",
    "                existing_label = (row.get(\"existing_label\") or \"\").strip()\n",
    "                if (not force) and existing_label and existing_label.lower() != \"unknown\":\n",
    "                    continue\n",
    "\n",
    "                src_name = row[\"src_name\"] or \"\"\n",
    "                tgt_name = row[\"tgt_name\"] or \"\"\n",
    "                src_sum  = row.get(\"src_summary\") or \"\"\n",
    "                tgt_sum  = row.get(\"tgt_summary\") or \"\"\n",
    "                tgt_cit  = _first_citation(row.get(\"tgt_cite\") or \"\")\n",
    "\n",
    "                # Gather snippet_1, snippet_2, ... from relation properties\n",
    "                snippets = _extract_numbered_snippets(row.get(\"rel_props\") or {})\n",
    "                if not snippets:\n",
    "                    # No snippets available — mark Unknown and move on\n",
    "                    reason_text = \"No snippets available on the relation (snippet_1..N).\"\n",
    "                    s.run(Q_WRITE_REL_ANNOT, {\n",
    "                        \"src_id\": row[\"src_id\"],\n",
    "                        \"tgt_id\": row[\"tgt_id\"],\n",
    "                        \"label\": \"Unknown\",\n",
    "                        \"rationale\": reason_text,\n",
    "                        \"snippet_joined\": \"\",\n",
    "                        \"model_used\": BEDROCK_MODEL_ID\n",
    "                    })\n",
    "                    missing_snippets += 1\n",
    "                    unk_cnt += 1\n",
    "                    if echo:\n",
    "                        print(f\"{src_name} → {tgt_name}: Unknown\")\n",
    "\n",
    "                    common_row = {\n",
    "                        \"Source Case ID\": row[\"src_id\"],\n",
    "                        \"Source Case Name\": src_name,\n",
    "                        \"Source Case Decision Date\": row.get(\"src_date\") or \"\",\n",
    "                        \"Source Case citation_pipe\": row.get(\"src_cite\") or \"\",\n",
    "                        \"Source Case Summary\": src_sum,\n",
    "                        \"Target Case ID\": row[\"tgt_id\"],\n",
    "                        \"Target Case Name\": tgt_name,\n",
    "                        \"Target Case Decision Date\": row.get(\"tgt_date\") or \"\",\n",
    "                        \"Target Case citation_pipe\": row.get(\"tgt_cite\") or \"\",\n",
    "                        \"Target Case Summary\": tgt_sum,\n",
    "                        \"Opinion Snippet\": \"\",\n",
    "                    }\n",
    "\n",
    "                    # record a row anyway for results CSV parity\n",
    "                    rows_out.append({\n",
    "                        **common_row,\n",
    "                        \"Citation Evaluation\": \"Unknown\",\n",
    "                        \"LLM Rationale\": reason_text,\n",
    "                        COL_URL_HEADER: row.get(\"src_url\") or \"\",\n",
    "                    })\n",
    "\n",
    "                    # record failed row (Attempt=0 since LLM was not called)\n",
    "                    failed_rows.append({\n",
    "                        **common_row,\n",
    "                        \"Attempt\": 0,\n",
    "                        \"Input to LLM\": \"\",\n",
    "                        \"LLM Output\": \"\",\n",
    "                        \"Type of Error\": _describe_error(\"missing_snippets\"),\n",
    "                    })\n",
    "\n",
    "                    processed += 1\n",
    "                    continue\n",
    "\n",
    "                # Build labeled block for the LLM and joined snippet for storage/CSV\n",
    "                snippet_block = _format_snippet_block_labeled(snippets)\n",
    "                joined_for_store = \"\\n\\n\".join(snippets)\n",
    "\n",
    "                lab, rat, status, raw, attempt_used, attempt_logs, user_input = classify_with_bedrock(\n",
    "                    citing_case_name=src_name, citing_case_summary=src_sum,\n",
    "                    cited_case_name=tgt_name, cited_case_citation=tgt_cit, cited_case_summary=tgt_sum,\n",
    "                    snippet_block_labeled=snippet_block,\n",
    "                    max_new_tokens=340,\n",
    "                    retries=3,\n",
    "                    echo=echo\n",
    "                )\n",
    "\n",
    "                common_row = {\n",
    "                    \"Source Case ID\": row[\"src_id\"],\n",
    "                    \"Source Case Name\": src_name,\n",
    "                    \"Source Case Decision Date\": row.get(\"src_date\") or \"\",\n",
    "                    \"Source Case citation_pipe\": row.get(\"src_cite\") or \"\",\n",
    "                    \"Source Case Summary\": src_sum,\n",
    "                    \"Target Case ID\": row[\"tgt_id\"],\n",
    "                    \"Target Case Name\": tgt_name,\n",
    "                    \"Target Case Decision Date\": row.get(\"tgt_date\") or \"\",\n",
    "                    \"Target Case citation_pipe\": row.get(\"tgt_cite\") or \"\",\n",
    "                    \"Target Case Summary\": tgt_sum,\n",
    "                    \"Opinion Snippet\": joined_for_store,\n",
    "                }\n",
    "\n",
    "                if lab:\n",
    "                    # Write clean classification + rationale to Neo4j\n",
    "                    s.run(Q_WRITE_REL_ANNOT, {\n",
    "                        \"src_id\": row[\"src_id\"],\n",
    "                        \"tgt_id\": row[\"tgt_id\"],\n",
    "                        \"label\": lab,\n",
    "                        \"rationale\": rat,\n",
    "                        \"snippet_joined\": joined_for_store,\n",
    "                        \"model_used\": BEDROCK_MODEL_ID\n",
    "                    })\n",
    "                    ok_rel += 1\n",
    "                    if lab == \"Positive\":\n",
    "                        pos_cnt += 1\n",
    "                    elif lab == \"Neutral\":\n",
    "                        neu_cnt += 1\n",
    "                    elif lab == \"Negative\":\n",
    "                        neg_cnt += 1\n",
    "\n",
    "                    if echo:\n",
    "                        print(f\"{src_name} → {tgt_name}: {lab}\")\n",
    "\n",
    "                    # Results CSV row (schema like the old pipeline)\n",
    "                    rows_out.append({\n",
    "                        **common_row,\n",
    "                        \"Citation Evaluation\": lab,\n",
    "                        \"LLM Rationale\": rat,\n",
    "                        COL_URL_HEADER: row.get(\"src_url\") or \"\",\n",
    "                    })\n",
    "                else:\n",
    "                    # Unknown with reason\n",
    "                    if status == \"too_long\":\n",
    "                        fail_too_long += 1\n",
    "                    elif status == \"json_parse_failed\":\n",
    "                        fail_json += 1\n",
    "                    elif status == \"bad_keys_or_values\":\n",
    "                        fail_bad += 1\n",
    "                    elif status in (\"api_error\", \"api_response_error\", \"api_error_throttled\"):\n",
    "                        fail_api += 1\n",
    "                    else:\n",
    "                        fail_other += 1\n",
    "\n",
    "                    unk_cnt += 1\n",
    "\n",
    "                    rationale_text = f\"Classification failed: {status}\"\n",
    "                    s.run(Q_WRITE_REL_ANNOT, {\n",
    "                        \"src_id\": row[\"src_id\"],\n",
    "                        \"tgt_id\": row[\"tgt_id\"],\n",
    "                        \"label\": \"Unknown\",\n",
    "                        \"rationale\": rationale_text,\n",
    "                        \"snippet_joined\": joined_for_store,\n",
    "                        \"model_used\": BEDROCK_MODEL_ID\n",
    "                    })\n",
    "                    if echo:\n",
    "                        print(f\"{src_name} → {tgt_name}: Unknown\")\n",
    "\n",
    "                    rows_out.append({\n",
    "                        **common_row,\n",
    "                        \"Citation Evaluation\": \"Unknown\",\n",
    "                        \"LLM Rationale\": rationale_text,\n",
    "                        COL_URL_HEADER: row.get(\"src_url\") or \"\",\n",
    "                    })\n",
    "\n",
    "                    # Record one failed row per attempt\n",
    "                    for log in attempt_logs:\n",
    "                        failed_rows.append({\n",
    "                            **common_row,\n",
    "                            \"Attempt\": log.get(\"attempt\", 0),\n",
    "                            \"Input to LLM\": log.get(\"input\", \"\"),\n",
    "                            \"LLM Output\": log.get(\"raw\", \"\"),\n",
    "                            \"Type of Error\": _describe_error(log.get(\"status\", \"\")),\n",
    "                        })\n",
    "\n",
    "                processed += 1\n",
    "\n",
    "    # Build df_results from this run\n",
    "    df_results = pd.DataFrame(rows_out)\n",
    "\n",
    "    # Optionally replace with ALL currently labeled edges from Neo4j\n",
    "    if show_all_labels_in_output_csv:\n",
    "        df_results = _fetch_all_labeled_results(driver)\n",
    "        if echo:\n",
    "            print(f\"\\nshow_all_labels_in_output_csv=True → df_results replaced with full labeled dataset (rows: {len(df_results)})\")\n",
    "    else:\n",
    "        if echo:\n",
    "            print(f\"\\nshow_all_labels_in_output_csv=False → df_results only has rows from this run (rows: {len(df_results)})\")\n",
    "\n",
    "    # -------- Optional CSV of results --------\n",
    "    if results_csv:\n",
    "        df_results.to_csv(results_csv_filename, index=False)\n",
    "        print(f\"\\nWrote {len(df_results)} rows → {results_csv_filename}\")\n",
    "    else:\n",
    "        print(f\"\\nSkipping results CSV write (results_csv=False). Rows buffered in df_results: {len(df_results)}\")\n",
    "\n",
    "    # -------- Optional CSV of failed classifications --------\n",
    "    if failed_csv:\n",
    "        df_failed = pd.DataFrame(failed_rows)\n",
    "        if not df_failed.empty:\n",
    "            # Ensure column order: first 11 columns, then Attempt, Input to LLM, LLM Output, Type of Error\n",
    "            ordered_cols = first_11_cols + [\n",
    "                c for c in [\"Attempt\", \"Input to LLM\", \"LLM Output\", \"Type of Error\"]\n",
    "                if c in df_failed.columns\n",
    "            ]\n",
    "            # Add any missing columns as empty\n",
    "            for c in ordered_cols:\n",
    "                if c not in df_failed.columns:\n",
    "                    df_failed[c] = \"\"\n",
    "            df_failed = df_failed[ordered_cols]\n",
    "            df_failed.to_csv(\"failed_citations.csv\", index=False)\n",
    "            print(f\"Wrote {len(df_failed)} failed rows → failed_citations.csv\")\n",
    "        else:\n",
    "            print(\"No failed classifications in this run; failed_citations.csv not written.\")\n",
    "    else:\n",
    "        if echo:\n",
    "            print(f\"Skipping failed CSV write (failed_csv=False). Failed rows in memory: {len(failed_rows)}\")\n",
    "\n",
    "    # -------- Optional labeled dataset join (no duplicate case-name columns) --------\n",
    "    if append_to_labeled_dataset_csv:\n",
    "        if labeled_output_csv is None:\n",
    "            stem = pathlib.Path(append_to_labeled_dataset_csv).stem\n",
    "            labeled_output_csv = f\"{stem} - model comparison.csv\"\n",
    "\n",
    "        try:\n",
    "            df_label = _read_csv_fallback(append_to_labeled_dataset_csv)\n",
    "            print(f\"Loaded labeled dataset with fallback reader: {append_to_labeled_dataset_csv}\")\n",
    "        except Exception as e:\n",
    "            df_label = None\n",
    "            print(f\"Could not read labeled dataset CSV: {e}\")\n",
    "\n",
    "        if df_label is not None:\n",
    "            # Labeled dataset must have these columns:\n",
    "            # [\"Source ID\",\"Source Name\",\"Target ID\",\"Target Name\",\"Chunk\",\"Label\",\"Rationale\"]\n",
    "            expected = [\"Source ID\",\"Source Name\",\"Target ID\",\"Target Name\",\"Chunk\",\"Label\",\"Rationale\"]\n",
    "            missing = [c for c in expected if c not in df_label.columns]\n",
    "            if missing:\n",
    "                print(f\"Labeled CSV missing required columns: {missing}. Skipping append.\")\n",
    "            else:\n",
    "                # 1) Standardize labeled columns (avoid later header collisions)\n",
    "                df_label_std = df_label.rename(columns={\n",
    "                    \"Source ID\":   \"Source Case ID\",\n",
    "                    \"Target ID\":   \"Target Case ID\",\n",
    "                    \"Source Name\": \"Source Case Name (Labeled)\",\n",
    "                    \"Target Name\": \"Target Case Name (Labeled)\",\n",
    "                    \"Rationale\":   \"Rational\"  # keep user's original header for labeled rationale\n",
    "                })\n",
    "\n",
    "                # 2) Prepare join frame from current df_results with MODEL-suffixed columns\n",
    "                df_join = df_results.rename(columns={\n",
    "                    \"Source Case Name\":        \"Source Case Name (Model)\",\n",
    "                    \"Target Case Name\":        \"Target Case Name (Model)\",\n",
    "                    \"Source Case Summary\":     \"Source Case Summary (Model)\",\n",
    "                    \"Target Case Summary\":     \"Target Case Summary (Model)\",\n",
    "                    \"Opinion Snippet\":         \"Chunk (Pulled by Pipeline)\",\n",
    "                    \"Citation Evaluation\":     \"Label (Model)\",\n",
    "                    \"LLM Rationale\":           \"Rational (Model)\",\n",
    "                })\n",
    "\n",
    "                needed_from_results = [\n",
    "                    \"Source Case ID\", \"Target Case ID\",\n",
    "                    \"Source Case Name (Model)\", \"Target Case Name (Model)\",\n",
    "                    \"Source Case Decision Date\", \"Source Case citation_pipe\",\n",
    "                    \"Source Case Summary (Model)\",\n",
    "                    \"Target Case Decision Date\", \"Target Case citation_pipe\",\n",
    "                    \"Target Case Summary (Model)\",\n",
    "                    \"Chunk (Pulled by Pipeline)\",\n",
    "                    \"Label (Model)\", \"Rational (Model)\",\n",
    "                    COL_URL_HEADER,\n",
    "                ]\n",
    "                for c in needed_from_results:\n",
    "                    if c not in df_join.columns:\n",
    "                        df_join[c] = \"\"\n",
    "\n",
    "                # 3) Merge labeled ↔ model outputs on IDs\n",
    "                merged = df_label_std.merge(\n",
    "                    df_join[needed_from_results],\n",
    "                    on=[\"Source Case ID\", \"Target Case ID\"],\n",
    "                    how=\"inner\"\n",
    "                )\n",
    "\n",
    "                # 4) Build unified case-name columns (prefer labeled, else model)\n",
    "                def _prefer_labeled(lbl, mdl):\n",
    "                    lbl_series = merged.get(lbl)\n",
    "                    mdl_series = merged.get(mdl)\n",
    "                    if lbl_series is None and mdl_series is None:\n",
    "                        return pd.Series([], dtype=\"object\")\n",
    "                    if lbl_series is None:\n",
    "                        return mdl_series\n",
    "                    if mdl_series is None:\n",
    "                        return lbl_series\n",
    "                    # treat empty/whitespace as missing\n",
    "                    lbl_clean = lbl_series.fillna(\"\").astype(str)\n",
    "                    use_lbl = ~lbl_clean.str.fullmatch(r\"\\s*\")\n",
    "                    out = mdl_series.copy()\n",
    "                    out[use_lbl] = lbl_series[use_lbl]\n",
    "                    return out\n",
    "\n",
    "                merged[\"Source Case Name\"] = _prefer_labeled(\"Source Case Name (Labeled)\", \"Source Case Name (Model)\")\n",
    "                merged[\"Target Case Name\"] = _prefer_labeled(\"Target Case Name (Labeled)\", \"Target Case Name (Model)\")\n",
    "\n",
    "                # 5) Assemble final ordered columns (single 'Source/Target Case Name')\n",
    "                final_cols = [\n",
    "                    \"Source Case ID\",\n",
    "                    \"Source Case Name\",\n",
    "                    \"Source Case Decision Date\",\n",
    "                    \"Source Case citation_pipe\",\n",
    "                    \"Source Case Summary (Model)\",\n",
    "                    \"Target Case ID\",\n",
    "                    \"Target Case Name\",\n",
    "                    \"Target Case Decision Date\",\n",
    "                    \"Target Case citation_pipe\",\n",
    "                    \"Target Case Summary (Model)\",\n",
    "                    \"Chunk\",                        # from labeled dataset\n",
    "                    \"Chunk (Pulled by Pipeline)\",   # from model/pipeline\n",
    "                    \"Label\",                        # from labeled dataset\n",
    "                    \"Label (Model)\",                # from model\n",
    "                    \"Rational\",                     # from labeled dataset\n",
    "                    \"Rational (Model)\",             # from model\n",
    "                    COL_URL_HEADER,\n",
    "                ]\n",
    "                for c in final_cols:\n",
    "                    if c not in merged.columns:\n",
    "                        merged[c] = \"\"\n",
    "\n",
    "                # 6) Drop the temporary name columns and write CSV\n",
    "                drop_cols = [\n",
    "                    \"Source Case Name (Labeled)\", \"Source Case Name (Model)\",\n",
    "                    \"Target Case Name (Labeled)\", \"Target Case Name (Model)\"\n",
    "                ]\n",
    "                merged = merged[[c for c in final_cols if c in merged.columns]]\n",
    "                for dc in drop_cols:\n",
    "                    if dc in merged.columns:\n",
    "                        merged = merged.drop(columns=[dc])\n",
    "\n",
    "                merged.to_csv(labeled_output_csv, index=False)\n",
    "                print(f\"Wrote labeled+model comparison CSV → {labeled_output_csv} (rows: {len(merged)})\")\n",
    "\n",
    "    # -------- Optional previous vs. recent comparison (schema like before) --------\n",
    "    if compare_with_previous_csv:\n",
    "        if comparison_output_csv is None:\n",
    "            stem = pathlib.Path(compare_with_previous_csv).stem\n",
    "            comparison_output_csv = f\"{stem} - model comparison.csv\"\n",
    "\n",
    "        try:\n",
    "            df_prev = pd.read_csv(compare_with_previous_csv)\n",
    "        except Exception as e:\n",
    "            df_prev = None\n",
    "            print(f\"Could not read previous model CSV: {e}\")\n",
    "\n",
    "        if df_prev is not None:\n",
    "            prev_cols_map = {\n",
    "                \"Source Case Summary\": \"Source Case Summary (Previous Model)\",\n",
    "                \"Target Case Summary\": \"Target Case Summary (Previous Model)\",\n",
    "                \"Opinion Snippet\": \"Opinion Snippet (Previous Model)\",\n",
    "                \"Citation Evaluation\": \"Citation Evaluation (Previous Model)\",\n",
    "                \"LLM Rationale\": \"LLM Rationale (Previous Model)\",\n",
    "            }\n",
    "            cur_cols_map = {\n",
    "                \"Source Case Summary\": \"Source Case Summary (Recent Model)\",\n",
    "                \"Target Case Summary\": \"Target Case Summary (Recent Model)\",\n",
    "                \"Opinion Snippet\": \"Opinion Snippet (Recent Model)\",\n",
    "                \"Citation Evaluation\": \"Citation Evaluation (Recent Model)\",\n",
    "                \"LLM Rationale\": \"LLM Rationale (Recent Model)\",\n",
    "            }\n",
    "\n",
    "            prev_ren = df_prev.rename(columns=prev_cols_map)\n",
    "            cur_ren  = df_results.rename(columns=cur_cols_map)\n",
    "\n",
    "            join_keys = [\"Source Case ID\", \"Target Case ID\"]\n",
    "            for k in join_keys:\n",
    "                if k not in prev_ren.columns and k in df_prev.columns:\n",
    "                    prev_ren[k] = df_prev[k]\n",
    "                if k not in cur_ren.columns and k in df_results.columns:\n",
    "                    cur_ren[k] = df_results[k]\n",
    "\n",
    "            comp = prev_ren.merge(cur_ren, on=join_keys, how=\"outer\")\n",
    "\n",
    "            ordered = [\n",
    "                \"Source Case ID\",\n",
    "                \"Source Case Name\",\n",
    "                \"Source Case Decision Date\",\n",
    "                \"Source Case citation_pipe\",\n",
    "                \"Source Case Summary (Previous Model)\",\n",
    "                \"Source Case Summary (Recent Model)\",\n",
    "                \"Target Case ID\",\n",
    "                \"Target Case Name\",\n",
    "                \"Target Case Decision Date\",\n",
    "                \"Target Case citation_pipe\",\n",
    "                \"Target Case Summary (Previous Model)\",\n",
    "                \"Target Case Summary (Recent Model)\",\n",
    "                \"Opinion Snippet (Previous Model)\",\n",
    "                \"Opinion Snippet (Recent Model)\",\n",
    "                \"Citation Evaluation (Previous Model)\",\n",
    "                \"Citation Evaluation (Recent Model)\",\n",
    "                \"LLM Rationale (Previous Model)\",\n",
    "                \"LLM Rationale (Recent Model)\",\n",
    "                COL_URL_HEADER,\n",
    "            ]\n",
    "            for c in ordered:\n",
    "                if c not in comp.columns:\n",
    "                    comp[c] = \"\"\n",
    "            comp = comp[ordered]\n",
    "            comp.to_csv(comparison_output_csv, index=False)\n",
    "            print(f\"Wrote previous vs recent model comparison CSV → {comparison_output_csv} (rows: {len(comp)})\")\n",
    "\n",
    "    # -------- Global label totals (after this run) --------\n",
    "    with driver.session(database=NEO4J_DATABASE) as s_counts:\n",
    "        row_counts = s_counts.run(Q_COUNT_LABELS, {}).single()\n",
    "        total_pos = row_counts[\"pos_total\"] or 0\n",
    "        total_neu = row_counts[\"neu_total\"] or 0\n",
    "        total_neg = row_counts[\"neg_total\"] or 0\n",
    "        total_unk = row_counts[\"unk_total\"] or 0\n",
    "\n",
    "    # Summary\n",
    "    dt_min = (time.time() - t0)/60\n",
    "    print(f\"\\nProcessed relations: {processed} | Elapsed: {dt_min:.1f} min\")\n",
    "    print(\"=== Diagnostics ===\")\n",
    "    print(f\"  Successful (classified): {ok_rel}\")\n",
    "    print(f\"  Missing snippets:        {missing_snippets}\")\n",
    "    print(f\"  Fail (too long):         {fail_too_long}\")\n",
    "    print(f\"  Fail (JSON parse):       {fail_json}\")\n",
    "    print(f\"  Fail (bad keys/values):  {fail_bad}\")\n",
    "    print(f\"  Fail (API error):        {fail_api}\")\n",
    "    print(f\"  Fail (other):            {fail_other}\")\n",
    "    print(\"=== Label counts (this run) ===\")\n",
    "    print(f\"  Positive: {pos_cnt}\")\n",
    "    print(f\"  Neutral : {neu_cnt}\")\n",
    "    print(f\"  Negative: {neg_cnt}\")\n",
    "    print(f\"  Unknown : {unk_cnt}\")\n",
    "    print(\"=== Dataset label totals (after run) ===\")\n",
    "    print(f\"  Positive: {total_pos}\")\n",
    "    print(f\"  Neutral : {total_neu}\")\n",
    "    print(f\"  Negative: {total_neg}\")\n",
    "    print(f\"  Unknown : {total_unk}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Example call\n",
    "# =========================\n",
    "# label_all_citations(\n",
    "#     force=False,\n",
    "#     echo=True,\n",
    "#     results_csv=True,\n",
    "#     results_csv_filename=\"edge_classifications_claude.csv\",\n",
    "#     append_to_labeled_dataset_csv=\"Phase One Final Labeled Dataset from WK.csv\",\n",
    "#     labeled_output_csv=None,\n",
    "#     compare_with_previous_csv=None,\n",
    "#     comparison_output_csv=None,\n",
    "#     show_all_labels_in_output_csv=True,\n",
    "#     failed_csv=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311426d6-601b-4bf7-9b53-1ae1128558f1",
   "metadata": {},
   "source": [
    "## Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "556408ce-c406-4d74-965b-d5dff856bdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch after rel_id -1: 68 relation(s)\n",
      "      · attempt 1/3\n",
      "Hastings v. Papillion-LaVista School District → Wisbey v. City of Lincoln, Neb.: Positive\n",
      "      · attempt 1/3\n",
      "Kirkeberg v. Canadian Pacific Railway → Equal Employment Opportunity Commission, and Judith Keane, Intervening v. Sears, Roebuck & Company: Positive\n",
      "      · attempt 1/3\n",
      "Connie M. Gretillat v. Care Initiatives → Ellen Fjellestad v. Pizza Hut of America, Inc.: Neutral\n",
      "      · attempt 1/3\n",
      "Keane, Judith v. Sears Roebuck → US Airways, Inc. v. Barnett: Positive\n",
      "      · attempt 1/3\n",
      "Equal Employment Opportunity Commission, and Judith Keane, Intervening v. Sears, Roebuck & Company → US Airways, Inc. v. Barnett: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.32 seconds\n",
      "      · attempt 2/3\n",
      "Kiphart v. Saturn Corp. → Roger Monette and Doris Monette v. Electronic Data Systems Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.37 seconds\n",
      "      · attempt 2/3\n",
      "Settle v. SW Rodgers, Co., Inc. → Dutcher v. Ingalls Shipbuilding: Neutral\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.37 seconds\n",
      "      · attempt 2/3\n",
      "Equal Employment Opportunity Commission v. Abercrombie & Fitch Stores, Inc. → Trans World Airlines, Inc. v. Hardison: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.23 seconds\n",
      "      · attempt 2/3\n",
      "      · API ClientError on attempt 2/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 4.29 seconds\n",
      "      · attempt 3/3\n",
      "Sanchez v. Vilsack → Sutton v. United Air Lines, Inc.: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.37 seconds\n",
      "      · attempt 2/3\n",
      "Alfred F. Marinelli v. City of Erie, Pennsylvania → Dutcher v. Ingalls Shipbuilding: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.33 seconds\n",
      "      · attempt 2/3\n",
      "Lewis v. Humboldt Acquisition Corp., Inc. → Roger Monette and Doris Monette v. Electronic Data Systems Corporation: Negative\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.22 seconds\n",
      "      · attempt 2/3\n",
      "Kevin J. Gilday v. Mecosta County → Schluter v. Industrial Coils, Inc.: Negative\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.17 seconds\n",
      "      · attempt 2/3\n",
      "Kathleen Lentini v. California Center for the Arts, Escondido Alan Corbin Randy Vogel, and Does 1-10 → Neff v. American Dairy Queen Corp.: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.18 seconds\n",
      "      · attempt 2/3\n",
      "Alberti v. City & County of San Francisco Sheriff's Department → John Doe v. University of Maryland Medical System Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.19 seconds\n",
      "      · attempt 2/3\n",
      "Mark Bledsoe v. Palm Beach County Soil and Water Conservation District, Board of County Commissioners for Palm Beach County → John Doe v. University of Maryland Medical System Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.38 seconds\n",
      "      · attempt 2/3\n",
      "Jamison v. Dow Chemical Co. → Roger Monette and Doris Monette v. Electronic Data Systems Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.21 seconds\n",
      "      · attempt 2/3\n",
      "US Airways, Inc. v. Barnett → Trans World Airlines, Inc. v. Hardison: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.18 seconds\n",
      "      · attempt 2/3\n",
      "Mogenhan v. Chertoff → Webb v. Choate: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.11 seconds\n",
      "      · attempt 2/3\n",
      "Acevedo v. City of Philadelphia → Frazier v. Simmons: Neutral\n",
      "      · attempt 1/3\n",
      "Ronald Jeffrey Kiphart v. Saturn Corporation → Roger Monette and Doris Monette v. Electronic Data Systems Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.23 seconds\n",
      "      · attempt 2/3\n",
      "      · API ClientError on attempt 2/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 4.28 seconds\n",
      "      · attempt 3/3\n",
      "Torres-Alman v. Verizon Wireless Puerto Rico, Inc. → Bankston v. Chertoff: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.16 seconds\n",
      "      · attempt 2/3\n",
      "Donald Brown v. Kansas City Freightliner Sales → Rask v. Fresenius Medical Care North America: Positive\n",
      "      · attempt 1/3\n",
      "Downs v. Massachusetts Bay Transportation Authority → John Doe v. University of Maryland Medical System Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.03 seconds\n",
      "      · attempt 2/3\n",
      "Brooklyn Center for Independence of Disabled v. Bloomberg → Dupler v. Costco Wholesale Corp.: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.10 seconds\n",
      "      · attempt 2/3\n",
      "Sharon MacY v. Hopkins County School Board of Education → Roger Monette and Doris Monette v. Electronic Data Systems Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.09 seconds\n",
      "      · attempt 2/3\n",
      "Gerard Cotter v. Ajilon Services, Inc. → Roger Monette and Doris Monette v. Electronic Data Systems Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.05 seconds\n",
      "      · attempt 2/3\n",
      "Ellen Fjellestad v. Pizza Hut of America, Inc. → Rebecca A. Berg v. Norand Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.09 seconds\n",
      "      · attempt 2/3\n",
      "Hastings v. Papillion-LaVista School District → Nyrop v. Independent School District No. 11: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.47 seconds\n",
      "      · attempt 2/3\n",
      "Kirkeberg v. Canadian Pacific Railway → Connie M. Gretillat v. Care Initiatives: Positive\n",
      "      · attempt 1/3\n",
      "Equal Employment Opportunity Commission v. Abercrombie & Fitch Stores, Inc. → Woodman v. Runyon: Positive\n",
      "      · attempt 1/3\n",
      "Sanchez v. Vilsack → Woodman v. Runyon: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.43 seconds\n",
      "      · attempt 2/3\n",
      "Alfred F. Marinelli v. City of Erie, Pennsylvania → Thompson v. Dot Foods, Inc.: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.01 seconds\n",
      "      · attempt 2/3\n",
      "Lewis v. Humboldt Acquisition Corp., Inc. → Gerard Cotter v. Ajilon Services, Inc.: Negative\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.24 seconds\n",
      "      · attempt 2/3\n",
      "Kevin J. Gilday v. Mecosta County → Coghlan v. HJ Heinz Co.: Positive\n",
      "      · attempt 1/3\n",
      "Alberti v. City & County of San Francisco Sheriff's Department → Mark Bledsoe v. Palm Beach County Soil and Water Conservation District, Board of County Commissioners for Palm Beach County: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.20 seconds\n",
      "      · attempt 2/3\n",
      "Jamison v. Dow Chemical Co. → Ronald Jeffrey Kiphart v. Saturn Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.42 seconds\n",
      "      · attempt 2/3\n",
      "US Airways, Inc. v. Barnett → Woodman v. Runyon: Positive\n",
      "      · attempt 1/3\n",
      "Acevedo v. City of Philadelphia → Reginald D. Fedro v. Janet Reno, 1 Attorney General of the United States: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.22 seconds\n",
      "      · attempt 2/3\n",
      "Schluter v. Industrial Coils, Inc. → Dutcher v. Ingalls Shipbuilding: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.09 seconds\n",
      "      · attempt 2/3\n",
      "Ronald Jeffrey Kiphart v. Saturn Corporation → Kiphart v. Saturn Corp.: Neutral\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.22 seconds\n",
      "      · attempt 2/3\n",
      "Downs v. Massachusetts Bay Transportation Authority → Mark Bledsoe v. Palm Beach County Soil and Water Conservation District, Board of County Commissioners for Palm Beach County: Positive\n",
      "      · attempt 1/3\n",
      "Daigle v. Liberty Life Insurance → Dutcher v. Ingalls Shipbuilding: Positive\n",
      "      · attempt 1/3\n",
      "Hastings v. Papillion-LaVista School District → Rebecca A. Berg v. Norand Corporation: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.38 seconds\n",
      "      · attempt 2/3\n",
      "Sanchez v. Vilsack → McGeshick v. Principi: Positive\n",
      "      · attempt 1/3\n",
      "Lewis v. Humboldt Acquisition Corp., Inc. → Sharon MacY v. Hopkins County School Board of Education: Negative\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.41 seconds\n",
      "      · attempt 2/3\n",
      "Jamison v. Dow Chemical Co. → Gerard Cotter v. Ajilon Services, Inc.: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.02 seconds\n",
      "      · attempt 2/3\n",
      "Hastings v. Papillion-LaVista School District → Ellen Fjellestad v. Pizza Hut of America, Inc.: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.29 seconds\n",
      "      · attempt 2/3\n",
      "Sanchez v. Vilsack → Reginald D. Fedro v. Janet Reno, 1 Attorney General of the United States: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.32 seconds\n",
      "      · attempt 2/3\n",
      "Dutcher v. Ingalls Shipbuilding → Coghlan v. HJ Heinz Co.: Neutral\n",
      "      · attempt 1/3\n",
      "Ragan v. JEFFBOAT, LLC → Thomas Amadio v. Ford Motor Company: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.24 seconds\n",
      "      · attempt 2/3\n",
      "Sutton v. United Air Lines, Inc. → Schluter v. Industrial Coils, Inc.: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.10 seconds\n",
      "      · attempt 2/3\n",
      "Perez-Sosa v. Garland → Barrett v. Salt Lake County: Negative\n",
      "      · attempt 1/3\n",
      "Williamson v. American National Insurance Company → Dutcher v. Ingalls Shipbuilding: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.19 seconds\n",
      "      · attempt 2/3\n",
      "Sutton v. United Air Lines, Inc. → Coghlan v. HJ Heinz Co.: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.23 seconds\n",
      "      · attempt 2/3\n",
      "Way v. City of Missouri City → Daigle v. Liberty Life Insurance: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.41 seconds\n",
      "      · attempt 2/3\n",
      "Scot L. Zimmerman v. State of Oregon Department of Justice → John Doe v. University of Maryland Medical System Corporation: Neutral\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.05 seconds\n",
      "      · attempt 2/3\n",
      "Woodman v. Runyon → Reginald D. Fedro v. Janet Reno, 1 Attorney General of the United States: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.34 seconds\n",
      "      · attempt 2/3\n",
      "Williamson v. American National Insurance Company → Grant v. Lone Star Co.: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.21 seconds\n",
      "      · attempt 2/3\n",
      "Nyrop v. Independent School District No. 11 → Connie M. Gretillat v. Care Initiatives: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.30 seconds\n",
      "      · attempt 2/3\n",
      "Scot L. Zimmerman v. State of Oregon Department of Justice → Mark Bledsoe v. Palm Beach County Soil and Water Conservation District, Board of County Commissioners for Palm Beach County: Negative\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.33 seconds\n",
      "      · attempt 2/3\n",
      "Sutton v. United Air Lines, Inc. → Dutcher v. Ingalls Shipbuilding: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.45 seconds\n",
      "      · attempt 2/3\n",
      "Roger Monette and Doris Monette v. Electronic Data Systems Corporation → Reginald D. Fedro v. Janet Reno, 1 Attorney General of the United States: Positive\n",
      "      · attempt 1/3\n",
      "Javonda Scruggs v. Pulaski County, Arkansas → Kirkeberg v. Canadian Pacific Railway: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.08 seconds\n",
      "      · attempt 2/3\n",
      "Sutton v. United Air Lines, Inc. → Kevin J. Gilday v. Mecosta County: Positive\n",
      "      · attempt 1/3\n",
      "Scot L. Zimmerman v. State of Oregon Department of Justice → Alberti v. City & County of San Francisco Sheriff's Department: Negative\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.06 seconds\n",
      "      · attempt 2/3\n",
      "Henderson v. New York Life, Inc. → Schluter v. Industrial Coils, Inc.: Positive\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.08 seconds\n",
      "      · attempt 2/3\n",
      "Scot L. Zimmerman v. State of Oregon Department of Justice → Downs v. Massachusetts Bay Transportation Authority: Negative\n",
      "      · attempt 1/3\n",
      "      · API ClientError on attempt 1/3: ThrottlingException - Too many requests, please wait before trying again.\n",
      "      · throttled, backing off for 2.23 seconds\n",
      "      · attempt 2/3\n",
      "Henderson v. New York Life, Inc. → Dutcher v. Ingalls Shipbuilding: Positive\n",
      "All done. No more relations.\n",
      "\n",
      "show_all_labels_in_output_csv=True → df_results replaced with full labeled dataset (rows: 68)\n",
      "\n",
      "Wrote 68 rows → edge_classifications_snippet_method - Claude.csv\n",
      "No failed classifications in this run; failed_citations.csv not written.\n",
      "\n",
      "Processed relations: 68 | Elapsed: 21.8 min\n",
      "=== Diagnostics ===\n",
      "  Successful (classified): 68\n",
      "  Missing snippets:        0\n",
      "  Fail (too long):         0\n",
      "  Fail (JSON parse):       0\n",
      "  Fail (bad keys/values):  0\n",
      "  Fail (API error):        0\n",
      "  Fail (other):            0\n",
      "=== Label counts (this run) ===\n",
      "  Positive: 54\n",
      "  Neutral : 6\n",
      "  Negative: 8\n",
      "  Unknown : 0\n",
      "=== Dataset label totals (after run) ===\n",
      "  Positive: 54\n",
      "  Neutral : 6\n",
      "  Negative: 8\n",
      "  Unknown : 0\n"
     ]
    }
   ],
   "source": [
    "# Assumes relations already have snippet_1, snippet_2, ... properties.\n",
    "label_all_citations(force=True,\n",
    "                    echo=True,\n",
    "                    results_csv=True, results_csv_filename=\"edge_classifications_snippet_method - Claude.csv\",\n",
    "                    show_all_labels_in_output_csv=True,\n",
    "                    failed_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cab10b-2eb0-4623-923e-f4f8e844d298",
   "metadata": {},
   "source": [
    "## Compare with labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c66c0d1-8629-4774-b137-3f68a23bc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_all_citations(force=True,\n",
    "#                     echo=True,\n",
    "#                     append_to_labeled_dataset_csv=\"Phase One Final Labeled Dataset from WK.csv\",\n",
    "#                     labeled_output_csv=\"WK Labeled vs Snippet Method Model Comparison - Claude.csv\",\n",
    "#                     show_all_labels_in_output_csv=True,\n",
    "#                     failed_csv=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
